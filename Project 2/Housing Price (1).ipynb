{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a199029b",
   "metadata": {
    "id": "a199029b"
   },
   "source": [
    "# The Housing Price Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8935f190",
   "metadata": {
    "id": "8935f190"
   },
   "source": [
    "## Prelude:\n",
    "\n",
    "House forms the most basic need for any human. The real estate market all over the world is considered to be the most lucrative and sustainable business. It is a huge market and contributes majorly to economy of any country. \n",
    "An US-based company \"Surprise Housing\" is planning to penetrate Australian market.\n",
    "A new world strategy is to use data science for decision making even in this domain. The data science role in this field would be majorly to find the necessary variables impacting the sale price of the houses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386c414",
   "metadata": {
    "id": "c386c414"
   },
   "source": [
    "## About the variables:\n",
    "The independent variables:\n",
    "\n",
    "Id - A unque identifier variable\n",
    "\n",
    "MSSubClass -  A numeric categorical variable\n",
    "\n",
    "MSZoning  - A categorical variable\n",
    "\n",
    "LotFrontage  - A numeric categorical variable   \n",
    "\n",
    "LotArea  - A continuous variable\n",
    "\n",
    "Street - A categorical variable\n",
    "\n",
    "Alley  - A categorical variable\n",
    "\n",
    "LotShape - A categorical variable\n",
    "    \n",
    "LandContour  - A categorical variable\n",
    "\n",
    "Utilities - A categorical variable\n",
    "\n",
    "LotConfig   - A categorical variable\n",
    "\n",
    "LandSlope - A categorical variable\n",
    "\n",
    "Neighborhood - A categorical variable\n",
    "\n",
    "Condition1  - A categorical variable\n",
    "\n",
    "Condition2 - A categorical variable    \n",
    "\n",
    "BldgType  - A categorical variable   \n",
    " \n",
    "HouseStyle  - A categorical variable\n",
    "\n",
    "OverallQual - A numeric categorical variable\n",
    "\n",
    "OverallCond  - A numeric categorical variable\n",
    "\n",
    "YearBuilt - A numeric categorical variable\n",
    "\n",
    "YearRemodAdd - A numeric categorical variable \n",
    "\n",
    "RoofStyle - A categorical variable\n",
    "\n",
    "RoofMatl    - A categorical variable\n",
    "\n",
    "Exterior1st    - A categorical variable\n",
    "\n",
    "Exterior2nd   - A categorical variable \n",
    "\n",
    "MasVnrType   - A categorical variable \n",
    "\n",
    "MasVnrArea   - A numeric categorical variable \n",
    "\n",
    "ExterQual  - A categorical variable \n",
    "\n",
    "ExterCond  - A categorical variable \n",
    "\n",
    "Foundation - A categorical variable \n",
    "\n",
    "BsmtQual  - A categorical variable \n",
    "\n",
    "BsmtCond - A categorical variable \n",
    "\n",
    "BsmtExposure - A categorical variable \n",
    "\n",
    "BsmtFinType1  - A categorical variable \n",
    "\n",
    "BsmtFinSF1 -  A numeric categorical variable\n",
    "\n",
    "BsmtFinType2 - A categorical variable \n",
    "\n",
    "BsmtFinSF2    -  A numeric categorical variable\n",
    "\n",
    "BsmtUnfSF - A numeric variable\n",
    "\n",
    "TotalBsmtSF - A numeric variable\n",
    "\n",
    "Heating  -  A  categorical variable\n",
    "\n",
    "HeatingQC -  A categorical variable\n",
    "\n",
    "CentralAir  - A categorical variable\n",
    "\n",
    "Electrical   - A categorical variable\n",
    "\n",
    "1stFlrSF - A numeric variable\n",
    "\n",
    "2ndFlrSF -  A numeric categorical variable  \n",
    "\n",
    "LowQualFinSF \n",
    "\n",
    "GrLivArea     \n",
    "\n",
    "BsmtFullBath\n",
    "\n",
    "BsmtHalfBath  \n",
    "\n",
    "FullBath\n",
    "\n",
    "HalfBath  \n",
    "\n",
    "BedroomAbvGr   \n",
    "\n",
    "KitchenAbvGr \n",
    "\n",
    "KitchenQual   \n",
    "\n",
    "TotRmsAbvGrd \n",
    "\n",
    "Functional  \n",
    "\n",
    "Fireplaces\n",
    "\n",
    "FireplaceQu  \n",
    "\n",
    "GarageType \n",
    "\n",
    "GarageYrBlt \n",
    "\n",
    "GarageFinish   \n",
    "\n",
    "GarageCars    \n",
    "\n",
    "GarageArea    \n",
    "\n",
    "GarageQual     \n",
    "\n",
    "GarageCond   \n",
    "\n",
    "PavedDrive    \n",
    "\n",
    "WoodDeckSF    \n",
    "\n",
    "OpenPorchSF    \n",
    "\n",
    "EnclosedPorch  \n",
    "\n",
    "3SsnPorch      \n",
    "\n",
    "ScreenPorch \n",
    "\n",
    "PoolArea     \n",
    "\n",
    "PoolQC   \n",
    "\n",
    "Fence   \n",
    "\n",
    "MiscFeature   \n",
    "\n",
    "MiscVal        \n",
    "\n",
    "MoSold \n",
    "\n",
    "YrSold        \n",
    "\n",
    "SaleType  \n",
    "\n",
    "SaleCondition \n",
    "\n",
    "The dependent variable:\n",
    "\n",
    "SalePrice - A continuous dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afffd0",
   "metadata": {
    "id": "49afffd0"
   },
   "source": [
    "## Problem Statement:\n",
    "\n",
    "We are to build a prediction model to predict the sale price of the property in order to maximize profits with available independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c715a",
   "metadata": {
    "id": "047c715a"
   },
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "\n",
    "# Importing fundamental packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell        ## To display multiple outputs\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pyforest            \n",
    "\n",
    "## For visualization\n",
    "import matplotlib.pyplot as plt                                         \n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "## Data Pre-Processing Packages\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## To create copy of data\n",
    "import copy\n",
    "\n",
    "## Pipeline Packages\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "## Ensemble Learning Algorithms Packages\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "## Evaluation Metrics Packages\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report, plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# Bagging and Boosting\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier                           \n",
    "\n",
    "# Saving the model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32eace",
   "metadata": {
    "id": "0a32eace"
   },
   "outputs": [],
   "source": [
    "def read(link):\n",
    "    global data\n",
    "\n",
    "    data=pd.read_csv(link)\n",
    "    \n",
    "    data=pd.DataFrame(data)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26919124",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "26919124",
    "outputId": "755602d7-11bc-4c7d-ef12-0340133d9088"
   },
   "outputs": [],
   "source": [
    "read(link=\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(link):\n",
    "    global data_test\n",
    "\n",
    "    data_test=pd.read_csv(link)\n",
    "    \n",
    "    data_test=pd.DataFrame(data_test)\n",
    "    print(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "read(link=\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57c35b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db57c35b",
    "outputId": "45b150cb-fb09-4b45-a837-26cfa76863ec"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8d178",
   "metadata": {
    "id": "81d8d178"
   },
   "source": [
    "For Training Data:\n",
    "\n",
    "- There are 1168 rows and 81 columns\n",
    "- There are 43 categorical/object data variables and 38 numeric data variables \n",
    "- Data has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef8c1f",
   "metadata": {},
   "source": [
    "For Testing Data:\n",
    "\n",
    "- There are 292 rows and 80 columns\n",
    "- There are 42 categorical/object data variables and 38 numeric data variables \n",
    "- Data has missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a90bee",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec20f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "94ec20f7",
    "outputId": "3619db87-e556-4c9b-86cb-0df7216822a8"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f97eaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "76f97eaf",
    "outputId": "69322ed1-2caf-4c22-f2bc-90c0b2e63d37"
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523228a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a523228a",
    "outputId": "ab177ff4-cff5-4184-d6ae-bec8109443e9"
   },
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    print(\"For \",i,\":\" , \"\\n\")\n",
    "    print(\"The Unique values for \" ,i ,\" = \", data[i].nunique(),\"\\n\")\n",
    "    print(data[i].value_counts())\n",
    "    print(\"\\n\",\"\\n\",\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49491bd3",
   "metadata": {
    "id": "49491bd3"
   },
   "source": [
    "Columns: Alley, PoolQC, Fence and MiscFeature has less than 25% of values. Which means we would need to impute more than 75% of the values. This will result in changing the character of the dataset and will make the model faulty. Due to data inconsistency I am dropping those columns.\n",
    "\n",
    "Even \"Id\" column is just an identifier having 1168 unique values which does not contribute anything significantly so it's safe to drop that variable too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b741c",
   "metadata": {
    "id": "4a0b741c"
   },
   "outputs": [],
   "source": [
    "data= data.drop([\"Id\",\"Alley\",\"PoolQC\",\"Fence\",\"MiscFeature\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cfddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test= data_test.drop([\"Id\",\"Alley\",\"PoolQC\",\"Fence\",\"MiscFeature\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871c4fa",
   "metadata": {
    "id": "3871c4fa"
   },
   "source": [
    "Before data pre-processing let us first see the significant vriables and check their multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2badd04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "c2badd04",
    "outputId": "afdfe741-d660-4473-938f-a45ec0a57cac"
   },
   "outputs": [],
   "source": [
    "data_cat = pd.DataFrame()\n",
    "data_num = pd.DataFrame()\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == \"O\":\n",
    "        data_cat[i] = data[i]\n",
    "    else:\n",
    "        data_num[i] = data[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f00dd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "03f00dd2",
    "outputId": "3b85e569-5b13-41b0-af41-539a6d071527"
   },
   "outputs": [],
   "source": [
    "data_cat.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b15ff6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "62b15ff6",
    "outputId": "8b713d89-24b7-4539-eed9-a1398b18ffaf"
   },
   "outputs": [],
   "source": [
    "data_num.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ab3e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "679ab3e3",
    "outputId": "8f67ab32-fb39-425d-ad2e-b02ed2b7729a"
   },
   "outputs": [],
   "source": [
    "data_anova= pd.DataFrame()\n",
    "data_anova= pd.concat([data_cat,data_num[\"SalePrice\"]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f47bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d3f47bd",
    "outputId": "9286592f-854a-4013-c3ba-ca9c8ddcf9be",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in data_anova:\n",
    "    print(\"Title: \",i, \"with SalePrice\" )\n",
    "    data_anova.groupby([\"SalePrice\"]).describe().transpose()\n",
    "    print(\"\\n\",\"\\n\")\n",
    "    data_anova.groupby([\"SalePrice\"]).sum().transpose()\n",
    "    print(\"XXXXXXX###############XXXXXXXXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24180c48",
   "metadata": {
    "id": "24180c48"
   },
   "outputs": [],
   "source": [
    "for i in data_num:\n",
    "    print(\"Title: \",i, \"with SalePrice\" )\n",
    "    data_num.groupby([i,\"SalePrice\"]).describe().transpose()\n",
    "    print(\"\\n\",\"\\n\")\n",
    "    data_num[i].value_counts()\n",
    "    print(\"XXXXXXX###############XXXXXXXXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1281c",
   "metadata": {
    "id": "7ca1281c"
   },
   "outputs": [],
   "source": [
    "for i in data_anova:\n",
    "    crosstab= pd.crosstab(data_anova[i], data_anova[\"SalePrice\"])\n",
    "    print(\" For \",i,\"& Price\",\"\\n\")\n",
    "    print (\" h0: Average \", i, \" of a customer does not affect sales price\")\n",
    "    print (\" ha: Average \",i,\" of a customer does affect sales price\")\n",
    "    \n",
    "# defining the table\n",
    "    mod=ols('SalePrice~data_anova[i]',data=data_anova).fit()\n",
    "\n",
    "# Performing Anova\n",
    "    avotable= sm.stats.anova_lm(mod)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Print result\n",
    "    print(avotable)\n",
    "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b1e23",
   "metadata": {
    "id": "fd9b1e23",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting heat map to check correlation using Pearson\n",
    "\n",
    "plt.figure(figsize=(50,50))                      \n",
    "plot=sns.heatmap(data_num.corr(method = \"pearson\"), annot = True)\n",
    "fig=plot.get_figure()\n",
    "fig.savefig(\"heat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db4ec0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data_num.corr()[\"SalePrice\"].sort_values(ascending=False).drop([\"SalePrice\"]).plot(kind='bar',color='g')\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "plt.ylabel('column with target names',fontsize=14)\n",
    "plt.title('correlation',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb450a1a",
   "metadata": {
    "id": "fb450a1a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_num.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb2156",
   "metadata": {
    "id": "6b19c88d"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed52dde",
   "metadata": {
    "id": "aed52dde"
   },
   "outputs": [],
   "source": [
    "# Pair Plot\n",
    "sns.pairplot(data)\n",
    "fig2=plot.get_figure()\n",
    "fig2.savefig(\"pairplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram for all the variables using plotly_express\n",
    "\n",
    "for i in data.columns:\n",
    "    fig = px.histogram(data, x= i, histfunc = \"count\", \n",
    "                       width=750, height=600, title = \"Histogram for \" + i, \n",
    "                       template=\"plotly_dark\", cumulative=True)\n",
    "\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(mydata):                        # Outlier Plotting\n",
    "    for i in mydata.columns:\n",
    "        fig = px.box(mydata, y= i, width=600, height=400, title=i, template=\"plotly_dark\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier(mydata=data.drop([\"SalePrice\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1d614",
   "metadata": {
    "id": "23f1d614"
   },
   "outputs": [],
   "source": [
    "data2= copy.deepcopy(data) # Making a copy of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95435ead",
   "metadata": {},
   "source": [
    "\"Street\",\"LandSlope\" and \"Condition2\" are insignificant variables.\n",
    "\n",
    "\"BsmtFinSF1\",\"OverallQual\",\"YearBuilt\",\"YearRemodAdd\", 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr','GarageYrBlt', 'GarageArea',\"TotRmsAbvGrd\" and \"GarageCars\" are columns with multicollinearity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c084e6",
   "metadata": {
    "id": "24a549f7"
   },
   "outputs": [],
   "source": [
    "data2=data.drop([\"Street\",\"LandSlope\",\"Condition2\",\"BsmtFinSF1\",\"OverallQual\",\"YearBuilt\",\"YearRemodAdd\", 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n",
    "                 '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr','GarageYrBlt', 'GarageArea',\"TotRmsAbvGrd\",\n",
    "                 \"GarageCars\"], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7345e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=data_test.drop([\"Street\",\"LandSlope\",\"Condition2\",\"BsmtFinSF1\",\"OverallQual\",\"YearBuilt\",\"YearRemodAdd\", 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n",
    "                 '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr','GarageYrBlt', 'GarageArea',\"TotRmsAbvGrd\",\n",
    "                 \"GarageCars\"], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc42e49",
   "metadata": {},
   "source": [
    "### **Findings:**\n",
    "\n",
    "- 1. It is found that Residential Low Density zoning has maximum count, for the feature general zoning classification of the sale.\n",
    "\n",
    "- 2. In Paved streets we can observe maximum count, for the feature Type of road access to property.\n",
    "\n",
    "- 3. Regular shaped property has maximum count, for the feature General shape of property.\n",
    "\n",
    "- 4. Near Flat/Level property has maximum count, for the feature Flatness of the property.\n",
    "\n",
    "- 5. Inside lot configured property has maximum count, for the feature Lot configuration.\n",
    "\n",
    "- 6. Gentle sloped property has maximum count, for the feature Slope of property.\n",
    "\n",
    "- 7.  If the property is located in North Ames then count is good compared to other locations, for the feature Physical locations within Ames city limits.\n",
    "\n",
    "- 8. If the Proximity to various conditions-1 is normal then count is high for the feature Proximity to various conditions.\n",
    "\n",
    "- 9. If the Proximity to various conditions-2 is normal then count is high for the feature Proximity to various conditions (if more than one is present).\n",
    "\n",
    "- 10. Single-family Detached dwelling has maximum count for the feature Type of dwelling.\n",
    "\n",
    "- 11. One story dwelling housestyle has maximum count for the feature Style of dwelling.\n",
    "\n",
    "- 12. For Gable roof style the count is high for the feature Type of roof.\n",
    "\n",
    "- 13. For Standard Shingle roof material the count is high for the feature Roof material.\n",
    "\n",
    "- 14. For Vinyl Siding exterior-1 covering on house has maximum counts for the feature Exterior covering on house.\n",
    "\n",
    "- 15. For Vinyl Siding exterior-2 covering on house has maximum counts for the feature Exterior covering on house (if more than one material).\n",
    "\n",
    "- 16. For Masonry veneer type None has maximum count.\n",
    "\n",
    "- 17.For Typical/Average(TA) quality of the material on the exterior has maximum count, for the feature Evaluates the quality of the material on the exterior (ExterQual).\n",
    "\n",
    "- 18. For Typical/Average(TA) condition of the material on the exterior has maximum count for the feature Evaluates the present condition of the material on the exterior.\n",
    "\n",
    "- 19. For Cinder Block and Poured Contrete foundations the count is maximum for the feature Type of foundation.\n",
    "\n",
    "- 20. For good and average quality heights of the basement the count is high for the feature Evaluates the height of the basement.\n",
    "\n",
    "- 21. For Typical/Average(TA) general condition of the basement the count is high for the feature Evaluates the general condition of the basement.\n",
    "\n",
    "- 22. For No Exposure garden level walls the count is maximum for the feature Refers to walkout or garden level walls(BsmtExposure).\n",
    "\n",
    "- 23. For unfinished Rating of basement finished area-1 the count is maximum for the feature Rating of basement finished area(BsmtFinType1).\n",
    "\n",
    "- 24. For unfinished Rating of basement finished area-2 the count is maximum for the feature Rating of basement finished area (if multiple types)(BsmtFinType2).\n",
    "\n",
    "- 25. For Gas forced warm air furnace type of heating the count is maximum for the feature Type of heating(Heating).\n",
    "\n",
    "- 26. For Excellent Heating quality and condition the count is high for the feature Heating quality and condition(HeatingQC).\n",
    "\n",
    "- 27. For Central air conditioning-yes has maximum count for the feature Central air conditioning(CentralAir).\n",
    "\n",
    "- 28. For Standard Circuit Breakers & Romex Electrical system the count is high for the feature Electrical system(Electrical).\n",
    "\n",
    "- 29. For Typical/Average(TA) and good Kitchen quality the count is maximum for the feature Kitchen quality(KitchenQual).\n",
    "\n",
    "- 30. Typical Functionality has highest count for Home functionality (Assume typical unless deductions are warranted)(Functional).\n",
    "\n",
    "- 31. For good Fireplace quality the count is high for the feature Fireplace quality(FireplaceQu).\n",
    "\n",
    "- 32. If Garage location Attached to home then the count is high, for the feature Garage location(GarageType).\n",
    "\n",
    "- 33. For Unfinished Interior of the garage the count is maximum, for the feature Interior finish of the garage(GarageFinish).\n",
    "\n",
    "- 34. For Typical/Average(TA) Garage quality the count is high, for the feature Garage quality(GarageQual).\n",
    "\n",
    "- 35. For Typical/Average(TA) Garage condition the count is high, for the feature Garage condition(GarageCond).\n",
    "\n",
    "- 36. For Paved driveway the count is maximum, for the feature Paved driveway(PavedDrive).\n",
    "\n",
    "- 37. For Warranty Deed - Conventional type of sales the count is maximum, for the feature Type of sale(SaleType).\n",
    "\n",
    "- 38. For Normal sales condition the count is high, for the feature Condition of sale(SaleCondition).\n",
    "\n",
    "- 39.As Linear feet of street connected to property(LotFrontage) is increseing sales is decreasing and the SalePrice is rangeing between 0-3 lakhs.\n",
    "\n",
    "- 40.As Lot size in square feet(LotArea) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 41.As Masonry veneer area in square feet(MasVnrArea) is increasing sales is decreasing and saleprice is rangeing between 0-4 lakhs.\n",
    "\n",
    "- 42.As Type 1 finished square feet(BsmtFinSF1) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 43.As Unfinished square feet of basement area(BsmtUnfSF) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs. There are some outliers also.\n",
    "\n",
    "- 44.As Total square feet of basement area(TotalBsmtSF) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 45.As First Floor square feet(1stFlrSF) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 46.As Second floor square feet(2ndFlrSF) is increseing sales is increasing in the range 500-1000 and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 47.As Above grade (ground) living area square feet(GrLivArea) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 48.As Size of garage in square feet(GarageArea) is increseing sales is increseing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 49.As Wood deck area in square feet(WoodDeckSF) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 50.As Open porch area in square feet(OpenPorchSF) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 51.As Year_SinceBuilt is increseing sales is decreasing and the saleprice is high for newly built building and the sales price is in between 0-4 lakhs.\n",
    "\n",
    "- 52.As Since Remodel date (same as construction date if no remodeling or additions)(Year_SinceRemodAdded) is increseing sales is decreasing and the saleprice is in between 1-4 lakhs.\n",
    "\n",
    "- 53.As Since Year garage was built(GarageAge) is increseing sales is decreasing and the saleprice is in between 0-4 lakhs.\n",
    "\n",
    "- 54.For 1-STORY 1946 & NEWER ALL STYLES(20) and 2-STORY 1946 & NEWER(60) types of dwelling(MSSuubClass) the sales is good and SalePrice is also high.\n",
    "\n",
    "- 55.As Rates the overall material and finish of the house(OverallQual) is increasing linearly sales is also increasing And SalePrice is also increasing linearly.\n",
    "\n",
    "- 56.For 5(Average) overall condition of the house(OverallCond) the sales is high and SalePrice is also high. \n",
    "\n",
    "- 57.For 0 and 1 Basement full bathrooms(BsmtFullBath) the sales as well as SalePrice is high.\n",
    "\n",
    "- 58.For 0 Basement half bathrooms(BsmtHalfBath) the sales as well as SalePrice is high.\n",
    "\n",
    "- 59.For 1 and 2 Full bathrooms above grade(FullBath) the sales as well as SalePrice is high.\n",
    "\n",
    "- 60.For 0 and 1 Half baths above grade(HalfBath) the sales as well as SalePrice is high. \n",
    "\n",
    "- 61.For 2, 3 and 4 Bedrooms above grade (does NOT include basement bedrooms)(BedroomAbvGr) the sales as well as SalePrice is high.\n",
    "\n",
    "- 62.For 1 Kitchens above grade(KitchenAbvGr) the sales as well as SalePrice is high. \n",
    "\n",
    "- 63.For 4-9 Total rooms above grade (does not include bathrooms)(TotRmsAbvGrd) the sales as well as SalePrice is high.\n",
    "\n",
    "- 64.For 0 and 1 Number of fireplaces(Fireplaces) the sales as well as SalePrice is high.\n",
    "\n",
    "- 65.For 1 and 2 Size of garage in car capacity(GarageCars) the sales is high and for 3 Size of garage in car capacity(GarageCars) the SalePrice is high.\n",
    "\n",
    "- 66.In between april to august for Month Sold(MoSold) the sales is good with SalePrice.\n",
    "\n",
    "- 67.For all the Year_SinceSold the salePrice and sales both are same.\n",
    "\n",
    "- 68. For Floating Village Residential(FV) and Residential Low Density(RL) zoning classification of the sale(MSZoning) the saleprice is high.\n",
    "\n",
    "- 69. For paved type of road access to property(Street) the SalePrice is high.\n",
    "\n",
    "- 70. For Slightly irregular(IR1), Moderately Irregular(IR2) and Irregular(IR3) shape of property(LotShape) the SalePrice is high.\n",
    "\n",
    "- 71. For Hillside - Significant slope from side to side(HLS) Flatness of the property(LandContour) the SalePrice is High.\n",
    "\n",
    "- 72. For Cul-de-sac(CulDSac) Lot configuration(LotConfig) the SalePrice is High.\n",
    "\n",
    "- 73. For all types of Slope of property(LandSlope) i.e.,Gentle slope(Gtl), Moderate Slope(Mod) and Severe Slope(Sev) the SalePrice is High.\n",
    "\n",
    "- 74. For Northridge(NoRidge) locations within Ames city limits(Neighborhood) the SalePrice is High.\n",
    "\n",
    "- 75. For Within 200' of North-South Railroad(RRNn), Adjacent to postive off-site feature(PosA) and Near positive off-site feature--park, greenbelt, etc.(PosN) Proximity to various conditions(Condition1) has the maximum SalePrice.\n",
    "\n",
    "- 76. For Adjacent to postive off-site feature(PosA) and Near positive off-site feature--park, greenbelt, etc.(PosN) Proximity to various conditions (if more than one is present)(Condition2) has maximum SalePrice.\n",
    "\n",
    "- 77. For Single-family Detached(1Fam) and Townhouse End Unit(TwnhsE) type of dwelling(BldgType) the SalePrice is high.\n",
    "\n",
    "- 78. For 2Story and Two and one-half story: 2nd level finished(2.5Fin) Style of dwelling(HouseStyle) the SalePrice is high.\n",
    "\n",
    "- 79. For Shed Type of roof(RoofStyle) the SalePrice is high.\n",
    "\n",
    "- 80. For Wood Shingles(WdShngl) Roof material(RoofMat1) the SalePrice is high.\n",
    "\n",
    "- 81. For Cement Board(CemntBd), Imitation Stucco(ImStucc) and Stone type of Exterior covering on house(Exterior1st) the SalePrice is high.\n",
    "\n",
    "- 82. For Cement Board(CemntBd), Imitation Stucco(ImStucc) and other Exterior covering on house (if more than one material)(Exterior2) has maximum SalePrice.\n",
    "\n",
    "- 83. For Stone Masonry veneer type(MasvnrType) the SalePrice is high.\n",
    "\n",
    "- 84. For Excellent(Ex) quality of the material on the exterior(ExterQual) the SalePrice is high.\n",
    "\n",
    "- 85. For Excellent(Ex) present condition of the material on the exterior(ExterCond) the SalePrice is high.\n",
    "\n",
    "- 86. For Poured Contrete(PConc) Type of foundation(Foundation) the SalePrice is high.\n",
    "\n",
    "- 87. For Excellent(100+ inches)(Ex) height of the basement(BsmtQual) the SalePrice is high.\n",
    "\n",
    "- 88. For Good(Gd) general condition of the basement(BsmtCond) the SalePrice is high.\n",
    "\n",
    "- 89. For Good Exposure(Gd) of walkout or garden level walls(BsmtExposure) has maximum SalePrice.\n",
    "\n",
    "- 90. For Good Living Quarters(GLQ) of basement finished area(BsmtFinType1) has maximum SalePrice.\n",
    "\n",
    "- 91. For Good Living Quarters(GLQ) and Average Living Quarters(ALQ) of basement finished area (if multiple types)(BsmtFinType2) has maximum SalePrice. \n",
    "\n",
    "- 92. For Gas forced warm air furnace(GasA) and\tGas hot water or steam heat(GasW) Type of heating(Heating) has high SalePrice.\n",
    "\n",
    "- 93. For Excellent(Ex) Heating quality and condition(HeatingQC) the SalePriceis high.\n",
    "\n",
    "- 94. For building having Central air conditioning(CentralAir) the SalePrice is high.\n",
    "\n",
    "- 95. For Standard Circuit Breakers & Romex(Sbrkr) of Electrical system(Electrical) the SalePrice is Maximum.\n",
    "\n",
    "- 96. For Excellent(Ex) Kitchen quality(KitchenQual) the SalePrice is high.\n",
    "\n",
    "- 97. For Typical Functionality(Typ) type of Home functionality (Assume typical unless deductions are warranted)(Functional) the SalePrice is high.\n",
    "\n",
    "- 98. For Excellent - Exceptional Masonry Fireplace(Ex) of Fireplace quality(FireplaceQual) has highest SalePrice. \n",
    "\n",
    "- 99. For Built-In (Garage part of house - typically has room above garage)(BuiltIn) Garage location(GarageType) the SalePrice is maximum.\n",
    "\n",
    "- 100. For Completely finished(Fin) Interior of the garage(GarageFinish) the SalePrice is high.\n",
    "\n",
    "- 101. For Excellent(Ex) Garage quality(GarageQual) the SalePrice is high.\n",
    "\n",
    "- 102. For Typical/Average(TA) and Good(Gd) Garage condition(GarageCond) the SalePrice is high.\n",
    "\n",
    "- 103. For having Paved driveway(PavedDrive) the SalePriceis high.\n",
    "\n",
    "- 104. For Home just constructed and sold(New) and Contract 15% Down payment regular terms(Con) of type of sale(SaleType) has highest SalePrice.\n",
    "\n",
    "- 105. For Home was not completed when last assessed (associated with New Homes)(Partial) Condition of sale(SalesCondition) the SalePrice is maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat1 = pd.DataFrame()\n",
    "data_num1 = pd.DataFrame()\n",
    "for i in data2.columns:\n",
    "    if data2[i].dtype == \"O\":\n",
    "        data_cat1[i] = data2[i]\n",
    "    else:\n",
    "        data_num1[i] = data2[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = pd.DataFrame()\n",
    "test_num = pd.DataFrame()\n",
    "for i in data_test.columns:\n",
    "    if data_test[i].dtype == \"O\":\n",
    "        test_cat[i] = data_test[i]\n",
    "    else:\n",
    "        test_num[i] = data_test[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897ad54",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_num1.drop(['SalePrice'],axis=1).columns:\n",
    "    data_num1[i] = np.where(data_num1[i] > (data_num1[i].quantile(0.75) + (data_num1[i].quantile(0.75) - data_num1[i].quantile(0.25))*1.5),\n",
    "                           (data_num1[i].quantile(0.75) + (data_num1[i].quantile(0.75) - data_num1[i].quantile(0.25))*1.5),\n",
    "                          np.where(data_num1[i] < (data_num1[i].quantile(0.25) - (data_num1[i].quantile(0.75) - data_num1[i].quantile(0.25))*1.5),\n",
    "                           (data_num1[i].quantile(0.25) - (data_num1[i].quantile(0.75) - data_num1[i].quantile(0.25))*1.5),data_num1[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_num.columns:\n",
    "    test_num[i] = np.where(test_num[i] > (test_num[i].quantile(0.75) + (test_num[i].quantile(0.75) - test_num[i].quantile(0.25))*1.5),\n",
    "                           (test_num[i].quantile(0.75) + (test_num[i].quantile(0.75) - test_num[i].quantile(0.25))*1.5),\n",
    "                          np.where(test_num[i] < (test_num[i].quantile(0.25) - (test_num[i].quantile(0.75) - test_num[i].quantile(0.25))*1.5),\n",
    "                           (test_num[i].quantile(0.25) - (test_num[i].quantile(0.75) - test_num[i].quantile(0.25))*1.5),test_num[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84072972",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier(mydata= data_num1.drop([\"SalePrice\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce16db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num1.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b582772",
   "metadata": {},
   "outputs": [],
   "source": [
    "col= [\"MSSubClass\",\"OverallCond\",\"MasVnrArea\",\"Fireplaces\",\"WoodDeckSF\",\"OpenPorchSF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e10199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing skewness using Yeo-Johnson\n",
    "# Removing data skewness\n",
    "pt = PowerTransformer(method='yeo-johnson',standardize='True')\n",
    "    \n",
    "data_num1[col]= pt.fit_transform(data_num1[col].values)\n",
    "#data_noskewtab= pd.DataFrame(data_noskew)\n",
    "#data_noskewtab.columns = col\n",
    "\n",
    "print(data_num1.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col2= [\"MSSubClass\",\"OverallCond\",\"MasVnrArea\",\"Fireplaces\",\"WoodDeckSF\",\"OpenPorchSF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing skewness using Yeo-Johnson\n",
    "# Removing data skewness\n",
    "pt = PowerTransformer(method='yeo-johnson',standardize='True')\n",
    "    \n",
    "test_num[col2]= pt.fit_transform(test_num[col2].values)\n",
    "#data_noskewtab= pd.DataFrame(data_noskew)\n",
    "#data_noskewtab.columns = col\n",
    "\n",
    "print(test_num.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59c879",
   "metadata": {
    "id": "9f59c879"
   },
   "outputs": [],
   "source": [
    "# Encoding Variables\n",
    "enc = LabelEncoder()\n",
    "for i in data_cat1:\n",
    "    data_cat1[i] = enc.fit_transform(data_cat1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5089f",
   "metadata": {
    "id": "e7d5089f"
   },
   "outputs": [],
   "source": [
    "data_pre= pd.concat([data_cat1,data_num1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5caa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Variables\n",
    "enc = LabelEncoder()\n",
    "for i in data_cat1:\n",
    "    test_cat[i] = enc.fit_transform(test_cat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27778745",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test2= pd.concat([test_cat,test_num], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22848b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution plot for all numerical columns\n",
    "plt.figure(figsize = (20,40))\n",
    "plotnumber = 1\n",
    "for column in data_pre:\n",
    "    if plotnumber <=35:\n",
    "        ax = plt.subplot(12,3,plotnumber)\n",
    "        sns.distplot(data_pre[column])\n",
    "        plt.xlabel(column,fontsize = 20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bf021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regplot for numerical columns\n",
    "col= data_pre.columns\n",
    "\n",
    "plt.figure(figsize=(20,130))\n",
    "for i in range(len(col)):\n",
    "    plt.subplot(56,2,i+1)\n",
    "    sns.regplot(x=data_pre[col[i]] , y=data_pre['SalePrice'],color=\"g\")\n",
    "    plt.title(f\"SalePrice VS {col[i]}\",fontsize=20)\n",
    "    plt.xticks(fontsize=15)  \n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel(col[i],fontsize = 20)\n",
    "    plt.ylabel('SalePrice',fontsize = 20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c535f",
   "metadata": {
    "id": "a4c27aaf"
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc546ca",
   "metadata": {},
   "source": [
    "### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_1= SimpleImputer(strategy= \"mean\")\n",
    "impute_2= IterativeImputer(max_iter=10, random_state= 0)\n",
    "impute_3= KNNImputer(n_neighbors=9)\n",
    "model= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('impute',impute_1),\n",
    "    ('model',model),\n",
    "])    \n",
    "pipe2 = Pipeline([\n",
    "    ('impute',impute_2),\n",
    "    ('model',model),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('impute',impute_3),\n",
    "    ('model',model),  \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3338543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data, target, pipe):\n",
    "    \n",
    "    data_reset_index = data.reset_index(drop=True)\n",
    "    \n",
    "# Create Global Variable\n",
    "    global X\n",
    "    global y\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global y_train\n",
    "    global y_test\n",
    "    global y_pred\n",
    "\n",
    "# Segregate Feature & Target Variables\n",
    "    X = data_reset_index.drop(target, axis=1)\n",
    "    y = data_reset_index[target]\n",
    "\n",
    "# Split train & Test\n",
    "\n",
    "    X_train,  X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3, random_state=1)\n",
    "\n",
    "# Pipe.fit, pipe.predict and accuracy\n",
    "    \n",
    "    pipe.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_pred, y_test))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data_pre,\n",
    "           target='SalePrice',\n",
    "           pipe= pipe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee70552",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data_pre,\n",
    "           target='SalePrice',\n",
    "           pipe= pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data_pre,\n",
    "           target='SalePrice',\n",
    "           pipe= pipe3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features= data_pre.drop([\"SalePrice\"],axis=1)\n",
    "data_label=data_pre[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = pd.DataFrame(impute_2.fit_transform(data_features))\n",
    "ft_train.columns = data_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f65851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre2= pd.concat([ft_train,data_label],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ft_train:\n",
    "    print(\"For \",i,\":\" , \"\\n\")\n",
    "    print(\"The Unique values for \" ,i ,\" = \", ft_train[i].nunique(),\"\\n\")\n",
    "    print(ft_train[i].value_counts())\n",
    "    print(\"\\n\",\"\\n\",\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e1b5b",
   "metadata": {},
   "source": [
    "### Missing Value Imputation for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features2= data_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9baf1",
   "metadata": {
    "id": "6ef9baf1"
   },
   "outputs": [],
   "source": [
    "ft_test = pd.DataFrame(impute_2.fit_transform(data_features2))\n",
    "ft_test.columns = data_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test2= pd.DataFrame(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6529ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_pre2:\n",
    "    print(\"For \",i,\":\" , \"\\n\")\n",
    "    print(\"The Unique values for \" ,i ,\" = \", data_pre2[i].nunique(),\"\\n\")\n",
    "    print(data_pre2[i].value_counts())\n",
    "    print(\"\\n\",\"\\n\",\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b324e4",
   "metadata": {
    "id": "b6171e67"
   },
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0bce5",
   "metadata": {
    "id": "67f0bce5"
   },
   "outputs": [],
   "source": [
    "def split (data,target):\n",
    "    data_reset_index = data.reset_index(drop=True)\n",
    "# Data split\n",
    "    global x\n",
    "    global y\n",
    "    global x_train\n",
    "    global y_train\n",
    "    global x_test\n",
    "    global y_test\n",
    "# Segregate Feature & Target Variables\n",
    "    x = data_reset_index.drop(target, axis=1)\n",
    "    y = data_reset_index[target]\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3, random_state=3)\n",
    "    \n",
    "    print(x_train.info())\n",
    "    (\"\\n\")\n",
    "    print(x_test.info())\n",
    "    (\"\\n\")\n",
    "    print(y_train.shape)\n",
    "    (\"\\n\")\n",
    "    print(y_test.shape)\n",
    "    (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c886c6",
   "metadata": {
    "id": "27c886c6"
   },
   "outputs": [],
   "source": [
    "split(data=data_pre2,\n",
    "      target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90de8b",
   "metadata": {
    "id": "64cb7d62"
   },
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07e957",
   "metadata": {
    "id": "ca07e957"
   },
   "outputs": [],
   "source": [
    "scale_1= StandardScaler()\n",
    "scale_2= MinMaxScaler()\n",
    "scale_3= RobustScaler()\n",
    "model= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c94da",
   "metadata": {
    "id": "5d1c94da"
   },
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('Scale',scale_1),\n",
    "    ('model',model),\n",
    "])    \n",
    "pipe2 = Pipeline([\n",
    "    ('Scale',scale_2),\n",
    "    ('model',model),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('Scale',scale_3),\n",
    "    ('model',model),  \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d213c66",
   "metadata": {
    "id": "1d213c66"
   },
   "outputs": [],
   "source": [
    "# Create Function Name \n",
    "def pre_process(data, pipe):\n",
    "\n",
    "# Pipe.fit, pipe.predict and accuracy\n",
    "    \n",
    "    pipe.fit(x_train,y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(x_test)\n",
    "    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_pred, y_test))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b175eae",
   "metadata": {
    "id": "8b175eae"
   },
   "outputs": [],
   "source": [
    "pre_process(data= data_pre2,\n",
    "           pipe= pipe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1adca",
   "metadata": {
    "id": "c9f1adca"
   },
   "outputs": [],
   "source": [
    "pre_process(data= data_pre2,\n",
    "           pipe= pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ea3ac",
   "metadata": {
    "id": "ea3ea3ac"
   },
   "outputs": [],
   "source": [
    "pre_process(data= data_pre2,\n",
    "           pipe= pipe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec2119",
   "metadata": {
    "id": "eb590243"
   },
   "source": [
    "Since Standard Scalar scores the most, I will choose that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar= StandardScaler()\n",
    "scalar.fit(x_train)\n",
    "x_trainsc =  scalar.transform(x_train)\n",
    "x_testsc  =  scalar.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbebf85",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.5)\n",
    "pca.fit(x_trainsc)\n",
    "x_train_model = pca.transform(x_trainsc)\n",
    "x_test_model = pca.transform(x_testsc)\n",
    "ex_variance=np.var(x_train_model,axis=0)\n",
    "ex_variance_ratio = ex_variance/np.sum(ex_variance)\n",
    "\n",
    "print(\"shape of x_train_pca\", x_train_model.shape)\n",
    "print('')    \n",
    "print(\"Explained Variance Ratio for Training Dataset: \\n\", ex_variance_ratio)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "ex_variance_1 = np.var(x_test_model , axis=0)\n",
    "ex_variance_ratio_1 = ex_variance_1 / np.sum(ex_variance_1)\n",
    "    \n",
    "print(\"Explained Variance Ratio for Test Dataset: \\n\", ex_variance_ratio_1) \n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50074ab1",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb872d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_trainsc, y_train)\n",
    "\n",
    "\n",
    "model = sm.OLS(y_train, x_trainsc).fit()\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_testsc)\n",
    "\n",
    "print(\"Linear Regression Train Score: \", lr.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Linear Regression Test Score: \", lr.score(x_testsc , y_test))\n",
    "\n",
    "print('Mean Absolute Error:', np.round(metrics.mean_absolute_error(y_test, y_pred),2))  \n",
    "print('Mean Squared Error:', np.round(metrics.mean_squared_error(y_test, y_pred),2))  \n",
    "print('Root Mean Squared Error:', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "rfe = RFE(lr, n_features_to_select=6)             \n",
    "rfe = rfe.fit(x, y)\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "scores = cross_val_score(lr, x, y, scoring='r2', cv=5)\n",
    "scores   \n",
    "\n",
    "# create a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "scores = cross_val_score(lr, x, y, scoring='r2', cv=folds)\n",
    "scores  \n",
    "\n",
    "scores = cross_val_score(lr, x, y, scoring='neg_mean_squared_error', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc61e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_elastic1 = ElasticNet()\n",
    "\n",
    "lm_elastic1.fit(x_trainsc, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_elastic = lm_elastic1.predict(x_testsc)\n",
    "\n",
    "\n",
    "print(\"Elastic Net Train Score: \", lm_elastic1.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Elastic Net Test Score: \", lm_elastic1.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, y_pred_elastic),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, y_pred_elastic),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_elastic)),2))\n",
    "print(\" \")\n",
    "\n",
    "y_pred = lm_elastic1.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)\n",
    "\n",
    "coeff_used_003 = np.sum(lm_elastic1.coef_ != 0)\n",
    "\n",
    "print(\"Coefficient used: \",coeff_used_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17025f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(lm_elastic1, n_features_to_select=6)             \n",
    "rfe = rfe.fit(x_trainsc, y_train)\n",
    "\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "scores = cross_val_score(lm_elastic1, x, y, scoring='r2', cv=5)\n",
    "scores   \n",
    "\n",
    "# create a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "scores = cross_val_score(lm_elastic1, x, y, scoring='r2', cv=folds)\n",
    "scores  \n",
    "\n",
    "scores = cross_val_score(lm_elastic1, x, y, scoring='neg_mean_squared_error', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52127970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning Hyperparameters\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 3, shuffle = True, random_state = 100)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = [{'normalize':[True, False],'random_state':list(range(0, 10)), 'tol': list(range(-10, 100, 20)), \n",
    "                 'selection': ['cyclic', 'random']}]\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "lm_elastic = ElasticNet()\n",
    "\n",
    "lm_elastic.fit(x_trainsc, y_train)\n",
    "            \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = lm_elastic,\n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = 3,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "    \n",
    "\n",
    "# fit the model\n",
    "grid_result= model_cv.fit(x_trainsc, y_train)   \n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_elastic2 = ElasticNet( normalize= False, random_state= 7, selection='cyclic', tol= 10)\n",
    "\n",
    "lm_elastic2.fit(x_trainsc, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_elastic = lm_elastic2.predict(x_testsc)\n",
    "\n",
    "\n",
    "print(\"Elastic Net Train Score: \", lm_elastic2.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Elastic Net Test Score: \", lm_elastic2.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, y_pred_elastic),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, y_pred_elastic),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_elastic)),2))\n",
    "print(\" \")\n",
    "\n",
    "y_pred = lm_elastic2.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)\n",
    "\n",
    "coeff_used_003 = np.sum(lm_elastic2.coef_ != 0)\n",
    "\n",
    "print(\"Coefficient used: \",coeff_used_003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045df1fd",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1 = SVR()\n",
    "svm1.fit(x_trainsc, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_svm = svm1.predict(x_testsc)\n",
    "\n",
    "\n",
    "print(\"Support Vector Model Train Score: \", svm1.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Support Vector Model Test Score: \", svm1.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, y_pred_svm),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, y_pred_svm),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_svm)),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "y_pred = svm1.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24150169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for CV\n",
    "X, y = make_classification(n_samples=50, n_features=15, random_state=1)\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# create model\n",
    "model = SVR()\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: ' , scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ae53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning Hyperparameter\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 2, shuffle = True, random_state = 50)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "\n",
    "hyper_params = [{'kernel':['linear','poly','rbf','sigmoid'],\n",
    "                'gamma': ['scale','auto'],\n",
    "                'C':[0,0.1,0.2,0.5,1]}]\n",
    "\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "svm = SVR()\n",
    "\n",
    "svm.fit(x_trainsc, y_train)\n",
    "            \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = svm,\n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = 3,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "    \n",
    "\n",
    "# fit the model\n",
    "grid_result= model_cv.fit(x_trainsc, y_train)   \n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df309f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2 = SVR(C= 1, gamma= 'scale', kernel= 'linear')\n",
    "svm2.fit(x_trainsc, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_svm = svm2.predict(x_testsc)\n",
    "\n",
    "\n",
    "print(\"Support Vector Model Train Score: \", svm2.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Support Vector Model Test Score: \", svm2.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, y_pred_svm),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, y_pred_svm),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_svm)),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "y_pred = svm2.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8a770",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree1 = DecisionTreeRegressor(random_state=1)\n",
    "dTree1.fit(x_trainsc, y_train)\n",
    "y_predict = dTree1.predict(x_testsc)\n",
    "\n",
    "print(\"Decision Tree Model Train Score: \", dTree1.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Decision Tree Model Test Score: \", dTree1.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, y_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, y_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_predict)),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "y_pred = dTree1.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cad1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "max_depths = range(1, 10)\n",
    "training_error = []\n",
    "for max_depth in max_depths:\n",
    "    model_1 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model_1.fit(x_trainsc, y_train)\n",
    "    training_error.append(mse(y_train, model_1.predict(x_trainsc)))\n",
    "    \n",
    "testing_error = []\n",
    "for max_depth in max_depths:\n",
    "    model_2 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model_2.fit(x_trainsc, y_train)\n",
    "    testing_error.append(mse(y_test, model_2.predict(x_testsc)))\n",
    "\n",
    "plt.plot(max_depths, training_error, color='blue', label='Training error')\n",
    "plt.plot(max_depths, testing_error, color='green', label='Testing error')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.axvline(x=7, color='orange', linestyle='--')\n",
    "plt.annotate('optimum = 7', xy=(7.5, 1.17), color='red')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Hyperparameter Tuning', pad=15, size=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 3, shuffle = True, random_state = 50)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "\n",
    "hyper_params = [{'criterion':['squared_error','friedman_mse','absolute_error','poisson'],\n",
    "                 'max_features':['auto','sqrt','log2'],\n",
    "                'random_state':[0,3,5,7,]}]\n",
    "\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "dTree = DecisionTreeRegressor()\n",
    "\n",
    "dTree.fit(x_trainsc, y_train)\n",
    "            \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = dTree,\n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = 3,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "    \n",
    "\n",
    "# fit the model\n",
    "grid_result= model_cv.fit(x_trainsc, y_train)   \n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree2 = DecisionTreeRegressor(criterion='poisson',  max_features='auto', random_state=3)\n",
    "dTree2.fit(x_trainsc, y_train)\n",
    "y_predict = dTree2.predict(x_testsc)\n",
    "\n",
    "print(\"Decision Tree Model Train Score: \", dTree2.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Decision Tree Model Test Score: \", dTree2.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, y_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, y_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_predict)),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "y_pred = dTree2.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a42980",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model1= RandomForestRegressor()\n",
    "rf_model1.fit(x_trainsc, y_train)\n",
    "rf_model_predict = rf_model1.predict(x_testsc)\n",
    "\n",
    "print(\"Random Forest Model Train Score: \", rf_model1.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Random Forest Model Test Score: \", rf_model1.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, rf_model_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, rf_model_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, rf_model_predict)),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "y_pred = rf_model1.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 3, shuffle = True, random_state = 25)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "\n",
    "hyper_params = [{'criterion':['squared_error','absolute_error','poisson'],\n",
    "                'max_features':['log2','sqrt',None],\n",
    "                'random_state':[1,3,5,7],}]\n",
    "\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf.fit(x_trainsc, y_train)\n",
    "            \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = rf,\n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = 3,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "    \n",
    "\n",
    "# fit the model\n",
    "grid_result= model_cv.fit(x_trainsc, y_train)   \n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model2= RandomForestRegressor( criterion='absolute_error', max_features= 'sqrt', random_state=3)\n",
    "rf_model2.fit(x_trainsc, y_train)\n",
    "rf_model_predict = rf_model2.predict(x_testsc)\n",
    "\n",
    "print(\"Random Forest Model Train Score: \", rf_model2.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Random Forest Model Test Score: \", rf_model2.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, rf_model_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, rf_model_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, rf_model_predict)),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "y_pred = rf_model2.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9bf9c",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model1= GradientBoostingRegressor()\n",
    "gb_model1.fit(x_trainsc, y_train)\n",
    "gb_model_predict = gb_model1.predict(x_testsc)\n",
    "\n",
    "print(\"Gradient Boosting Model Train Score: \", gb_model1.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Gradient Boosting Model Test Score: \", gb_model1.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, gb_model_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, gb_model_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, gb_model_predict)),2))\n",
    "\n",
    "y_pred = gb_model1.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 3, shuffle = True, random_state = 50)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "\n",
    "hyper_params = [{'loss':['squared_error','absolute_error','huber', 'quantile'],\n",
    "                 'learning_rate': [0,0.1,1,3,5,7,10],\n",
    "                'criterion':['friedman_mse','squared_error','mse']}]\n",
    "\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "gb_model= GradientBoostingRegressor()\n",
    "\n",
    "gb_model.fit(x_trainsc, y_train)\n",
    "            \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = gb_model,\n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = 3,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "    \n",
    "\n",
    "# fit the model\n",
    "grid_result= model_cv.fit(x_trainsc, y_train)   \n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a008c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model2= GradientBoostingRegressor( criterion= 'squared_error', learning_rate= 0.1, loss= 'huber')\n",
    "gb_model2.fit(x_trainsc, y_train)\n",
    "gb_model_predict = gb_model2.predict(x_testsc)\n",
    "\n",
    "print(\"Gradient Boosting Model Train Score: \", gb_model2.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Gradient Boosting Model Test Score: \", gb_model2.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, gb_model_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, gb_model_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, gb_model_predict)),2))\n",
    "\n",
    "y_pred = gb_model2.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1228693",
   "metadata": {},
   "source": [
    "### Ada Boost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16568a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_model1= AdaBoostRegressor()\n",
    "adb_model1.fit(x_trainsc, y_train)\n",
    "adb_model_predict = adb_model1.predict(x_testsc)\n",
    "\n",
    "print(\"Ada  Boosting Model Train Score: \", adb_model1.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Ada Boosting Model Test Score: \", adb_model1.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, adb_model_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, adb_model_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, adb_model_predict)),2))\n",
    "\n",
    "y_pred = adb_model1.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits =5, shuffle = True, random_state = 50)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "\n",
    "hyper_params = [{'loss':['linear', 'square', 'exponential'],\n",
    "                 'learning_rate': [1,5,7,10],\n",
    "                'random_state':[1,3,5,7]}]\n",
    "\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "adb_model= AdaBoostRegressor()\n",
    "\n",
    "adb_model.fit(x_trainsc, y_train)\n",
    "            \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = adb_model,\n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = 3,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "    \n",
    "\n",
    "# fit the model\n",
    "grid_result= model_cv.fit(x_trainsc, y_train)   \n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_model2= AdaBoostRegressor(learning_rate=1, loss='exponential', random_state=3)\n",
    "adb_model2.fit(x_trainsc, y_train)\n",
    "adb_model_predict = adb_model2.predict(x_testsc)\n",
    "\n",
    "print(\"Ada  Boosting Model Train Score: \", adb_model2.score(x_trainsc , y_train))\n",
    "\n",
    "print(\"Ada Boosting Model Test Score: \", adb_model2.score(x_testsc , y_test))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error: \\n', np.round(metrics.mean_absolute_error(y_test, adb_model_predict),2)) \n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Mean Squared Error: \\n', np.round(metrics.mean_squared_error(y_test, adb_model_predict),2))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print('Root Mean Squared Error: \\n', np.round(np.sqrt(metrics.mean_squared_error(y_test, adb_model_predict)),2))\n",
    "\n",
    "y_pred = adb_model2.predict(x_testsc)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(\"R2= \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained regression model as a pickle string.\n",
    "saved_model = pickle.dumps(gb_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted_Price = gb_model1.predict(data_test2)\n",
    "Predicted_Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38040d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "prediction_plt = gb_model1.predict(x_testsc)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(y_test, prediction_plt, c='crimson')\n",
    "p1 = max(max(prediction_plt), max(y_test))\n",
    "p2 = min(min(prediction_plt), min(y_test))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.xlabel('Actual', fontsize=15)\n",
    "plt.ylabel('Predicted', fontsize=15)\n",
    "plt.title(gb_model1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = pd.DataFrame()\n",
    "Prediction['Price'] = Predicted_Price\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc5b15",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The important variables impacting the sales are :\n",
    "'MSZoning', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType',\n",
    "'SaleCondition', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallCond', 'MasVnrArea', 'BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', 'KitchenAbvGr', 'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice'\n",
    "\n",
    "The above gradient boosting alorithm is used to predict the Sales Price."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Housing Price.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
