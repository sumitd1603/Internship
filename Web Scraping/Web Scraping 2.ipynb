{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cb4b64",
   "metadata": {},
   "source": [
    "This notebook is for the answers to assignment no.2 of web scrapping using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064b2e0",
   "metadata": {},
   "source": [
    "Instructions\n",
    "1. All the questions must be done in a single Jupyter notebook.\n",
    "2. There should be proper comments in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91786884",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6eab00",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4478600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import selenium\n",
    "from selenium import webdriver as wd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608863f",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fe1a7",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcb723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "search_title= driver.find_element_by_class_name(\"suggestor-input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\") # to find Location field\n",
    "search_loc\n",
    "\n",
    "search_title.send_keys(\"Data Analyst\") # Entering Values in Search Title\n",
    "search_loc.send_keys(\"Bangalore\")         # Entering Value in Location field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795dfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search button using absolute Xpath\n",
    "search= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa78544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Job Titles\n",
    "\n",
    "job_titles= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "job=[]\n",
    "for i in job_titles:\n",
    "    job.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting companies\n",
    "\n",
    "job_company= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company=[]\n",
    "for i in job_company:\n",
    "    company.append(i.text)\n",
    "\n",
    "# Extracting location and experience\n",
    "\n",
    "job_loc= driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16 \"]')\n",
    "\n",
    "col=[]\n",
    "for i in job_loc:\n",
    "    col.append(i.text)\n",
    "\n",
    "loc=col[3:42:3] + col[46:55:3]\n",
    "exp=col[1:41:3] + col[44:55:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f36d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0                           Sr.Business Data Analyst   \n",
      "1                                    Sr Data Analyst   \n",
      "2                       Senior Data Analysis Analyst   \n",
      "3  Job opportunity For Data Analyst at Trellance ...   \n",
      "4                                       Data Analyst   \n",
      "5                                Senior Data Analyst   \n",
      "6                             Associate Data Analyst   \n",
      "7                             Associate Data Analyst   \n",
      "8                   Data Analyst/Senior Data Analyst   \n",
      "9                             Financial Data Analyst   \n",
      "\n",
      "                      Company                              Location Experience  \n",
      "0                   Collabera                   Bangalore/Bengaluru    5-8 Yrs  \n",
      "1             Thomson Reuters    Bangalore/Bengaluru, Pune, Chennai   7-12 Yrs  \n",
      "2                       Capco        Bangalore/Bengaluru, Ahmedabad    0-2 Yrs  \n",
      "3  CURise Analytics Pvt. Ltd.                   Bangalore/Bengaluru    2-3 Yrs  \n",
      "4             Thomson Reuters  Bangalore/Bengaluru(Old Madras Road)    3-6 Yrs  \n",
      "5                    KrazyBee                   Bangalore/Bengaluru    2-7 Yrs  \n",
      "6                       Optum                   Bangalore/Bengaluru    1-4 Yrs  \n",
      "7                       Optum                   Bangalore/Bengaluru    3-6 Yrs  \n",
      "8                      Meesho                   Bangalore/Bengaluru    3-8 Yrs  \n",
      "9                Nuance India                   Bangalore/Bengaluru    2-4 Yrs  \n"
     ]
    }
   ],
   "source": [
    "table= pd.DataFrame({\"Job Title\":job[0:10], \"Company\":company[0:10], \"Location\":loc[0:10], \"Experience\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7db5c9",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4e5b5",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad8a90",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942eae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "search_title= driver.find_element_by_class_name(\"suggestor-input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\") # to find Location field\n",
    "search_loc\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title\n",
    "search_loc.send_keys(\"Bangalore\")         # Entering Value in Location field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7cec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search button using absolute Xpath\n",
    "search= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0be94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Job Titles\n",
    "\n",
    "job_titles= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "job=[]\n",
    "for i in job_titles:\n",
    "    job.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting companies\n",
    "\n",
    "job_company= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company=[]\n",
    "for i in job_company:\n",
    "    company.append(i.text)\n",
    "\n",
    "# Extracting location and experience\n",
    "\n",
    "job_loc= driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16 \"]')\n",
    "\n",
    "col=[]\n",
    "for i in job_loc:\n",
    "    col.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "# Splitting location and Experience in different lists\n",
    "\n",
    "\n",
    "\n",
    "loc= col[2:9:3]+ col[12:30:3] + col[34:55:3]\n",
    "exp= col[0:7:3] + col[10:29:3] + col[32:54:3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f865b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title             Company  \\\n",
      "0              Data Scientist/ Senior Data Scientist   Fractal Analytics   \n",
      "1  Urgent Job Opening For AI Practitioner - Data ...               Wipro   \n",
      "2                                 Dataiku Consultant               Wipro   \n",
      "3                                     Data Scientist   Applied Materials   \n",
      "4  Data & Analytics Tech - Informatica Cloud- Sen...                 PwC   \n",
      "5                 Data Scientist: Advanced Analytics                 IBM   \n",
      "6                                 Research Scientist                 IBM   \n",
      "7   Data Science - Senior Data Scientist - Analytics               Paytm   \n",
      "8                         Principal - Data Scientist  Schneider Electric   \n",
      "9            Research and Development -AI/ML -(PhD )                 EXL   \n",
      "\n",
      "                                            Location     Experience  \n",
      "0  Bangalore/Bengaluru, Pune, Gurgaon/Gurugram, C...        3-8 Yrs  \n",
      "1  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...      11-21 Yrs  \n",
      "2                 Bangalore/Bengaluru, Pune, Chennai      10-16 Yrs  \n",
      "3                                           6-10 Yrs  Not disclosed  \n",
      "4                                            2-7 Yrs  Not disclosed  \n",
      "5                                            3-7 Yrs  Not disclosed  \n",
      "6                                            3-5 Yrs  Not disclosed  \n",
      "7                                           7-10 Yrs  Not disclosed  \n",
      "8                                            4-8 Yrs  Not disclosed  \n",
      "9                                      Not disclosed  Not disclosed  \n"
     ]
    }
   ],
   "source": [
    "table= pd.DataFrame({\"Job Title\":job[0:10], \"Company\":company[0:10], \"Location\":loc[0:10], \"Experience\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8b371",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b6bd7",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92838a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "search_title= driver.find_element_by_class_name(\"suggestor-input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\") # to find Location field\n",
    "search_loc\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title\n",
    "\n",
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Filters\n",
    "# Checking location Delhi/NCR\n",
    "loc_chk= driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\")\n",
    "loc_chk.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking salary 3-6 lakhs\n",
    "sal_chk= driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]')\n",
    "sal_chk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ba621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Job Titles\n",
    "\n",
    "job_titles= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "job=[]\n",
    "for i in job_titles:\n",
    "    job.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting companies\n",
    "\n",
    "job_company= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company=[]\n",
    "for i in job_company:\n",
    "    company.append(i.text)\n",
    "\n",
    "# Extracting location and experience\n",
    "\n",
    "loc_exp= driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16 \"]')\n",
    "\n",
    "col=[]\n",
    "for i in loc_exp:\n",
    "    col.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "# Splitting location and Experience in different lists\n",
    "\n",
    "    \n",
    "loc= col[2:8:3] + col[12:28:3] +  col[31:55:3]\n",
    "exp= col[0:6:3] + col[10:25:3] + col[29:55:3]\n",
    "\n",
    "table= pd.DataFrame({\"Job Title\":job[0:10], \"Company\":company[0:10], \"Location\":loc[0:10], \"Experience\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca82540",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0e909",
   "metadata": {},
   "source": [
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "    \n",
    "1. Brand\n",
    "\n",
    "2. Product Description\n",
    "\n",
    "3. Price\n",
    "\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "\n",
    "5. Now scrape data from this page as usual\n",
    "\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88b4ba",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71431358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the login in pop-up\n",
    "close= driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To find the web element - search title field\n",
    "search_title= driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\") \n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"sunglasses\") # Entering Values in Search Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod=[]\n",
    "for i in prod_rows:\n",
    "    prod.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc=[]\n",
    "for i in disc_rows:\n",
    "    disc.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span\")\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand2=[]\n",
    "for i in brand_title:\n",
    "    brand2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod2=[]\n",
    "for i in prod_rows:\n",
    "    prod2.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price2=[]\n",
    "for i in price_rows:\n",
    "    price2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc2=[]\n",
    "for i in disc_rows:\n",
    "    disc2.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c579e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand3=[]\n",
    "for i in brand_title:\n",
    "    brand3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod3=[]\n",
    "for i in prod_rows:\n",
    "    prod3.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price3=[]\n",
    "for i in price_rows:\n",
    "    price3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc3=[]\n",
    "for i in disc_rows:\n",
    "    disc3.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385df288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comabining results and limiting records till 100 rows\n",
    "brands=brand+brand2+brand3[0:20]\n",
    "prices=price+price2+price3[0:20]\n",
    "prods=prod+prod2+prod3[0:20]\n",
    "discs=disc+disc2+disc3[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce706d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brands,\"Product Description\":prods,\"Price\":prices,\"Discount\":discs})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1715da3",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb023088",
   "metadata": {},
   "source": [
    "Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "\n",
    "2. Enter “iphone 11” in “Search” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "You will reach to the below shown webpage\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "\n",
    "1. Rating\n",
    "\n",
    "2. Review summary\n",
    "\n",
    "3. Full review\n",
    "\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6cb5b",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c63894c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'        # Defining the url\n",
    "\n",
    "driver.get(url)                       # opening the url in our test chrome explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "37e5bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings=[]\n",
    "for i in rating_row:\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review=[]\n",
    "for i in review_sum:\n",
    "    review.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "\n",
    "full=[]\n",
    "for i in review_full:\n",
    "    full.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b48c9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings),len(review),len(full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "abdcaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]/span')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5392048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings2=[]\n",
    "for i in rating_row:\n",
    "    ratings2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review2=[]\n",
    "for i in review_sum:\n",
    "    review2.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full2=[]\n",
    "for i in review_full:\n",
    "    full2.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "beb2c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings2),len(review2),len(full2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b19d4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "47bb651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings3=[]\n",
    "for i in rating_row:\n",
    "    ratings3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review3=[]\n",
    "for i in review_sum:\n",
    "    review3.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full3=[]\n",
    "for i in review_full:\n",
    "    full3.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "040ec3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings3),len(review3),len(full3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5ebfb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c1464d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings4=[]\n",
    "for i in rating_row:\n",
    "    ratings4.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review4=[]\n",
    "for i in review_sum:\n",
    "    review4.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full4=[]\n",
    "for i in review_full:\n",
    "    full4.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7b84c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings4),len(review4),len(full4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0f4d3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b45a5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings5=[]\n",
    "for i in rating_row:\n",
    "    ratings5.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review5=[]\n",
    "for i in review_sum:\n",
    "    review5.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full5=[]\n",
    "for i in review_full:\n",
    "    full5.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "85a41ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings5),len(review5),len(full5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c5ae5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "95aef934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings6=[]\n",
    "for i in rating_row:\n",
    "    ratings6.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review6=[]\n",
    "for i in review_sum:\n",
    "    review6.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full6=[]\n",
    "for i in review_full:\n",
    "    full6.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ff99b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings6),len(review6),len(full6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2629ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b1b102b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings7=[]\n",
    "for i in rating_row:\n",
    "    ratings7.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review7=[]\n",
    "for i in review_sum:\n",
    "    review7.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full7=[]\n",
    "for i in review_full:\n",
    "    full7.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "908c65e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings7),len(review7),len(full7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5fc4b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d1d6c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings8=[]\n",
    "for i in rating_row:\n",
    "    ratings8.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review8=[]\n",
    "for i in review_sum:\n",
    "    review8.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full8=[]\n",
    "for i in review_full:\n",
    "    full8.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cc311894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings8),len(review8),len(full8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "77301581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b9b1374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings9=[]\n",
    "for i in rating_row:\n",
    "    ratings9.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review9=[]\n",
    "for i in review_sum:\n",
    "    review9.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full9=[]\n",
    "for i in review_full:\n",
    "    full9.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "269401b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings9),len(review9),len(full9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a3cd2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7a800bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings10=[]\n",
    "for i in rating_row:\n",
    "    ratings10.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review10=[]\n",
    "for i in review_sum:\n",
    "    review10.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full10=[]\n",
    "for i in review_full:\n",
    "    full10.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8fefeb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings10),len(review10),len(full10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4f85eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8fbca233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings11=[]\n",
    "for i in rating_row:\n",
    "    ratings11.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review11=[]\n",
    "for i in review_sum:\n",
    "    review11.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full11=[]\n",
    "for i in review_full:\n",
    "    full11.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f29b9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_ratings= ratings+ratings2+ratings3+ratings4+ratings5+ratings6+ratings7+ratings8+ratings9+ratings10+ratings11\n",
    "tot_reviews= review+review2+review3+review4+review5+review6+review7+review8+review9+review10+review11\n",
    "tot_full= full+full2+full3+full4+full5+full6+full7+full8+full9+full10+full11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7e1a42ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 103 103\n"
     ]
    }
   ],
   "source": [
    "print(len(tot_ratings),len(tot_reviews),len(tot_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "94592be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ratings      Reviews Summary  \\\n",
      "0        5       Simply awesome   \n",
      "1        5     Perfect product!   \n",
      "2        5  Best in the market!   \n",
      "3        5   Highly recommended   \n",
      "4        5    Worth every penny   \n",
      "..     ...                  ...   \n",
      "95       5               Super!   \n",
      "96       5            Wonderful   \n",
      "97       5    Worth every penny   \n",
      "98       5  Best in the market!   \n",
      "99       4            Fabulous!   \n",
      "\n",
      "                                  Reviews Description  \n",
      "0   Really satisfied with the Product I received.....  \n",
      "1   Amazing phone with great cameras and better ba...  \n",
      "2   Great iPhone very snappy experience as apple k...  \n",
      "3   What a camera .....just awesome ..you can feel...  \n",
      "4   Previously I was using one plus 3t it was a gr...  \n",
      "..                                                ...  \n",
      "95  This is my first ever iPhone.\\nAnd I truly don...  \n",
      "96  Nice value for money good and best price I pho...  \n",
      "97  Undoubtedly Iphone 11 is the most successful m...  \n",
      "98  Don’t expect much from front camera… especiall...  \n",
      "99  I purchased the iPhone 11 a month back. I must...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Ratings\":tot_ratings[0:100],\"Reviews Summary\":tot_reviews[0:100],\"Reviews Description\":tot_full[0:100]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1679700",
   "metadata": {},
   "source": [
    "# Question 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a25a2",
   "metadata": {},
   "source": [
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "\n",
    "1. Brand\n",
    "\n",
    "2. Product Description\n",
    "\n",
    "3. Price\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c1b94",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f718b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "# closing the login in pop-up\n",
    "close= driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "close.click()\n",
    "\n",
    "\n",
    "# To find the web element - search title field\n",
    "search_title= driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\") \n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"sneakers\") # Entering Values in Search Title\n",
    "\n",
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d99c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod=[]\n",
    "for i in prod_rows:\n",
    "    prod.append(i.text)\n",
    "    \n",
    "prod_1=[]\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa _2-ICcC\"]')\n",
    "for i in prod_rows:\n",
    "    prod_1.append(i.text)   \n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc=[]\n",
    "for i in disc_rows:\n",
    "    disc.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span\")\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand2=[]\n",
    "for i in brand_title:\n",
    "    brand2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod2=[]\n",
    "for i in prod_rows:\n",
    "    prod2.append(i.text)\n",
    "    \n",
    "prod2_1=[]\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa _2-ICcC\"]')\n",
    "for i in prod_rows:\n",
    "    prod2_1.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price2=[]\n",
    "for i in price_rows:\n",
    "    price2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc2=[]\n",
    "for i in disc_rows:\n",
    "    disc2.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d25a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand3=[]\n",
    "for i in brand_title:\n",
    "    brand3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod3=[]\n",
    "for i in prod_rows:\n",
    "    prod3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price3=[]\n",
    "for i in price_rows:\n",
    "    price3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc3=[]\n",
    "for i in disc_rows:\n",
    "    disc3.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comabining results and limiting records till 100 rows\n",
    "brands=brand+brand2+brand3[0:20]\n",
    "prices=price+price2+price3[0:20]\n",
    "prods=prod+prod_1+prod2+prod2_1+prod3[0:20]\n",
    "discs=disc+disc2+disc3[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f90209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brands,\"Product Description\":prods,\"Price\":prices,\"Discount\":discs})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599b698",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679ffe0",
   "metadata": {},
   "source": [
    "Go to the link - https://www.myntra.com/shoes\n",
    "\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.myntra.com/shoes'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "\n",
    "# Using Filters\n",
    "# Checking Black color filters\n",
    "col_chk= driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/span[1]\")\n",
    "col_chk.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Filters\n",
    "# Checking price filters\n",
    "col_chk= driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "col_chk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_class_name(\"product-brand\")\n",
    "\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_class_name(\"product-product\")\n",
    "\n",
    "prod=[]\n",
    "for i in prod_rows:\n",
    "    prod.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_class_name(\"product-price\")\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking next button\n",
    "nextb= driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[12]/a\")\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b82de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_class_name(\"product-brand\")\n",
    "\n",
    "brand2=[]\n",
    "for i in brand_title:\n",
    "    brand2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_class_name(\"product-product\")\n",
    "\n",
    "prod2=[]\n",
    "for i in prod_rows:\n",
    "    prod2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_class_name(\"product-price\")\n",
    "\n",
    "price2=[]\n",
    "for i in price_rows:\n",
    "    price2.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand2),len(prod2),len(price2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comabining results and limiting records till 100 rows\n",
    "brands=brand+brand2\n",
    "prices=price+price2\n",
    "prods=prod+prod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brands,\"Product Description\":prods,\"Price\":prices})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f01765",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64712114",
   "metadata": {},
   "source": [
    "Go to webpage https://www.amazon.in/\n",
    "\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "\n",
    "2. Ratings\n",
    "\n",
    "3. Price\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9cdc0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.amazon.in'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3492e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the web element - search title field\n",
    "search_title= driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\") \n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Laptop\") # Entering Values in Search Title\n",
    "\n",
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a82d9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on Intel Core i7 filter\n",
    "\n",
    "cpu_chk= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[2]/li[14]/span/a/span\")\n",
    "\n",
    "cpu_chk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b47ab3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4d5a1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "ratings_rows=driver.find_elements_by_xpath('//span[@class=\"a-icon-alt\"]')\n",
    "\n",
    "ratings=[]\n",
    "for i in ratings_rows:\n",
    "    ratings.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5883b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3d412c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Brand Product Rating   Price\n",
      "0  Dell New Inspiron 3521 Laptop, Intel Pqc-N5030...                 30,490\n",
      "1  Acer Extensa 15 Thin & Light Intel Processor P...                 26,900\n",
      "2  Hp 14S-Intel Pentium Silver N6000- 8Gb Ram/256...                 31,647\n",
      "3  Microsoft Surface GO 3 8VA-00013 10.5\" (26.67 ...                 49,936\n",
      "4  ASUS VivoBook 14 (2021), 14-inch (35.56 cm) HD...                 30,450\n",
      "5  ASUS Windows 11 Home VivoBook 13 Slate OLED, I...                 63,980\n",
      "6  Acer Travelmate Intel® Pentium® Gold 7505 Proc...                 28,990\n",
      "7  Acer Travelmate Business Laptop Intel Pentium ...                 25,990\n",
      "8  ASUS EeeBook 14 Pentium Silver - (8 GB/256 GB ...                 31,990\n",
      "9  (Renewed) ASUS VivoBook 13 Slate OLED, Intel P...                 34,392\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brand[0:10],\"Product Rating\":ratings[0:10],\"Price\":price[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb76e6a",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5385ee4",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "\n",
    "2. Click on the Job option as shown in the image\n",
    "\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98259ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.ambitionbox.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on Jobs tab\n",
    "job_tab= driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[6]\")\n",
    "job_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd552f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title\n",
    "\n",
    "# click search button\n",
    "search_btn= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/i\") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Noida\") # Entering Values in Search Title\n",
    "\n",
    "# click search button\n",
    "search_btn= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[5]/div/label\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9835fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Company Name\n",
    "\n",
    "company_title= driver.find_elements_by_xpath('//div[@class=\"company-info\"]')\n",
    "company=[]\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Posting duration\n",
    "\n",
    "post_rows= driver.find_elements_by_xpath('//span[@class=\"body-small-l\"]')\n",
    "\n",
    "post=[]\n",
    "for i in post_rows:\n",
    "    post.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d52613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Company\":company[0:10],\"Job Posting\":post[0:20:2]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f392448",
   "metadata": {},
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c817a8",
   "metadata": {},
   "source": [
    "Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "\n",
    "2. Click on the salaries option as shown in the image.\n",
    "\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "then click on “Data Scientist”.\n",
    "\n",
    "You have to scrape the data ticked in the above image.\n",
    "\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "\n",
    "5. Store the data in a dataframe.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.ambitionbox.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on Salaries tab\n",
    "sal_tab= driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[4]\")\n",
    "sal_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe791dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title= driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input\") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking Data Scientist from suggestion\n",
    "sel_btn= driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p\")\n",
    "sel_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b15c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Company Name\n",
    "\n",
    "company_title= driver.find_elements_by_tag_name('a')\n",
    "company=[]\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Total Salary record\n",
    "\n",
    "total_rows= driver.find_elements_by_xpath('//span[@class=\"datapoints\"]')\n",
    "\n",
    "sal_tot=[]\n",
    "for i in total_rows:\n",
    "    sal_tot.append(i.text)\n",
    "    \n",
    "# Extracting Minimum Salary record\n",
    "\n",
    "min_rows= driver.find_elements_by_xpath('//div[@class=\"value body-medium\"]')\n",
    "\n",
    "sal=[]\n",
    "for i in min_rows:\n",
    "    sal.append(i.text)\n",
    "    \n",
    "# Extracting Average Salary \n",
    "\n",
    "avg_rows= driver.find_elements_by_class_name(\"averageCtc\")\n",
    "\n",
    "sal_avg=[]\n",
    "for i in avg_rows:\n",
    "    sal_avg.append(i.text)\n",
    "\n",
    "    \n",
    "# Extracting Experience required\n",
    "\n",
    "exp_reqd= driver.find_elements_by_xpath('//div[@class=\"sbold-list-header\"]')\n",
    "\n",
    "exp=[]\n",
    "for i in exp_reqd:\n",
    "    exp.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sal=sal[0:19:2]\n",
    "max_sal=sal[1::2]\n",
    "company=company[15:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Company\":company[0:10],\"Salary Records\":sal_tot[0:10],\"Minimum Salary\":min_sal,\"Average Salary\":min_sal[0:10],\"Maximum Salary\":max_sal,\"Experience Required\":exp[0:10]})\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
