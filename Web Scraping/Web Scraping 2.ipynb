{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cb4b64",
   "metadata": {},
   "source": [
    "This notebook is for the answers to assignment no.2 of web scrapping using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064b2e0",
   "metadata": {},
   "source": [
    "Instructions\n",
    "1. All the questions must be done in a single Jupyter notebook.\n",
    "2. There should be proper comments in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91786884",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6eab00",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4478600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import selenium\n",
    "from selenium import webdriver as wd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608863f",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fe1a7",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcb723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "search_title= driver.find_element_by_class_name(\"suggestor-input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\") # to find Location field\n",
    "search_loc\n",
    "\n",
    "search_title.send_keys(\"Data Analyst\") # Entering Values in Search Title\n",
    "search_loc.send_keys(\"Bangalore\")         # Entering Value in Location field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795dfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search button using absolute Xpath\n",
    "search= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa78544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Job Titles\n",
    "\n",
    "job_titles= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "job=[]\n",
    "for i in job_titles:\n",
    "    job.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting companies\n",
    "\n",
    "job_company= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company=[]\n",
    "for i in job_company:\n",
    "    company.append(i.text)\n",
    "\n",
    "# Extracting location and experience\n",
    "\n",
    "job_loc= driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16 \"]')\n",
    "\n",
    "col=[]\n",
    "for i in job_loc:\n",
    "    col.append(i.text)\n",
    "\n",
    "loc=col[3:42:3] + col[46:55:3]\n",
    "exp=col[1:41:3] + col[44:55:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f36d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0                                     Data Analyst I   \n",
      "1  Data Science / Data Engineer / Business Analys...   \n",
      "2                           Sr.Business Data Analyst   \n",
      "3                                Senior Data Analyst   \n",
      "4                                    Sr Data Analyst   \n",
      "5             Data Analyst - IIM/ISB/MDI/FMS/SP Jain   \n",
      "6  Data Analyst / Business analyst - US MNC (anal...   \n",
      "7                      Business Analyst/Data Analyst   \n",
      "8                                Junior Data Analyst   \n",
      "9                       Senior Data Analysis Analyst   \n",
      "\n",
      "                                  Company                   Location  \\\n",
      "0                                  Cerner                    0-3 Yrs   \n",
      "1  NETWORTH DATA PRODUCTS PRIVATE LIMITED              Not disclosed   \n",
      "2                               Collabera        Bangalore/Bengaluru   \n",
      "3               Hudsons bay Company (HBC)        Bangalore/Bengaluru   \n",
      "4                         Thomson Reuters        Bangalore/Bengaluru   \n",
      "5             K12 Techno Services Pvt Ltd              Not disclosed   \n",
      "6                      Aspyra HR Services              Not disclosed   \n",
      "7                    Telamon HR Solutions  10,00,000 - 20,00,000 PA.   \n",
      "8                                ICF Next              Not disclosed   \n",
      "9                                   Capco    1,75,000 - 4,75,000 PA.   \n",
      "\n",
      "                                          Experience  \n",
      "0                                      Not disclosed  \n",
      "1                            4,00,000 - 6,50,000 PA.  \n",
      "2                                            3-4 Yrs  \n",
      "3                                            5-8 Yrs  \n",
      "4                                            4-9 Yrs  \n",
      "5                          10,00,000 - 20,00,000 PA.  \n",
      "6  Bangalore/Bengaluru, Hyderabad/Secunderabad, D...  \n",
      "7                     Bangalore/Bengaluru, New Delhi  \n",
      "8                 Bangalore/Bengaluru, Pune, Chennai  \n",
      "9                                Bangalore/Bengaluru  \n"
     ]
    }
   ],
   "source": [
    "table= pd.DataFrame({\"Job Title\":job[0:10], \"Company\":company[0:10], \"Location\":loc[0:10], \"Experience\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7db5c9",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4e5b5",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad8a90",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942eae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "search_title= driver.find_element_by_class_name(\"suggestor-input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\") # to find Location field\n",
    "search_loc\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title\n",
    "search_loc.send_keys(\"Bangalore\")         # Entering Value in Location field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7cec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search button using absolute Xpath\n",
    "search= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0be94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Job Titles\n",
    "\n",
    "job_titles= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "job=[]\n",
    "for i in job_titles:\n",
    "    job.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting companies\n",
    "\n",
    "job_company= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company=[]\n",
    "for i in job_company:\n",
    "    company.append(i.text)\n",
    "\n",
    "# Extracting location and experience\n",
    "\n",
    "job_loc= driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16 \"]')\n",
    "\n",
    "col=[]\n",
    "for i in job_loc:\n",
    "    col.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "# Splitting location and Experience in different lists\n",
    "\n",
    "\n",
    "\n",
    "loc= col[2:9:3]+ col[12:30:3] + col[34:55:3]\n",
    "exp= col[0:7:3] + col[10:29:3] + col[32:54:3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f865b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0  Urgent Job Opening For AI Practitioner - Data ...   \n",
      "1                   Hiring For Senior Data Scientist   \n",
      "2                              Senior Data Scientist   \n",
      "3                                 Dataiku Consultant   \n",
      "4            Research and Development -AI/ML -(PhD )   \n",
      "5  Opportunity For Data Scientist - Female Candid...   \n",
      "6                       Senior Data Science Engineer   \n",
      "7                 Data Science - Engineering Manager   \n",
      "8                                     Data Scientist   \n",
      "9  Data & Analytics Tech - Informatica Cloud- Sen...   \n",
      "\n",
      "                           Company  \\\n",
      "0                            Wipro   \n",
      "1  TATA CONSULTANCY SERVICES (TCS)   \n",
      "2                       Spiceworks   \n",
      "3                            Wipro   \n",
      "4                              EXL   \n",
      "5                             PayU   \n",
      "6                Fractal Analytics   \n",
      "7                            Paytm   \n",
      "8                Applied Materials   \n",
      "9                              PwC   \n",
      "\n",
      "                                            Location     Experience  \n",
      "0  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...      11-21 Yrs  \n",
      "1                          Bangalore/Bengaluru, Pune       8-13 Yrs  \n",
      "2  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...        5-7 Yrs  \n",
      "3                                            4-8 Yrs  Not disclosed  \n",
      "4                                            1-3 Yrs  Not disclosed  \n",
      "5                                            5-9 Yrs  Not disclosed  \n",
      "6                                           9-13 Yrs  Not disclosed  \n",
      "7                                            4-7 Yrs  Not disclosed  \n",
      "8                                           6-10 Yrs  Not disclosed  \n",
      "9                                      Not disclosed  Not disclosed  \n"
     ]
    }
   ],
   "source": [
    "table= pd.DataFrame({\"Job Title\":job[0:10], \"Company\":company[0:10], \"Location\":loc[0:10], \"Experience\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8b371",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b6bd7",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92838a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "search_title= driver.find_element_by_class_name(\"suggestor-input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\") # to find Location field\n",
    "search_loc\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title\n",
    "\n",
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f482b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Filters\n",
    "# Checking location Delhi/NCR\n",
    "loc_chk= driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\")\n",
    "loc_chk.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f8dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking salary 3-6 lakhs\n",
    "sal_chk= driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]')\n",
    "sal_chk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5ba621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Job Title  \\\n",
      "0             DigitalBCG GAMMA Data Scientist   \n",
      "1            Data Scientist - Noida/Bangalore   \n",
      "2             Senior Associate - Data Science   \n",
      "3  Data Scientist For Healthcare Product team   \n",
      "4  Data Scientist For Healthcare Product team   \n",
      "5              Data Scientist - MIND Infotech   \n",
      "6                      Data Science Associate   \n",
      "7           Data Scientist - Engine Algorithm   \n",
      "8                    Knowledge/Data Scientist   \n",
      "9                              Data Scientist   \n",
      "\n",
      "                                    Company                        Location  \\\n",
      "0                   Boston Consulting Group  New Delhi, Bangalore/Bengaluru   \n",
      "1                                       EXL      Noida, Bangalore/Bengaluru   \n",
      "2                              Black Turtle                         2-7 Yrs   \n",
      "3                  SECUREKLOUD TECHNOLOGIES                         4-8 Yrs   \n",
      "4                  SECUREKLOUD TECHNOLOGIES                         2-4 Yrs   \n",
      "5  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED                         1-3 Yrs   \n",
      "6                             Kreate Energy                         3-6 Yrs   \n",
      "7                              Primo Hiring                         2-4 Yrs   \n",
      "8                   BOLD Technology Systems                   Not disclosed   \n",
      "9   Mount Talent Consulting Private Limited                         2-5 Yrs   \n",
      "\n",
      "                               Experience  \n",
      "0                                 2-5 Yrs  \n",
      "1                                5-10 Yrs  \n",
      "2                           Not disclosed  \n",
      "3                           Not disclosed  \n",
      "4                           Not disclosed  \n",
      "5                 4,00,000 - 8,00,000 PA.  \n",
      "6                           Not disclosed  \n",
      "7  Delhi / NCR, Pune, Bangalore/Bengaluru  \n",
      "8  Delhi / NCR, Pune, Bangalore/Bengaluru  \n",
      "9                           Not disclosed  \n"
     ]
    }
   ],
   "source": [
    "# Extracting Job Titles\n",
    "\n",
    "job_titles= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "job=[]\n",
    "for i in job_titles:\n",
    "    job.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting companies\n",
    "\n",
    "job_company= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company=[]\n",
    "for i in job_company:\n",
    "    company.append(i.text)\n",
    "\n",
    "# Extracting location and experience\n",
    "\n",
    "loc_exp= driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16 \"]')\n",
    "\n",
    "col=[]\n",
    "for i in loc_exp:\n",
    "    col.append(i.text)\n",
    "    \n",
    "  \n",
    "    \n",
    "# Splitting location and Experience in different lists\n",
    "\n",
    "    \n",
    "loc= col[2:8:3] + col[12:28:3] +  col[31:55:3]\n",
    "exp= col[0:6:3] + col[10:25:3] + col[29:55:3]\n",
    "\n",
    "table= pd.DataFrame({\"Job Title\":job[0:10], \"Company\":company[0:10], \"Location\":loc[0:10], \"Experience\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca82540",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0e909",
   "metadata": {},
   "source": [
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "    \n",
    "1. Brand\n",
    "\n",
    "2. Product Description\n",
    "\n",
    "3. Price\n",
    "\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "\n",
    "2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and\n",
    "click the search icon\n",
    "\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "\n",
    "4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom other page , then\n",
    "click on it.\n",
    "\n",
    "5. Now scrape data from this page as usual\n",
    "\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88b4ba",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71431358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69ee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the login in pop-up\n",
    "close= driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b6d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To find the web element - search title field\n",
    "search_title= driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\") \n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"sunglasses\") # Entering Values in Search Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0199fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1e6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod=[]\n",
    "for i in prod_rows:\n",
    "    prod.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc=[]\n",
    "for i in disc_rows:\n",
    "    disc.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4acfdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span\")\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb8c19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand2=[]\n",
    "for i in brand_title:\n",
    "    brand2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod2=[]\n",
    "for i in prod_rows:\n",
    "    prod2.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price2=[]\n",
    "for i in price_rows:\n",
    "    price2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc2=[]\n",
    "for i in disc_rows:\n",
    "    disc2.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f07f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c579e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand3=[]\n",
    "for i in brand_title:\n",
    "    brand3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod3=[]\n",
    "for i in prod_rows:\n",
    "    prod3.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price3=[]\n",
    "for i in price_rows:\n",
    "    price3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc3=[]\n",
    "for i in disc_rows:\n",
    "    disc3.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "385df288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comabining results and limiting records till 100 rows\n",
    "brands=brand+brand2+brand3[0:20]\n",
    "prices=price+price2+price3[0:20]\n",
    "prods=prod+prod2+prod3[0:20]\n",
    "discs=disc+disc2+disc3[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ce706d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Brand                                Product Description   Price  \\\n",
      "0   VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...    ‚Çπ749   \n",
      "1   VINCENT CHASE          UV Protection Rectangular Sunglasses (52)    ‚Çπ649   \n",
      "2        DAHAAZIL  UV Protection, Night Vision, Riding Glasses Wa...    ‚Çπ177   \n",
      "3       New Specs   UV Protection Rectangular Sunglasses (Free Size)    ‚Çπ264   \n",
      "4          PIRASO              UV Protection Aviator Sunglasses (54)    ‚Çπ249   \n",
      "..            ...                                                ...     ...   \n",
      "95      ROYAL SON    Polarized, UV Protection Sports Sunglasses (68)  ‚Çπ1,234   \n",
      "96      ROYAL SON  UV Protection, Gradient Butterfly Sunglasses (62)    ‚Çπ699   \n",
      "97   Silver Kartz      UV Protection Wayfarer Sunglasses (Free Size)    ‚Çπ288   \n",
      "98         GANSTA              UV Protection Aviator Sunglasses (57)    ‚Çπ314   \n",
      "99      ROYAL SON  Polarized, UV Protection Retro Square Sunglass...    ‚Çπ664   \n",
      "\n",
      "   Discount  \n",
      "0   62% off  \n",
      "1   67% off  \n",
      "2   82% off  \n",
      "3   89% off  \n",
      "4   84% off  \n",
      "..      ...  \n",
      "95  50% off  \n",
      "96  65% off  \n",
      "97  80% off  \n",
      "98  84% off  \n",
      "99  55% off  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brands,\"Product Description\":prods,\"Price\":prices,\"Discount\":discs})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1715da3",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb023088",
   "metadata": {},
   "source": [
    "Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "\n",
    "2. Enter ‚Äúiphone 11‚Äù in ‚ÄúSearch‚Äù field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "You will reach to the below shown webpage\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "\n",
    "1. Rating\n",
    "\n",
    "2. Review summary\n",
    "\n",
    "3. Full review\n",
    "\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6cb5b",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c63894c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'        # Defining the url\n",
    "\n",
    "driver.get(url)                       # opening the url in our test chrome explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37e5bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings=[]\n",
    "for i in rating_row:\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review=[]\n",
    "for i in review_sum:\n",
    "    review.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "\n",
    "full=[]\n",
    "for i in review_full:\n",
    "    full.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abdcaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]/span')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5392048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings2=[]\n",
    "for i in rating_row:\n",
    "    ratings2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review2=[]\n",
    "for i in review_sum:\n",
    "    review2.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full2=[]\n",
    "for i in review_full:\n",
    "    full2.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b19d4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47bb651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings3=[]\n",
    "for i in rating_row:\n",
    "    ratings3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review3=[]\n",
    "for i in review_sum:\n",
    "    review3.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full3=[]\n",
    "for i in review_full:\n",
    "    full3.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ebfb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1464d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings4=[]\n",
    "for i in rating_row:\n",
    "    ratings4.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review4=[]\n",
    "for i in review_sum:\n",
    "    review4.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full4=[]\n",
    "for i in review_full:\n",
    "    full4.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f4d3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b45a5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings5=[]\n",
    "for i in rating_row:\n",
    "    ratings5.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review5=[]\n",
    "for i in review_sum:\n",
    "    review5.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full5=[]\n",
    "for i in review_full:\n",
    "    full5.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5ae5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95aef934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings6=[]\n",
    "for i in rating_row:\n",
    "    ratings6.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review6=[]\n",
    "for i in review_sum:\n",
    "    review6.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full6=[]\n",
    "for i in review_full:\n",
    "    full6.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2629ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1b102b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings7=[]\n",
    "for i in rating_row:\n",
    "    ratings7.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review7=[]\n",
    "for i in review_sum:\n",
    "    review7.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full7=[]\n",
    "for i in review_full:\n",
    "    full7.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fc4b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1d6c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings8=[]\n",
    "for i in rating_row:\n",
    "    ratings8.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review8=[]\n",
    "for i in review_sum:\n",
    "    review8.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full8=[]\n",
    "for i in review_full:\n",
    "    full8.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77301581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9b1374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings9=[]\n",
    "for i in rating_row:\n",
    "    ratings9.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review9=[]\n",
    "for i in review_sum:\n",
    "    review9.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full9=[]\n",
    "for i in review_full:\n",
    "    full9.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3cd2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a800bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings10=[]\n",
    "for i in rating_row:\n",
    "    ratings10.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review10=[]\n",
    "for i in review_sum:\n",
    "    review10.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full10=[]\n",
    "for i in review_full:\n",
    "    full10.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f85eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fbca233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "\n",
    "rating_row= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "\n",
    "ratings11=[]\n",
    "for i in rating_row:\n",
    "    ratings11.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Review Summary\n",
    "\n",
    "review_sum= driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "\n",
    "review11=[]\n",
    "for i in review_sum:\n",
    "    review11.append(i.text)\n",
    "    \n",
    "# Extracting Full Reeview \n",
    "\n",
    "review_full= driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "\n",
    "full11=[]\n",
    "for i in review_full:\n",
    "    full11.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f29b9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_ratings= ratings+ratings2+ratings3+ratings4+ratings5+ratings6+ratings7+ratings8+ratings9+ratings10+ratings11\n",
    "tot_reviews= review+review2+review3+review4+review5+review6+review7+review8+review9+review10+review11\n",
    "tot_full= full+full2+full3+full4+full5+full6+full7+full8+full9+full10+full11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94592be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ratings        Reviews Summary  \\\n",
      "0        5         Simply awesome   \n",
      "1        5       Perfect product!   \n",
      "2        5    Best in the market!   \n",
      "3        5     Highly recommended   \n",
      "4        5      Worth every penny   \n",
      "..     ...                    ...   \n",
      "95       5       Perfect product!   \n",
      "96       5         Classy product   \n",
      "97       5      Worth every penny   \n",
      "98       5              Fabulous!   \n",
      "99       5  Mind-blowing purchase   \n",
      "\n",
      "                                  Reviews Description  \n",
      "0   Really satisfied with the Product I received.....  \n",
      "1   Amazing phone with great cameras and better ba...  \n",
      "2   Great iPhone very snappy experience as apple k...  \n",
      "3   What a camera .....just awesome ..you can feel...  \n",
      "4   Previously I was using one plus 3t it was a gr...  \n",
      "..                                                ...  \n",
      "95  Value for money\\n5 star rating\\nExcellent came...  \n",
      "96  Gifted my man on his 30th birthday üéÇ He loves ...  \n",
      "97  It is better to buy iPhone 11 over iPhone 12 i...  \n",
      "98  Gift this to your loved ones fabulous product ...  \n",
      "99  awesome Phone Smooth Touch Too good Sexyy look...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Ratings\":tot_ratings[0:100],\"Reviews Summary\":tot_reviews[0:100],\"Reviews Description\":tot_full[0:100]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1679700",
   "metadata": {},
   "source": [
    "# Question 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a25a2",
   "metadata": {},
   "source": [
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the\n",
    "search field.\n",
    "\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "\n",
    "1. Brand\n",
    "\n",
    "2. Product Description\n",
    "\n",
    "3. Price\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c1b94",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f718b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "# closing the login in pop-up\n",
    "close= driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "close.click()\n",
    "\n",
    "\n",
    "# To find the web element - search title field\n",
    "search_title= driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\") \n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"sneakers\") # Entering Values in Search Title\n",
    "\n",
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55d99c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod=[]\n",
    "for i in prod_rows:\n",
    "    prod.append(i.text)\n",
    "    \n",
    "prod_1=[]\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa _2-ICcC\"]')\n",
    "for i in prod_rows:\n",
    "    prod_1.append(i.text)   \n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc=[]\n",
    "for i in disc_rows:\n",
    "    disc.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a06d0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Next button using absolute Xpath\n",
    "nextb = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span\")\n",
    "nextb\n",
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "661d047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand2=[]\n",
    "for i in brand_title:\n",
    "    brand2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod2=[]\n",
    "for i in prod_rows:\n",
    "    prod2.append(i.text)\n",
    "    \n",
    "prod2_1=[]\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa _2-ICcC\"]')\n",
    "for i in prod_rows:\n",
    "    prod2_1.append(i.text)\n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price2=[]\n",
    "for i in price_rows:\n",
    "    price2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc2=[]\n",
    "for i in disc_rows:\n",
    "    disc2.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6d25a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the Next button\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f77c35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "brand3=[]\n",
    "for i in brand_title:\n",
    "    brand3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "prod3=[]\n",
    "for i in prod_rows:\n",
    "    prod3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "price3=[]\n",
    "for i in price_rows:\n",
    "    price3.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Discount\n",
    "\n",
    "disc_rows= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "disc3=[]\n",
    "for i in disc_rows:\n",
    "    disc3.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec0f31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comabining results and limiting records till 100 rows\n",
    "brands=brand+brand2+brand3[0:20]\n",
    "prices=price+price2+price3[0:20]\n",
    "prods=prod+prod_1+prod2+prod2_1+prod3[0:20]\n",
    "discs=disc+disc2+disc3[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37f90209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Brand                                Product Description Price  \\\n",
      "0            Nilatin                                   Sneakers For Men  ‚Çπ549   \n",
      "1        ZF - ALFIYA                                   Sneakers For Men  ‚Çπ449   \n",
      "2             BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men  ‚Çπ284   \n",
      "3           Magnolia                                   Sneakers For Men  ‚Çπ399   \n",
      "4           URBANBOX                   Puma Smash v2 L Sneakers For Men  ‚Çπ219   \n",
      "..               ...                                                ...   ...   \n",
      "95  MAST AND HARBOUR                                   Sneakers For Men  ‚Çπ999   \n",
      "96          PROVOGUE                                   Sneakers For Men  ‚Çπ759   \n",
      "97           Stinson                                   Sneakers For Men  ‚Çπ249   \n",
      "98            Eiffel  Comfortable & Ultra Light Weight Sneaker Sneak...  ‚Çπ469   \n",
      "99          KWIK FIT                                   Sneakers For Men  ‚Çπ499   \n",
      "\n",
      "   Discount  \n",
      "0   45% off  \n",
      "1   55% off  \n",
      "2   78% off  \n",
      "3   60% off  \n",
      "4   78% off  \n",
      "..      ...  \n",
      "95  62% off  \n",
      "96  62% off  \n",
      "97  50% off  \n",
      "98  53% off  \n",
      "99  75% off  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brands,\"Product Description\":prods,\"Price\":prices,\"Discount\":discs})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599b698",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679ffe0",
   "metadata": {},
   "source": [
    "Go to the link - https://www.myntra.com/shoes\n",
    "\n",
    "Set second Price filter and Color filter to ‚ÄúBlack‚Äù, as shown in the below image.\n",
    "\n",
    "And then scrape First 100 shoes data you get. The data should include ‚ÄúBrand‚Äù of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e4b9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.myntra.com/shoes'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n",
    "\n",
    "# Using Filters\n",
    "# Checking Black color filters\n",
    "col_chk= driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/span[1]\")\n",
    "col_chk.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb6c8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Filters\n",
    "# Checking price filters\n",
    "col_chk= driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "col_chk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e36619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_class_name(\"product-brand\")\n",
    "\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_class_name(\"product-product\")\n",
    "\n",
    "prod=[]\n",
    "for i in prod_rows:\n",
    "    prod.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_class_name(\"product-price\")\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8da4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking next button\n",
    "nextb= driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[12]/a\")\n",
    "nextb.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88b82de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_class_name(\"product-brand\")\n",
    "\n",
    "brand2=[]\n",
    "for i in brand_title:\n",
    "    brand2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Product\n",
    "\n",
    "prod_rows= driver.find_elements_by_class_name(\"product-product\")\n",
    "\n",
    "prod2=[]\n",
    "for i in prod_rows:\n",
    "    prod2.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "price_rows= driver.find_elements_by_class_name(\"product-price\")\n",
    "\n",
    "price2=[]\n",
    "for i in price_rows:\n",
    "    price2.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e9fdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comabining results and limiting records till 100 rows\n",
    "brands=brand+brand2\n",
    "prices=price+price2\n",
    "prods=prod+prod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6b5de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Brand             Product Description                        Price\n",
      "0        ALDO             Men Leather Loafers   Rs. 7799Rs. 12999(40% OFF)\n",
      "1        Nike    Men React Infinity 3 Running  Rs. 11196Rs. 13995(20% OFF)\n",
      "2        ALDO             Men Leather Loafers   Rs. 7999Rs. 15999(50% OFF)\n",
      "3        Nike      Men KD 15 Basketball Shoes  Rs. 11895Rs. 13995(15% OFF)\n",
      "4        Nike  Women React MR 3 Running Shoes   Rs. 7871Rs. 10495(25% OFF)\n",
      "..        ...                             ...                          ...\n",
      "95     ADIDAS   Women Supernova Running Shoes    Rs. 7999Rs. 9999(20% OFF)\n",
      "96       Geox     Men Leather Formal Slip-Ons    Rs. 8991Rs. 9990(10% OFF)\n",
      "97       Xtep               Men Running Shoes                     Rs. 7699\n",
      "98  J.FONTINI      Men Leather Formal Loafers                     Rs. 7490\n",
      "99  J.FONTINI       Men Black Leather Loafers                     Rs. 8490\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brands,\"Product Description\":prods,\"Price\":prices})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f01765",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64712114",
   "metadata": {},
   "source": [
    "Go to webpage https://www.amazon.in/\n",
    "\n",
    "Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon.\n",
    "\n",
    "Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "\n",
    "2. Ratings\n",
    "\n",
    "3. Price\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cdc0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.amazon.in'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3492e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the web element - search title field\n",
    "search_title= driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\") \n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Laptop\") # Entering Values in Search Title\n",
    "\n",
    "# Finding search button using absolute Xpath\n",
    "search = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "search\n",
    "# Clicking the search button\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a82d9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on Intel Core i7 filter\n",
    "\n",
    "cpu_chk= driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[2]/li[14]/span/a/span\")\n",
    "\n",
    "cpu_chk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b47ab3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Brand Titles\n",
    "\n",
    "brand_title= driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "brand=[]\n",
    "for i in brand_title:\n",
    "    brand.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Price \n",
    "\n",
    "\n",
    "\n",
    "price=[]\n",
    "for i in price_rows:\n",
    "    price.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d5a1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Ratings\n",
    "ratings_rows=driver.find_elements_by_xpath('//span[@class=\"a-icon-alt\"]')\n",
    "\n",
    "ratings=[]\n",
    "for i in ratings_rows:\n",
    "    ratings.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d412c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Brand Product Rating  \\\n",
      "0  Acer Extensa 15 Thin & Light Intel Processor P...                  \n",
      "1  Acer Aspire 3 Laptop (Made in India) A314-35 3...                  \n",
      "2  ASUS VivoBook 14 (2021), 14-inch (35.56 cm) HD...                  \n",
      "3  Dell New Inspiron 3521 Laptop, Intel Pqc-N5030...                  \n",
      "4  Acer Extensa 15 Thin & Light Intel Processor P...                  \n",
      "5  Hp 14S-Intel Pentium Silver N6000- 8Gb Ram/256...                  \n",
      "6  Acer Travelmate Business Laptop Intel Pentium ...                  \n",
      "7  Microsoft Surface GO 3 8VA-00013 10.5\" (26.67 ...                  \n",
      "8  Acer Travelmate Intel¬Æ Pentium¬Æ Gold 7505 Proc...                  \n",
      "9  Acer Extensa 15 Thin & Light Laptop Intel Proc...                  \n",
      "\n",
      "                         Price  \n",
      "0   Rs. 8249Rs. 10999(25% OFF)  \n",
      "1   Rs. 7199Rs. 11999(40% OFF)  \n",
      "2                     Rs. 7999  \n",
      "3   Rs. 8505Rs. 10500(19% OFF)  \n",
      "4                    Rs. 11999  \n",
      "5  Rs. 11192Rs. 13990(20% OFF)  \n",
      "6   Rs. 10355Rs. 10900(5% OFF)  \n",
      "7  Rs. 10349Rs. 11499(10% OFF)  \n",
      "8  Rs. 12591Rs. 13990(10% OFF)  \n",
      "9                     Rs. 9990  \n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Brand\":brand[0:10],\"Product Rating\":ratings[0:10],\"Price\":price[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb76e6a",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5385ee4",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "\n",
    "2. Click on the Job option as shown in the image\n",
    "\n",
    "3. After reaching to the next webpage, In place of ‚ÄúSearch by Designations, Companies, Skills‚Äù enter\n",
    "‚ÄúData Scientist‚Äù and click on search button.\n",
    "\n",
    "4. You will reach to the following web page click on location and in place of ‚ÄúSearch location‚Äù enter\n",
    "‚ÄúNoida‚Äù and select location ‚ÄúNoida‚Äù.\n",
    "\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98259ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.ambitionbox.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a35d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on Jobs tab\n",
    "job_tab= driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[6]\")\n",
    "job_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd552f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input \") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title\n",
    "\n",
    "# click search button\n",
    "search_btn= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dfd472d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-down search\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input\") # To find the web element - search title field\n",
    "search_loc.click()                                                        # calling the web element selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb06d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_loc.send_keys(\"Noida\") # Entering Values in Search Title\n",
    "\n",
    "# click search button\n",
    "search_btn2= driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[5]/div/label\")\n",
    "search_btn2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe9835fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Company Name\n",
    "\n",
    "company_title= driver.find_elements_by_xpath('//div[@class=\"company-info\"]')\n",
    "company=[]\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Posting duration\n",
    "\n",
    "post_rows= driver.find_elements_by_xpath('//span[@class=\"body-small-l\"]')\n",
    "\n",
    "post=[]\n",
    "for i in post_rows:\n",
    "    post.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35d52613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Company Job Posting\n",
      "0  Optum Global Solutions (India) Private Limited...     11d ago\n",
      "1  Optum Global Solutions (India) Private Limited...     18d ago\n",
      "2  GENPACT India Private Limited\\n4.0\\n(17.9k Rev...      2d ago\n",
      "3                   Latent bridge\\n4.5\\n(54 Reviews)      9d ago\n",
      "4         Dew Solutions Pvt. Ltd.\\n4.3\\n(80 Reviews)     16d ago\n",
      "5           InfoEdge India Ltd.\\n3.9\\n(1.7k Reviews)     17d ago\n",
      "6       Info Edge India Limited\\n3.9\\n(1.7k Reviews)     17d ago\n",
      "7       Info Edge India Limited\\n3.9\\n(1.7k Reviews)      4d ago\n",
      "8                      Careerera\\n3.8\\n(100 Reviews)     12d ago\n",
      "9                         CRMnext\\n4.1\\n(74 Reviews)    1mon ago\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Company\":company[0:10],\"Job Posting\":post[0:20:2]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f392448",
   "metadata": {},
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c817a8",
   "metadata": {},
   "source": [
    "Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "\n",
    "2. Click on the salaries option as shown in the image.\n",
    "\n",
    "3. After reaching to the following webpage, In place of ‚ÄúSearch Job Profile‚Äù enters ‚ÄúData Scientist‚Äù and\n",
    "then click on ‚ÄúData Scientist‚Äù.\n",
    "\n",
    "You have to scrape the data ticked in the above image.\n",
    "\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "\n",
    "5. Store the data in a dataframe.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a4c0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to a webdriver\n",
    "driver=wd.Chrome(r'D:\\Internship\\Web Scraping\\Web Scraping 2\\chromedriver.exe')\n",
    "\n",
    "url='https://www.ambitionbox.com/'        # Defining the url\n",
    "driver.get(url)                       # opening the url in our test chrome explorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af6c977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on Salaries tab\n",
    "sal_tab= driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[4]\")\n",
    "sal_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fe791dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title= driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input\") # To find the web element - search title field\n",
    "search_title                                                        # calling the web element selected\n",
    "\n",
    "search_title.send_keys(\"Data Scientist\") # Entering Values in Search Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "562f52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking Data Scientist from suggestion\n",
    "sel_btn= driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p\")\n",
    "sel_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67b15c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Company Name\n",
    "\n",
    "company_title= driver.find_elements_by_tag_name('a')\n",
    "company=[]\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "    \n",
    "    \n",
    "# Extracting Total Salary record\n",
    "\n",
    "total_rows= driver.find_elements_by_xpath('//span[@class=\"datapoints\"]')\n",
    "\n",
    "sal_tot=[]\n",
    "for i in total_rows:\n",
    "    sal_tot.append(i.text)\n",
    "    \n",
    "# Extracting Minimum Salary record\n",
    "\n",
    "min_rows= driver.find_elements_by_xpath('//div[@class=\"value body-medium\"]')\n",
    "\n",
    "sal=[]\n",
    "for i in min_rows:\n",
    "    sal.append(i.text)\n",
    "    \n",
    "# Extracting Average Salary \n",
    "\n",
    "avg_rows= driver.find_elements_by_class_name(\"averageCtc\")\n",
    "\n",
    "sal_avg=[]\n",
    "for i in avg_rows:\n",
    "    sal_avg.append(i.text)\n",
    "\n",
    "    \n",
    "# Extracting Experience required\n",
    "\n",
    "exp_reqd= driver.find_elements_by_xpath('//div[@class=\"sbold-list-header\"]')\n",
    "\n",
    "exp=[]\n",
    "for i in exp_reqd:\n",
    "    exp.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9255c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sal=sal[0:19:2]\n",
    "max_sal=sal[1::2]\n",
    "company=company[15:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3ee4341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Company          Salary Records  \\\n",
      "0           Ab Inbev\\nData Scientist Salary  (based on 12 salaries)   \n",
      "1                 ZS\\nData Scientist Salary  (based on 33 salaries)   \n",
      "2              Optum\\nData Scientist Salary  (based on 15 salaries)   \n",
      "3       Reliance Jio\\nData Scientist Salary  (based on 32 salaries)   \n",
      "4  Fractal Analytics\\nData Scientist Salary  (based on 21 salaries)   \n",
      "5    Tiger Analytics\\nData Scientist Salary  (based on 89 salaries)   \n",
      "6       UnitedHealth\\nData Scientist Salary  (based on 51 salaries)   \n",
      "7        EXL Service\\nData Scientist Salary  (based on 57 salaries)   \n",
      "8           Deloitte\\nData Scientist Salary  (based on 21 salaries)   \n",
      "9                  Software Engineer Salary  (based on 69 salaries)   \n",
      "\n",
      "  Minimum Salary Average Salary Maximum Salary  \\\n",
      "0        ‚Çπ 25.0L        ‚Çπ 25.0L        ‚Çπ 36.0L   \n",
      "1        ‚Çπ 15.0L        ‚Çπ 15.0L        ‚Çπ 26.2L   \n",
      "2        ‚Çπ 11.0L        ‚Çπ 11.0L        ‚Çπ 22.0L   \n",
      "3        ‚Çπ 11.0L        ‚Çπ 11.0L        ‚Çπ 22.5L   \n",
      "4         ‚Çπ 5.6L         ‚Çπ 5.6L        ‚Çπ 26.2L   \n",
      "5        ‚Çπ 10.0L        ‚Çπ 10.0L        ‚Çπ 23.0L   \n",
      "6         ‚Çπ 9.0L         ‚Çπ 9.0L        ‚Çπ 20.0L   \n",
      "7         ‚Çπ 8.3L         ‚Çπ 8.3L        ‚Çπ 21.1L   \n",
      "8         ‚Çπ 7.6L         ‚Çπ 7.6L        ‚Çπ 21.0L   \n",
      "9         ‚Çπ 7.0L         ‚Çπ 7.0L        ‚Çπ 25.0L   \n",
      "\n",
      "                         Experience Required  \n",
      "0    3 yrs experience (based on 12 salaries)  \n",
      "1  3-4 yrs experience (based on 33 salaries)  \n",
      "2    2 yrs experience (based on 15 salaries)  \n",
      "3  3-4 yrs experience (based on 32 salaries)  \n",
      "4  3-4 yrs experience (based on 21 salaries)  \n",
      "5  2-4 yrs experience (based on 89 salaries)  \n",
      "6  2-4 yrs experience (based on 51 salaries)  \n",
      "7  2-4 yrs experience (based on 57 salaries)  \n",
      "8  3-4 yrs experience (based on 21 salaries)  \n",
      "9  2-4 yrs experience (based on 69 salaries)  \n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "table=pd.DataFrame({\"Company\":company[0:10],\"Salary Records\":sal_tot[0:10],\"Minimum Salary\":min_sal,\"Average Salary\":min_sal[0:10],\"Maximum Salary\":max_sal,\"Experience Required\":exp[0:10]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3bd13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
