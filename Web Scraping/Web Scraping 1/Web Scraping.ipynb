{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb83bae",
   "metadata": {},
   "source": [
    "This notebook contains answers of the Web Scraping Assignment given to me for my internship with Flip Robo Techologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff9fee",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "In all the following questions, you have to use BeautifulSoup to scrape different websites and collect data as per\n",
    "the requirement of the question.\n",
    "Every answer to the question should be in form of a python function which should take URL as the parameter.\n",
    "Use Jupyter Notebooks to program, upload it on your GitHub and send the link of the Jupyter notebook to\n",
    "your SME.\n",
    "\n",
    "The deadline for the submission of the assignment is Thursday, 30-June-2022, 11:59 PM.\n",
    "\n",
    "NOTE: kindly do all the questions in one jupyter notebook and upload on your GitHub profile and share the link of the same with me using PMT message tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6ddba",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea88d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest    # For importing, analysing and manipulation of Data\n",
    "import bs4         # For Webscraping\n",
    "from bs4 import BeautifulSoup    # For Webscraping by parsing the source code and to extract required data from the parsed structure\n",
    "import requests       # To get access by requesting connection establishment to website for source codes\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65990c",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ad072",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01026888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    page2=requests.get(url2)\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b59135",
   "metadata": {},
   "source": [
    "Recalling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f781fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p># robots.txt for http://www.wikipedia.org/ and friends\n",
      "#\n",
      "# Please note: There are a lot of pages on this site, and there are\n",
      "# some misbehaved spiders out there that go _way_ too fast. If you're\n",
      "# irresponsible, your access to the site may be blocked.\n",
      "#\n",
      "\n",
      "# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\n",
      "# and ignoring 429 ratelimit responses, claims to respect robots:\n",
      "# http://mj12bot.com/\n",
      "User-agent: MJ12bot\n",
      "Disallow: /\n",
      "\n",
      "# advertising-related bots:\n",
      "User-agent: Mediapartners-Google*\n",
      "Disallow: /\n",
      "\n",
      "# Wikipedia work bots:\n",
      "User-agent: IsraBot\n",
      "Disallow:\n",
      "\n",
      "User-agent: Orthogaffe\n",
      "Disallow:\n",
      "\n",
      "# Crawlers that are kind enough to obey, but which we'd rather not have\n",
      "# unless they're feeding search engines.\n",
      "User-agent: UbiCrawler\n",
      "Disallow: /\n",
      "\n",
      "User-agent: DOC\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Zao\n",
      "Disallow: /\n",
      "\n",
      "# Some bots are known to be trouble, particularly those designed to copy\n",
      "# entire sites. Please obey robots.txt.\n",
      "User-agent: sitecheck.internetseer.com\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Zealbot\n",
      "Disallow: /\n",
      "\n",
      "User-agent: MSIECrawler\n",
      "Disallow: /\n",
      "\n",
      "User-agent: SiteSnagger\n",
      "Disallow: /\n",
      "\n",
      "User-agent: WebStripper\n",
      "Disallow: /\n",
      "\n",
      "User-agent: WebCopier\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Fetch\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Offline Explorer\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Teleport\n",
      "Disallow: /\n",
      "\n",
      "User-agent: TeleportPro\n",
      "Disallow: /\n",
      "\n",
      "User-agent: WebZIP\n",
      "Disallow: /\n",
      "\n",
      "User-agent: linko\n",
      "Disallow: /\n",
      "\n",
      "User-agent: HTTrack\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Microsoft.URL.Control\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Xenu\n",
      "Disallow: /\n",
      "\n",
      "User-agent: larbin\n",
      "Disallow: /\n",
      "\n",
      "User-agent: libwww\n",
      "Disallow: /\n",
      "\n",
      "User-agent: ZyBORG\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Download Ninja\n",
      "Disallow: /\n",
      "\n",
      "# Misbehaving: requests much too fast:\n",
      "User-agent: fast\n",
      "Disallow: /\n",
      "\n",
      "#\n",
      "# Sorry, wget in its recursive mode is a frequent problem.\n",
      "# Please read the man page and use it properly; there is a\n",
      "# --wait option you can use to set the delay between hits,\n",
      "# for instance.\n",
      "#\n",
      "User-agent: wget\n",
      "Disallow: /\n",
      "\n",
      "#\n",
      "# The 'grub' distributed client has been *very* poorly behaved.\n",
      "#\n",
      "User-agent: grub-client\n",
      "Disallow: /\n",
      "\n",
      "#\n",
      "# Doesn't follow robots.txt anyway, but...\n",
      "#\n",
      "User-agent: k2spider\n",
      "Disallow: /\n",
      "\n",
      "#\n",
      "# Hits many times per second, not acceptable\n",
      "# http://www.nameprotect.com/botinfo.html\n",
      "User-agent: NPBot\n",
      "Disallow: /\n",
      "\n",
      "# A capture bot, downloads gazillions of pages with no public benefit\n",
      "# http://www.webreaper.net/\n",
      "User-agent: WebReaper\n",
      "Disallow: /\n",
      "\n",
      "\n",
      "#\n",
      "# Friendly, low-speed bots are welcome viewing article pages, but not\n",
      "# dynamically-generated pages please.\n",
      "#\n",
      "# Inktomi's \"Slurp\" can read a minimum delay between hits; if your\n",
      "# bot supports such a thing using the 'Crawl-delay' or another\n",
      "# instruction, please let us know.\n",
      "#\n",
      "# There is a special exception for API mobileview to allow dynamic\n",
      "# mobile web &amp; app views to load section content.\n",
      "# These views aren't HTTP-cached but use parser cache aggressively\n",
      "# and don't expose special: pages etc.\n",
      "#\n",
      "# Another exception is for REST API documentation, located at\n",
      "# /api/rest_v1/?doc.\n",
      "#\n",
      "User-agent: *\n",
      "Allow: /w/api.php?action=mobileview&amp;\n",
      "Allow: /w/load.php?\n",
      "Allow: /api/rest_v1/?doc\n",
      "Disallow: /w/\n",
      "Disallow: /api/\n",
      "Disallow: /trap/\n",
      "Disallow: /wiki/Special:\n",
      "Disallow: /wiki/Spezial:\n",
      "Disallow: /wiki/Spesial:\n",
      "Disallow: /wiki/Special%3A\n",
      "Disallow: /wiki/Spezial%3A\n",
      "Disallow: /wiki/Spesial%3A\n",
      "\n",
      "#\n",
      "# ar:\n",
      "Disallow: /wiki/%D8%AE%D8%A7%D8%B5:Search\n",
      "Disallow: /wiki/%D8%AE%D8%A7%D8%B5%3ASearch\n",
      "#\n",
      "# dewiki:\n",
      "# T6937\n",
      "# sensible deletion and meta user discussion pages:\n",
      "Disallow: /wiki/Wikipedia:L%C3%B6schkandidaten/\n",
      "Disallow: /wiki/Wikipedia:Löschkandidaten/\n",
      "Disallow: /wiki/Wikipedia:Vandalensperrung/\n",
      "Disallow: /wiki/Wikipedia:Benutzersperrung/\n",
      "Disallow: /wiki/Wikipedia:Vermittlungsausschuss/\n",
      "Disallow: /wiki/Wikipedia:Administratoren/Probleme/\n",
      "Disallow: /wiki/Wikipedia:Adminkandidaturen/\n",
      "Disallow: /wiki/Wikipedia:Qualitätssicherung/\n",
      "Disallow: /wiki/Wikipedia:Qualit%C3%A4tssicherung/\n",
      "# 4937#5\n",
      "Disallow: /wiki/Wikipedia:Vandalismusmeldung/\n",
      "Disallow: /wiki/Wikipedia:Gesperrte_Lemmata/\n",
      "Disallow: /wiki/Wikipedia:Löschprüfung/\n",
      "Disallow: /wiki/Wikipedia:L%C3%B6schprüfung/\n",
      "Disallow: /wiki/Wikipedia:Administratoren/Notizen/\n",
      "Disallow: /wiki/Wikipedia:Schiedsgericht/Anfragen/\n",
      "Disallow: /wiki/Wikipedia:L%C3%B6schpr%C3%BCfung/\n",
      "# T14111\n",
      "Disallow: /wiki/Wikipedia:Checkuser/\n",
      "Disallow: /wiki/Wikipedia_Diskussion:Checkuser/\n",
      "Disallow: /wiki/Wikipedia_Diskussion:Adminkandidaturen/\n",
      "# T15961\n",
      "Disallow: /wiki/Wikipedia:Spam-Blacklist-Log\n",
      "Disallow: /wiki/Wikipedia%3ASpam-Blacklist-Log\n",
      "Disallow: /wiki/Wikipedia_Diskussion:Spam-Blacklist-Log\n",
      "Disallow: /wiki/Wikipedia_Diskussion%3ASpam-Blacklist-Log\n",
      "#\n",
      "# enwiki:\n",
      "# Folks get annoyed when VfD discussions end up the number 1 google hit for\n",
      "# their name. See T6776\n",
      "Disallow: /wiki/Wikipedia:Articles_for_deletion/\n",
      "Disallow: /wiki/Wikipedia%3AArticles_for_deletion/\n",
      "Disallow: /wiki/Wikipedia:Votes_for_deletion/\n",
      "Disallow: /wiki/Wikipedia%3AVotes_for_deletion/\n",
      "Disallow: /wiki/Wikipedia:Pages_for_deletion/\n",
      "Disallow: /wiki/Wikipedia%3APages_for_deletion/\n",
      "Disallow: /wiki/Wikipedia:Miscellany_for_deletion/\n",
      "Disallow: /wiki/Wikipedia%3AMiscellany_for_deletion/\n",
      "Disallow: /wiki/Wikipedia:Miscellaneous_deletion/\n",
      "Disallow: /wiki/Wikipedia%3AMiscellaneous_deletion/\n",
      "Disallow: /wiki/Wikipedia:Copyright_problems\n",
      "Disallow: /wiki/Wikipedia%3ACopyright_problems\n",
      "Disallow: /wiki/Wikipedia:Protected_titles/\n",
      "Disallow: /wiki/Wikipedia%3AProtected_titles/\n",
      "# T15398\n",
      "Disallow: /wiki/Wikipedia:WikiProject_Spam/\n",
      "Disallow: /wiki/Wikipedia%3AWikiProject_Spam/\n",
      "# T16075\n",
      "Disallow: /wiki/MediaWiki:Spam-blacklist\n",
      "Disallow: /wiki/MediaWiki%3ASpam-blacklist\n",
      "Disallow: /wiki/MediaWiki_talk:Spam-blacklist\n",
      "Disallow: /wiki/MediaWiki_talk%3ASpam-blacklist\n",
      "# T13261\n",
      "Disallow: /wiki/Wikipedia:Requests_for_arbitration/\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_arbitration/\n",
      "Disallow: /wiki/Wikipedia:Requests_for_comment/\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_comment/\n",
      "Disallow: /wiki/Wikipedia:Requests_for_adminship/\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_adminship/\n",
      "# T12288\n",
      "Disallow: /wiki/Wikipedia_talk:Articles_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk%3AArticles_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk:Votes_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk%3AVotes_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk:Pages_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk%3APages_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk:Miscellany_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk%3AMiscellany_for_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk:Miscellaneous_deletion/\n",
      "Disallow: /wiki/Wikipedia_talk%3AMiscellaneous_deletion/\n",
      "# T16793\n",
      "Disallow: /wiki/Wikipedia:Changing_username\n",
      "Disallow: /wiki/Wikipedia%3AChanging_username\n",
      "Disallow: /wiki/Wikipedia:Changing_username/\n",
      "Disallow: /wiki/Wikipedia%3AChanging_username/\n",
      "Disallow: /wiki/Wikipedia_talk:Changing_username\n",
      "Disallow: /wiki/Wikipedia_talk%3AChanging_username\n",
      "Disallow: /wiki/Wikipedia_talk:Changing_username/\n",
      "Disallow: /wiki/Wikipedia_talk%3AChanging_username/\n",
      "#\n",
      "# eswiki:\n",
      "# T8746\n",
      "Disallow: /wiki/Wikipedia:Consultas_de_borrado/\n",
      "Disallow: /wiki/Wikipedia%3AConsultas_de_borrado/\n",
      "#\n",
      "# fiwiki:\n",
      "# T10695\n",
      "Disallow: /wiki/Wikipedia:Poistettavat_sivut\n",
      "Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n",
      "Disallow: /wiki/Käyttäjä:\n",
      "Disallow: /wiki/Keskustelu_k%C3%A4ytt%C3%A4j%C3%A4st%C3%A4:\n",
      "Disallow: /wiki/Keskustelu_käyttäjästä:\n",
      "Disallow: /wiki/Wikipedia:Yll%C3%A4pit%C3%A4j%C3%A4t/\n",
      "Disallow: /wiki/Wikipedia:Ylläpitäjät/\n",
      "#\n",
      "# hewiki:\n",
      "Disallow: /wiki/%D7%9E%D7%99%D7%95%D7%97%D7%93:Search\n",
      "Disallow: /wiki/%D7%9E%D7%99%D7%95%D7%97%D7%93%3ASearch\n",
      "#T11517\n",
      "Disallow: /wiki/ויקיפדיה:רשימת_מועמדים_למחיקה/\n",
      "Disallow: /wiki/ויקיפדיה%3Aרשימת_מועמדים_למחיקה/\n",
      "Disallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94:%D7%A8%D7%A9%D7%99%D7%9E%D7%AA_%D7%9E%D7%95%D7%A2%D7%9E%D7%93%D7%99%D7%9D_%D7%9C%D7%9E%D7%97%D7%99%D7%A7%D7%94/\n",
      "Disallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94%3A%D7%A8%D7%A9%D7%99%D7%9E%D7%AA_%D7%9E%D7%95%D7%A2%D7%9E%D7%93%D7%99%D7%9D_%D7%9C%D7%9E%D7%97%D7%99%D7%A7%D7%94/\n",
      "Disallow: /wiki/ויקיפדיה:ערכים_לא_קיימים_ומוגנים\n",
      "Disallow: /wiki/ויקיפדיה%3Aערכים_לא_קיימים_ומוגנים\n",
      "Disallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94:%D7%A2%D7%A8%D7%9B%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\n",
      "Disallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94%3A%D7%A2%D7%A8%D7%9B%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\n",
      "Disallow: /wiki/ויקיפדיה:דפים_לא_קיימים_ומוגנים\n",
      "Disallow: /wiki/ויקיפדיה%3Aדפים_לא_קיימים_ומוגנים\n",
      "Disallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94:%D7%93%D7%A4%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\n",
      "Disallow: /wiki/%D7%95%D7%99%D7%A7%D7%99%D7%A4%D7%93%D7%99%D7%94%3A%D7%93%D7%A4%D7%99%D7%9D_%D7%9C%D7%90_%D7%A7%D7%99%D7%99%D7%9E%D7%99%D7%9D_%D7%95%D7%9E%D7%95%D7%92%D7%A0%D7%99%D7%9D\n",
      "#\n",
      "# huwiki:\n",
      "Disallow: /wiki/Speci%C3%A1lis:Search\n",
      "Disallow: /wiki/Speci%C3%A1lis%3ASearch\n",
      "#\n",
      "# itwiki:\n",
      "# T7545\n",
      "Disallow: /wiki/Wikipedia:Pagine_da_cancellare\n",
      "Disallow: /wiki/Wikipedia%3APagine_da_cancellare\n",
      "Disallow: /wiki/Wikipedia:Utenti_problematici\n",
      "Disallow: /wiki/Wikipedia%3AUtenti_problematici\n",
      "Disallow: /wiki/Wikipedia:Vandalismi_in_corso\n",
      "Disallow: /wiki/Wikipedia%3AVandalismi_in_corso\n",
      "Disallow: /wiki/Wikipedia:Amministratori\n",
      "Disallow: /wiki/Wikipedia%3AAmministratori\n",
      "Disallow: /wiki/Wikipedia:Proposte_di_cancellazione_semplificata\n",
      "Disallow: /wiki/Wikipedia%3AProposte_di_cancellazione_semplificata\n",
      "Disallow: /wiki/Categoria:Da_cancellare_subito\n",
      "Disallow: /wiki/Categoria%3ADa_cancellare_subito\n",
      "Disallow: /wiki/Wikipedia:Sospette_violazioni_di_copyright\n",
      "Disallow: /wiki/Wikipedia%3ASospette_violazioni_di_copyright\n",
      "Disallow: /wiki/Categoria:Da_controllare_per_copyright\n",
      "Disallow: /wiki/Categoria%3ADa_controllare_per_copyright\n",
      "Disallow: /wiki/Progetto:Rimozione_contributi_sospetti\n",
      "Disallow: /wiki/Progetto%3ARimozione_contributi_sospetti\n",
      "Disallow: /wiki/Categoria:Da_cancellare_subito_per_violazione_integrale_copyright\n",
      "Disallow: /wiki/Categoria%3ADa_cancellare_subito_per_violazione_integrale_copyright\n",
      "Disallow: /wiki/Progetto:Cococo\n",
      "Disallow: /wiki/Progetto%3ACococo\n",
      "Disallow: /wiki/Discussioni_progetto:Cococo\n",
      "Disallow: /wiki/Discussioni_progetto%3ACococo\n",
      "#\n",
      "# jawiki\n",
      "Disallow: /wiki/%E7%89%B9%E5%88%A5:Search\n",
      "Disallow: /wiki/%E7%89%B9%E5%88%A5%3ASearch\n",
      "# T7239\n",
      "Disallow: /wiki/Wikipedia:%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC/\n",
      "Disallow: /wiki/Wikipedia%3A%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC/\n",
      "Disallow: /wiki/Wikipedia:%E5%88%A9%E7%94%A8%E8%80%85%E3%83%9A%E3%83%BC%E3%82%B8%E3%81%AE%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC\n",
      "Disallow: /wiki/Wikipedia%3A%E5%88%A9%E7%94%A8%E8%80%85%E3%83%9A%E3%83%BC%E3%82%B8%E3%81%AE%E5%89%8A%E9%99%A4%E4%BE%9D%E9%A0%BC\n",
      "# nowiki\n",
      "# T13432\n",
      "Disallow: /wiki/Bruker:\n",
      "Disallow: /wiki/Bruker%3A\n",
      "Disallow: /wiki/Brukerdiskusjon\n",
      "Disallow: /wiki/Wikipedia:Administratorer\n",
      "Disallow: /wiki/Wikipedia%3AAdministratorer\n",
      "Disallow: /wiki/Wikipedia-diskusjon:Administratorer\n",
      "Disallow: /wiki/Wikipedia-diskusjon%3AAdministratorer\n",
      "Disallow: /wiki/Wikipedia:Sletting\n",
      "Disallow: /wiki/Wikipedia%3ASletting\n",
      "Disallow: /wiki/Wikipedia-diskusjon:Sletting\n",
      "Disallow: /wiki/Wikipedia-diskusjon%3ASletting\n",
      "#\n",
      "# plwiki\n",
      "# T10067\n",
      "Disallow: /wiki/Wikipedia:Strony_do_usuni%C4%99cia\n",
      "Disallow: /wiki/Wikipedia%3AStrony_do_usuni%C4%99cia\n",
      "Disallow: /wiki/Wikipedia:Do_usuni%C4%99cia\n",
      "Disallow: /wiki/Wikipedia%3ADo_usuni%C4%99cia\n",
      "Disallow: /wiki/Wikipedia:SDU/\n",
      "Disallow: /wiki/Wikipedia%3ASDU/\n",
      "Disallow: /wiki/Wikipedia:Strony_podejrzane_o_naruszenie_praw_autorskich\n",
      "Disallow: /wiki/Wikipedia%3AStrony_podejrzane_o_naruszenie_praw_autorskich\n",
      "#\n",
      "# ptwiki:\n",
      "# T7394\n",
      "Disallow: /wiki/Wikipedia:Páginas_para_eliminar/\n",
      "Disallow: /wiki/Wikipedia:P%C3%A1ginas_para_eliminar/\n",
      "Disallow: /wiki/Wikipedia%3AP%C3%A1ginas_para_eliminar/\n",
      "Disallow: /wiki/Wikipedia_Discussão:Páginas_para_eliminar/\n",
      "Disallow: /wiki/Wikipedia_Discuss%C3%A3o:P%C3%A1ginas_para_eliminar/\n",
      "Disallow: /wiki/Wikipedia_Discuss%C3%A3o%3AP%C3%A1ginas_para_eliminar/\n",
      "#\n",
      "# rowiki:\n",
      "# T14546\n",
      "Disallow: /wiki/Wikipedia:Pagini_de_%C5%9Fters\n",
      "Disallow: /wiki/Wikipedia%3APagini_de_%C5%9Fters\n",
      "Disallow: /wiki/Discu%C5%A3ie_Wikipedia:Pagini_de_%C5%9Fters\n",
      "Disallow: /wiki/Discu%C5%A3ie_Wikipedia%3APagini_de_%C5%9Fters\n",
      "#\n",
      "# ruwiki:\n",
      "Disallow: /wiki/%D0%A1%D0%BF%D0%B5%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5:Search\n",
      "Disallow: /wiki/%D0%A1%D0%BF%D0%B5%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%3ASearch\n",
      "#\n",
      "# svwiki:\n",
      "# T12229\n",
      "Disallow: /wiki/Wikipedia%3ASidor_f%C3%B6reslagna_f%C3%B6r_radering\n",
      "Disallow: /wiki/Wikipedia:Sidor_f%C3%B6reslagna_f%C3%B6r_radering\n",
      "Disallow: /wiki/Wikipedia:Sidor_föreslagna_för_radering\n",
      "Disallow: /wiki/Användare\n",
      "Disallow: /wiki/Anv%C3%A4ndare\n",
      "Disallow: /wiki/Användardiskussion\n",
      "Disallow: /wiki/Anv%C3%A4ndardiskussion\n",
      "Disallow: /wiki/Wikipedia:Skyddade_sidnamn\n",
      "Disallow: /wiki/Wikipedia%3ASkyddade_sidnamn\n",
      "# T13291\n",
      "Disallow: /wiki/Wikipedia:Sidor_som_bör_raderas\n",
      "Disallow: /wiki/Wikipedia:Sidor_som_b%C3%B6r_raderas\n",
      "Disallow: /wiki/Wikipedia%3ASidor_som_b%C3%B6r_raderas\n",
      "#\n",
      "# zhwiki:\n",
      "# T7104\n",
      "Disallow: /wiki/Wikipedia:删除投票/侵权\n",
      "Disallow: /wiki/Wikipedia:%E5%88%A0%E9%99%A4%E6%8A%95%E7%A5%A8/%E4%BE%B5%E6%9D%83\n",
      "Disallow: /wiki/Wikipedia:删除投票和请求\n",
      "Disallow: /wiki/Wikipedia:%E5%88%A0%E9%99%A4%E6%8A%95%E7%A5%A8%E5%92%8C%E8%AF%B7%E6%B1%82\n",
      "Disallow: /wiki/Category:快速删除候选\n",
      "Disallow: /wiki/Category:%E5%BF%AB%E9%80%9F%E5%88%A0%E9%99%A4%E5%80%99%E9%80%89\n",
      "Disallow: /wiki/Category:维基百科需要翻译的文章\n",
      "Disallow: /wiki/Category:%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E9%9C%80%E8%A6%81%E7%BF%BB%E8%AF%91%E7%9A%84%E6%96%87%E7%AB%A0\n",
      "#\n",
      "# sister projects\n",
      "#\n",
      "# enwikinews:\n",
      "# T7340\n",
      "Disallow: /wiki/Portal:Prepared_stories/\n",
      "Disallow: /wiki/Portal%3APrepared_stories/\n",
      "#\n",
      "# itwikinews\n",
      "# T11138\n",
      "Disallow: /wiki/Wikinotizie:Richieste_di_cancellazione\n",
      "Disallow: /wiki/Wikinotizie:Sospette_violazioni_di_copyright\n",
      "Disallow: /wiki/Categoria:Da_cancellare_subito\n",
      "Disallow: /wiki/Categoria:Da_cancellare_subito_per_violazione_integrale_copyright\n",
      "Disallow: /wiki/Wikinotizie:Storie_in_preparazione\n",
      "#\n",
      "# enwikiquote:\n",
      "# T17095\n",
      "Disallow: /wiki/Wikiquote:Votes_for_deletion/\n",
      "Disallow: /wiki/Wikiquote%3AVotes_for_deletion/\n",
      "Disallow: /wiki/Wikiquote_talk:Votes_for_deletion/\n",
      "Disallow: /wiki/Wikiquote_talk%3AVotes_for_deletion/\n",
      "Disallow: /wiki/Wikiquote:Votes_for_deletion_archive/\n",
      "Disallow: /wiki/Wikiquote%3AVotes_for_deletion_archive/\n",
      "Disallow: /wiki/Wikiquote_talk:Votes_for_deletion_archive/\n",
      "Disallow: /wiki/Wikiquote_talk%3AVotes_for_deletion_archive/\n",
      "#\n",
      "# enwikibooks\n",
      "Disallow: /wiki/Wikibooks:Votes_for_deletion\n",
      "#\n",
      "# working...\n",
      "Disallow: /wiki/Fundraising_2007/comments\n",
      "#\n",
      "#\n",
      "#\n",
      "#----------------------------------------------------------#\n",
      "#\n",
      "#\n",
      "#\n",
      " # <!-- Please do not remove the space at the start of this line, it breaks the rendering.  http://www.robotstxt.org/orig.html says spaces before comments are OK. --></p><pre>\n",
      "#\n",
      "# Localisable part of robots.txt for en.wikipedia.org\n",
      "#\n",
      "# Edit at https://en.wikipedia.org/w/index.php?title=MediaWiki:Robots.txt&amp;action=edit\n",
      "# Don't add newlines here. All rules set here are active for every user-agent.\n",
      "#\n",
      "# Please check any changes using a syntax validator\n",
      "# Enter https://en.wikipedia.org/robots.txt as the URL to check.\n",
      "#\n",
      "# https://phabricator.wikimedia.org/T16075\n",
      "Disallow: /wiki/MediaWiki:Spam-blacklist\n",
      "Disallow: /wiki/MediaWiki%3ASpam-blacklist\n",
      "Disallow: /wiki/MediaWiki_talk:Spam-blacklist\n",
      "Disallow: /wiki/MediaWiki_talk%3ASpam-blacklist\n",
      "Disallow: /wiki/Wikipedia:WikiProject_Spam\n",
      "Disallow: /wiki/Wikipedia_talk:WikiProject_Spam\n",
      "#\n",
      "# Folks get annoyed when XfD discussions end up the number 1 google hit for\n",
      "# their name. \n",
      "# https://phabricator.wikimedia.org/T16075\n",
      "Disallow: /wiki/Wikipedia:Articles_for_deletion\n",
      "Disallow: /wiki/Wikipedia%3AArticles_for_deletion\n",
      "Disallow: /wiki/Wikipedia:Votes_for_deletion\n",
      "Disallow: /wiki/Wikipedia%3AVotes_for_deletion\n",
      "Disallow: /wiki/Wikipedia:Pages_for_deletion\n",
      "Disallow: /wiki/Wikipedia%3APages_for_deletion\n",
      "Disallow: /wiki/Wikipedia:Miscellany_for_deletion\n",
      "Disallow: /wiki/Wikipedia%3AMiscellany_for_deletion\n",
      "Disallow: /wiki/Wikipedia:Miscellaneous_deletion\n",
      "Disallow: /wiki/Wikipedia%3AMiscellaneous_deletion\n",
      "Disallow: /wiki/Wikipedia:Categories_for_discussion\n",
      "Disallow: /wiki/Wikipedia%3ACategories_for_discussion\n",
      "Disallow: /wiki/Wikipedia:Templates_for_deletion\n",
      "Disallow: /wiki/Wikipedia%3ATemplates_for_deletion\n",
      "Disallow: /wiki/Wikipedia:Redirects_for_discussion\n",
      "Disallow: /wiki/Wikipedia%3ARedirects_for_discussion\n",
      "Disallow: /wiki/Wikipedia:Deletion_review\n",
      "Disallow: /wiki/Wikipedia%3ADeletion_review\n",
      "Disallow: /wiki/Wikipedia:WikiProject_Deletion_sorting\n",
      "Disallow: /wiki/Wikipedia%3AWikiProject_Deletion_sorting\n",
      "Disallow: /wiki/Wikipedia:Files_for_deletion\n",
      "Disallow: /wiki/Wikipedia%3AFiles_for_deletion\n",
      "Disallow: /wiki/Wikipedia:Files_for_discussion\n",
      "Disallow: /wiki/Wikipedia%3AFiles_for_discussion\n",
      "Disallow: /wiki/Wikipedia:Possibly_unfree_files\n",
      "Disallow: /wiki/Wikipedia%3APossibly_unfree_files\n",
      "#\n",
      "# https://phabricator.wikimedia.org/T12288\n",
      "Disallow: /wiki/Wikipedia_talk:Articles_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3AArticles_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Votes_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3AVotes_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Pages_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3APages_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Miscellany_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3AMiscellany_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Miscellaneous_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3AMiscellaneous_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Templates_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3ATemplates_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Categories_for_discussion\n",
      "Disallow: /wiki/Wikipedia_talk%3ACategories_for_discussion\n",
      "Disallow: /wiki/Wikipedia_talk:Deletion_review\n",
      "Disallow: /wiki/Wikipedia_talk%3ADeletion_review\n",
      "Disallow: /wiki/Wikipedia_talk:WikiProject_Deletion_sorting\n",
      "Disallow: /wiki/Wikipedia_talk%3AWikiProject_Deletion_sorting\n",
      "Disallow: /wiki/Wikipedia_talk:Files_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk%3AFiles_for_deletion\n",
      "Disallow: /wiki/Wikipedia_talk:Files_for_discussion\n",
      "Disallow: /wiki/Wikipedia_talk%3AFiles_for_discussion\n",
      "Disallow: /wiki/Wikipedia_talk:Possibly_unfree_files\n",
      "Disallow: /wiki/Wikipedia_talk%3APossibly_unfree_files\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Copyright_problems\n",
      "Disallow: /wiki/Wikipedia%3ACopyright_problems\n",
      "Disallow: /wiki/Wikipedia_talk:Copyright_problems\n",
      "Disallow: /wiki/Wikipedia_talk%3ACopyright_problems\n",
      "Disallow: /wiki/Wikipedia:Suspected_copyright_violations\n",
      "Disallow: /wiki/Wikipedia%3ASuspected_copyright_violations\n",
      "Disallow: /wiki/Wikipedia_talk:Suspected_copyright_violations\n",
      "Disallow: /wiki/Wikipedia_talk%3ASuspected_copyright_violations\n",
      "Disallow: /wiki/Wikipedia:Contributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia%3AContributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia:Contributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia%3AContributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia_talk:Contributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia_talk%3AContributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia_talk:Contributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia_talk%3AContributor_copyright_investigations\n",
      "Disallow: /wiki/Wikipedia:Protected_titles\n",
      "Disallow: /wiki/Wikipedia%3AProtected_titles\n",
      "Disallow: /wiki/Wikipedia_talk:Protected_titles\n",
      "Disallow: /wiki/Wikipedia_talk%3AProtected_titles\n",
      "Disallow: /wiki/Wikipedia:Articles_for_creation\n",
      "Disallow: /wiki/Wikipedia%3AArticles_for_creation\n",
      "Disallow: /wiki/Wikipedia_talk:Articles_for_creation\n",
      "Disallow: /wiki/Wikipedia_talk%3AArticles_for_creation\n",
      "Disallow: /wiki/Wikipedia_talk:Article_wizard\n",
      "Disallow: /wiki/Wikipedia_talk%3AArticle_wizard\n",
      "#\n",
      "# https://phabricator.wikimedia.org/T13261\n",
      "Disallow: /wiki/Wikipedia:Requests_for_arbitration\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_arbitration\n",
      "Disallow: /wiki/Wikipedia_talk:Requests_for_arbitration\n",
      "Disallow: /wiki/Wikipedia_talk%3ARequests_for_arbitration\n",
      "Disallow: /wiki/Wikipedia:Requests_for_comment\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_comment\n",
      "Disallow: /wiki/Wikipedia_talk:Requests_for_comment\n",
      "Disallow: /wiki/Wikipedia_talk%3ARequests_for_comment\n",
      "Disallow: /wiki/Wikipedia:Requests_for_adminship\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_adminship\n",
      "Disallow: /wiki/Wikipedia_talk:Requests_for_adminship\n",
      "Disallow: /wiki/Wikipedia_talk%3ARequests_for_adminship\n",
      "#\n",
      "# https://phabricator.wikimedia.org/T14111\n",
      "Disallow: /wiki/Wikipedia:Requests_for_checkuser\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_checkuser\n",
      "Disallow: /wiki/Wikipedia_talk:Requests_for_checkuser\n",
      "Disallow: /wiki/Wikipedia_talk%3ARequests_for_checkuser\n",
      "#\n",
      "# https://phabricator.wikimedia.org/T15398\n",
      "Disallow: /wiki/Wikipedia:WikiProject_Spam\n",
      "Disallow: /wiki/Wikipedia%3AWikiProject_Spam\n",
      "#\n",
      "# https://phabricator.wikimedia.org/T16793\n",
      "Disallow: /wiki/Wikipedia:Changing_username\n",
      "Disallow: /wiki/Wikipedia%3AChanging_username\n",
      "Disallow: /wiki/Wikipedia:Changing_username\n",
      "Disallow: /wiki/Wikipedia%3AChanging_username\n",
      "Disallow: /wiki/Wikipedia_talk:Changing_username\n",
      "Disallow: /wiki/Wikipedia_talk%3AChanging_username\n",
      "Disallow: /wiki/Wikipedia_talk:Changing_username\n",
      "Disallow: /wiki/Wikipedia_talk%3AChanging_username\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Administrators%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia%3AAdministrators%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Administrators%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3AAdministrators%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia:Community_sanction_noticeboard\n",
      "Disallow: /wiki/Wikipedia%3ACommunity_sanction_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Community_sanction_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3ACommunity_sanction_noticeboard\n",
      "Disallow: /wiki/Wikipedia:Bureaucrats%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia%3ABureaucrats%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Bureaucrats%27_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3ABureaucrats%27_noticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Sockpuppet_investigations\n",
      "Disallow: /wiki/Wikipedia%3ASockpuppet_investigations\n",
      "Disallow: /wiki/Wikipedia_talk:Sockpuppet_investigations\n",
      "Disallow: /wiki/Wikipedia_talk%3ASockpuppet_investigations\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Neutral_point_of_view/Noticeboard\n",
      "Disallow: /wiki/Wikipedia%3ANeutral_point_of_view/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Neutral_point_of_view/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3ANeutral_point_of_view/Noticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:No_original_research/noticeboard\n",
      "Disallow: /wiki/Wikipedia%3ANo_original_research/noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:No_original_research/noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3ANo_original_research/noticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Fringe_theories/Noticeboard\n",
      "Disallow: /wiki/Wikipedia%3AFringe_theories/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Fringe_theories/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3AFringe_theories/Noticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Conflict_of_interest/Noticeboard\n",
      "Disallow: /wiki/Wikipedia%3AConflict_of_interest/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Conflict_of_interest/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3AConflict_of_interest/Noticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Long-term_abuse\n",
      "Disallow: /wiki/Wikipedia%3ALong-term_abuse\n",
      "Disallow: /wiki/Wikipedia_talk:Long-term_abuse\n",
      "Disallow: /wiki/Wikipedia_talk%3ALong-term_abuse\n",
      "Disallow: /wiki/Wikipedia:Long_term_abuse\n",
      "Disallow: /wiki/Wikipedia%3ALong_term_abuse\n",
      "Disallow: /wiki/Wikipedia_talk:Long_term_abuse\n",
      "Disallow: /wiki/Wikipedia_talk%3ALong_term_abuse\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Wikiquette_assistance\n",
      "Disallow: /wiki/Wikipedia%3AWikiquette_assistance\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Abuse_reports\n",
      "Disallow: /wiki/Wikipedia%3AAbuse_reports\n",
      "Disallow: /wiki/Wikipedia_talk:Abuse_reports\n",
      "Disallow: /wiki/Wikipedia_talk%3AAbuse_reports\n",
      "Disallow: /wiki/Wikipedia:Abuse_response\n",
      "Disallow: /wiki/Wikipedia%3AAbuse_response\n",
      "Disallow: /wiki/Wikipedia_talk:Abuse_response\n",
      "Disallow: /wiki/Wikipedia_talk%3AAbuse_response\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Reliable_sources/Noticeboard\n",
      "Disallow: /wiki/Wikipedia%3AReliable_sources/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Reliable_sources/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3AReliable_sources/Noticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Suspected_sock_puppets\n",
      "Disallow: /wiki/Wikipedia%3ASuspected_sock_puppets\n",
      "Disallow: /wiki/Wikipedia_talk:Suspected_sock_puppets\n",
      "Disallow: /wiki/Wikipedia_talk%3ASuspected_sock_puppets\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Biographies_of_living_persons/Noticeboard\n",
      "Disallow: /wiki/Wikipedia%3ABiographies_of_living_persons/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Biographies_of_living_persons/Noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3ABiographies_of_living_persons/Noticeboard\n",
      "Disallow: /wiki/Wikipedia:Biographies_of_living_persons%2FNoticeboard\n",
      "Disallow: /wiki/Wikipedia%3ABiographies_of_living_persons%2FNoticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Biographies_of_living_persons%2FNoticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3ABiographies_of_living_persons%2FNoticeboard\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Content_noticeboard\n",
      "Disallow: /wiki/Wikipedia%3AContent_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk:Content_noticeboard\n",
      "Disallow: /wiki/Wikipedia_talk%3AContent_noticeboard\n",
      "#\n",
      "Disallow: /wiki/Template:Editnotices\n",
      "Disallow: /wiki/Template%3AEditnotices\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Arbitration\n",
      "Disallow: /wiki/Wikipedia%3AArbitration\n",
      "Disallow: /wiki/Wikipedia_talk:Arbitration\n",
      "Disallow: /wiki/Wikipedia_talk%3AArbitration\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Arbitration_Committee\n",
      "Disallow: /wiki/Wikipedia%3AArbitration_Committee\n",
      "Disallow: /wiki/Wikipedia_talk:Arbitration_Committee\n",
      "Disallow: /wiki/Wikipedia_talk%3AArbitration_Committee\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Arbitration_Committee_Elections\n",
      "Disallow: /wiki/Wikipedia%3AArbitration_Committee_Elections\n",
      "Disallow: /wiki/Wikipedia_talk:Arbitration_Committee_Elections\n",
      "Disallow: /wiki/Wikipedia_talk%3AArbitration_Committee_Elections\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Mediation_Committee\n",
      "Disallow: /wiki/Wikipedia%3AMediation_Committee\n",
      "Disallow: /wiki/Wikipedia_talk:Mediation_Committee\n",
      "Disallow: /wiki/Wikipedia_talk%3AMediation_Committee\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Mediation_Cabal/Cases\n",
      "Disallow: /wiki/Wikipedia%3AMediation_Cabal/Cases\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Requests_for_bureaucratship\n",
      "Disallow: /wiki/Wikipedia%3ARequests_for_bureaucratship\n",
      "Disallow: /wiki/Wikipedia_talk:Requests_for_bureaucratship\n",
      "Disallow: /wiki/Wikipedia_talk%3ARequests_for_bureaucratship\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Administrator_review\n",
      "Disallow: /wiki/Wikipedia%3AAdministrator_review\n",
      "Disallow: /wiki/Wikipedia_talk:Administrator_review\n",
      "Disallow: /wiki/Wikipedia_talk%3AAdministrator_review\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Editor_review\n",
      "Disallow: /wiki/Wikipedia%3AEditor_review\n",
      "Disallow: /wiki/Wikipedia_talk:Editor_review\n",
      "Disallow: /wiki/Wikipedia_talk%3AEditor_review\n",
      "#\n",
      "Disallow: /wiki/Wikipedia:Article_Incubator\n",
      "Disallow: /wiki/Wikipedia%3AArticle_Incubator\n",
      "Disallow: /wiki/Wikipedia_talk:Article_Incubator\n",
      "Disallow: /wiki/Wikipedia_talk%3AArticle_Incubator\n",
      "#\n",
      "Disallow: /wiki/Category:Noindexed_pages\n",
      "Disallow: /wiki/Category%3ANoindexed_pages\n",
      "#\n",
      "# User sandboxes for modules and Template Styles are placed in these subpages for testing\n",
      "#\n",
      "Disallow: /wiki/Module:Sandbox\n",
      "Disallow: /wiki/Module%3ASandbox\n",
      "Disallow: /wiki/Template:TemplateStyles_sandbox\n",
      "Disallow: /wiki/Template%3ATemplateStyles_sandbox\n",
      "#\n",
      "# </pre></body></html>\n"
     ]
    }
   ],
   "source": [
    "check(url=\"https://en.wikipedia.org/wiki/Main_Page\",\n",
    "     url2=\"https://en.wikipedia.org/robots.txt\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de25e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki(url):                                             # Defining function to get headings from wikipedia's main page\n",
    "    page= requests.get(url)                                     # for getting respnse and hitting wikipedia server\n",
    "    parse= BeautifulSoup(page.content)                       # to get contents of the webpage\n",
    "    title=[]                                                 # Creating an empty list to store data\n",
    "    for i in parse.find_all('span', class_=\"mw-headline\"):  # condition to get headings\n",
    "        title.append(i.text)                                # storing headers in the empty list we had created\n",
    "    heading= pd.DataFrame(title, columns=[\"Title\"])         # Creating a Data Frame or table\n",
    "    print(heading)                                          # Displaying results in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b8404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Title\n",
      "0           Welcome to Wikipedia\n",
      "1  From today's featured article\n",
      "2               Did you know ...\n",
      "3                    In the news\n",
      "4                    On this day\n",
      "5       Today's featured picture\n",
      "6       Other areas of Wikipedia\n",
      "7    Wikipedia's sister projects\n",
      "8            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "wiki(url=\"https://en.wikipedia.org/wiki/Main_Page\")  # Recalling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce4305",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "and make data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5ca34",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae3ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n",
    "    page2=requests.get(url2)                  # To connect with the desired webpage\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce2cda",
   "metadata": {},
   "source": [
    "Recalling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1effbad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p># robots.txt for https://www.imdb.com properties\n",
      "User-agent: *\n",
      "Disallow: /OnThisDay\n",
      "Disallow: /ads/\n",
      "Disallow: /ap/\n",
      "Disallow: /mymovies/\n",
      "Disallow: /r/\n",
      "Disallow: /register\n",
      "Disallow: /registration/\n",
      "Disallow: /search/name-text\n",
      "Disallow: /search/title-text\n",
      "Disallow: /find\n",
      "Disallow: /find$\n",
      "Disallow: /find/\n",
      "Disallow: /tvschedule\n",
      "Disallow: /updates\n",
      "Disallow: /watch/_ajax/option\n",
      "Disallow: /_json/video/mon\n",
      "Disallow: /_json/getAdsForMediaViewer/\n",
      "Disallow: /list/ls*/_ajax\n",
      "Disallow: /*/*/rg*/mediaviewer/rm*/tr\n",
      "Disallow: /*/rg*/mediaviewer/rm*/tr\n",
      "Disallow: /*/mediaviewer/*/tr\n",
      "Disallow: /title/tt*/mediaviewer/rm*/tr\n",
      "Disallow: /name/nm*/mediaviewer/rm*/tr\n",
      "Disallow: /gallery/rg*/mediaviewer/rm*/tr\n",
      "Disallow: /tr/\n",
      "Disallow: /title/tt*/watchoptions\n",
      "Disallow: /search/title/?title_type=feature,tv_movie,tv_miniseries,documentary,short,video,tv_short&amp;release_date=,2020-12-31&amp;lists=%21ls538187658,%21ls539867036,%21ls538186228&amp;view=simple&amp;sort=num_votes,asc&amp;aft\n",
      "\n",
      "User-agent: Baiduspider\n",
      "Disallow: /list/*\n",
      "Disallow: /user/*</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "check(url=\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\",\n",
    "     url2=\"https://www.imdb.com/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f29a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Function\n",
    "def imdb(url1,url2):\n",
    "\n",
    "# Creating empty list\n",
    "    col1=[]\n",
    "    col2=[]\n",
    "    col3=[]\n",
    "    col4=[]\n",
    "    col5=[]\n",
    "    col6=[]\n",
    "    col7=[]\n",
    "    col8=[]\n",
    "    \n",
    "# Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "# To establish connection with the required webpage\n",
    "    page1= requests.get(url1)\n",
    "    page2= requests.get(url2)\n",
    "\n",
    "# Fetching contents of the webpage\n",
    "    parse1=BeautifulSoup(page1.content)\n",
    "    parse2= BeautifulSoup(page2.content)\n",
    "    \n",
    "# Condition to fetch the desired content and saving the result in the specified list    \n",
    "    for i in parse1.find_all('span', class_=\"lister-item-index unbold text-primary\"):\n",
    "        col1.append(i.text)\n",
    "        \n",
    "    for i in parse1.find_all('h3', class_=\"lister-item-header\"):\n",
    "        col2.append(i.text)\n",
    "        \n",
    "    for i in parse1.find_all('div', class_=\"inline-block ratings-imdb-rating\"):\n",
    "        col3.append(i.text)\n",
    "        \n",
    "    for i in parse1.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
    "        col4.append(i.text)\n",
    "        \n",
    "    for i in parse2.find_all('span', class_=\"lister-item-index unbold text-primary\"):\n",
    "        col5.append(i.text)\n",
    "        \n",
    "    for i in parse2.find_all('h3', class_=\"lister-item-header\"):\n",
    "        col6.append(i.text)\n",
    "        \n",
    "    for i in parse2.find_all('div', class_=\"inline-block ratings-imdb-rating\"):\n",
    "        col7.append(i.text)\n",
    "        \n",
    "    for i in parse2.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
    "        col8.append(i.text)\n",
    " # Creating Data Frames\n",
    "    table1= pd.DataFrame({\"Rank\":col1, \"Name\":col2,\"Rating\":col3,\"Year Of Release\":col4 })      \n",
    "    table2= pd.DataFrame({\"Rank\":col5, \"Name\":col6,\"Rating\":col7,\"Year Of Release\":col8 })  \n",
    "    table= pd.DataFrame()\n",
    "    \n",
    "# Merging the results in two tables to a single table\n",
    "    table= pd.concat([table1,table2],axis=0,)\n",
    "# To remove \"\\n\" tags\n",
    "    table=table.replace('\\n','',regex=True)\n",
    "    \n",
    "# To drop index from being displayed\n",
    "    table=table.to_string(index=False, col_space=15,)\n",
    "    \n",
    "# To display result\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa5bad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Rank                                                                          Name          Rating Year Of Release\n",
      "             1.                                              1.The Shawshank Redemption(1994)             9.3          (1994)\n",
      "             2.                                                         2.The Godfather(1972)             9.2          (1972)\n",
      "             3.                                                       3.The Dark Knight(2008)             9.0          (2008)\n",
      "             4.                         4.The Lord of the Rings: The Return of the King(2003)             9.0          (2003)\n",
      "             5.                                                      5.Schindler's List(1993)             9.0          (1993)\n",
      "             6.                                                 6.The Godfather Part II(1974)             9.0          (1974)\n",
      "             7.                                                          7.12 Angry Men(1957)             9.0          (1957)\n",
      "             8.                                                          8.Pulp Fiction(1994)             8.9          (1994)\n",
      "             9.                                                             9.Inception(2010)             8.8          (2010)\n",
      "            10.                                10.The Lord of the Rings: The Two Towers(2002)             8.8          (2002)\n",
      "            11.                                                           11.Fight Club(1999)             8.8          (1999)\n",
      "            12.                    12.The Lord of the Rings: The Fellowship of the Ring(2001)             8.8          (2001)\n",
      "            13.                                                         13.Forrest Gump(1994)             8.8          (1994)\n",
      "            14.                                      14.Il buono, il brutto, il cattivo(1966)             8.8          (1966)\n",
      "            15.                                                           15.The Matrix(1999)             8.7          (1999)\n",
      "            16.                                                           16.Goodfellas(1990)             8.7          (1990)\n",
      "            17.                                              17.The Empire Strikes Back(1980)             8.7          (1980)\n",
      "            18.                                      18.One Flew Over the Cuckoo's Nest(1975)             8.7          (1975)\n",
      "            19.                                                    19.Top Gun: Maverick(2022)             8.6          (2022)\n",
      "            20.                                                         20.Interstellar(2014)             8.6          (2014)\n",
      "            21.                                                       21.Cidade de Deus(2002)             8.6          (2002)\n",
      "            22.                                        22.Sen to Chihiro no kamikakushi(2001)             8.6          (2001)\n",
      "            23.                                                  23.Saving Private Ryan(1998)             8.6          (1998)\n",
      "            24.                                                       24.The Green Mile(1999)             8.6          (1999)\n",
      "            25.                                                      25.La vita è bella(1997)             8.6          (1997)\n",
      "            26.                                                                26.Se7en(1995)             8.6          (1995)\n",
      "            27.                                           27.Terminator 2: Judgment Day(1991)             8.6          (1991)\n",
      "            28.                                             28.The Silence of the Lambs(1991)             8.6          (1991)\n",
      "            29.                                                            29.Star Wars(1977)             8.6          (1977)\n",
      "            30.                                                              30.Seppuku(1962)             8.6          (1962)\n",
      "            31.                                                 31.Shichinin no samurai(1954)             8.6          (1954)\n",
      "            32.                                                32.It's a Wonderful Life(1946)             8.6          (1946)\n",
      "            33.                                                         33.Gisaengchung(2019)             8.5          (2019)\n",
      "            34.                                                             34.Whiplash(2014)             8.5          (2014)\n",
      "            35.                                                     35.The Intouchables(2011)             8.5          (2011)\n",
      "            36.                                                         36.The Prestige(2006)             8.5          (2006)\n",
      "            37.                                                         37.The Departed(2006)             8.5          (2006)\n",
      "            38.                                                          38.The Pianist(2002)             8.5          (2002)\n",
      "            39.                                                            39.Gladiator(2000)             8.5          (2000)\n",
      "            40.                                                   40.American History X(1998)             8.5          (1998)\n",
      "            41.                                                   41.The Usual Suspects(1995)             8.5          (1995)\n",
      "            42.                                                                 42.Léon(1994)             8.5          (1994)\n",
      "            43.                                                        43.The Lion King(1994)             8.5          (1994)\n",
      "            44.                                                44.Nuovo Cinema Paradiso(1988)             8.5          (1988)\n",
      "            45.                                                       45.Hotaru no haka(1988)             8.5          (1988)\n",
      "            46.                                                   46.Back to the Future(1985)             8.5          (1985)\n",
      "            47.                                                       47.Apocalypse Now(1979)             8.5          (1979)\n",
      "            48.                                                                48.Alien(1979)             8.5          (1979)\n",
      "            49.                                         49.Once Upon a Time in the West(1968)             8.5          (1968)\n",
      "            50.                                                               50.Psycho(1960)             8.5          (1960)\n",
      "            51.                                                          51.Rear Window(1954)             8.5          (1954)\n",
      "            52.                                                           52.Casablanca(1942)             8.5          (1942)\n",
      "            53.                                                         53.Modern Times(1936)             8.5          (1936)\n",
      "            54.                                                          54.City Lights(1931)             8.5          (1931)\n",
      "            55.                                                           55.Capharnaüm(2018)             8.4          (2018)\n",
      "            56.                                                            56.Joker(I) (2019)             8.4      (I) (2019)\n",
      "            57.                                                       57.Kimi no na wa.(2016)             8.4          (2016)\n",
      "            58.                                    58.Spider-Man: Into the Spider-Verse(2018)             8.4          (2018)\n",
      "            59.                                                    59.Avengers: Endgame(2019)             8.4          (2019)\n",
      "            60.                                               60.Avengers: Infinity War(2018)             8.4          (2018)\n",
      "            61.                                                             61.Coco(I) (2017)             8.4      (I) (2017)\n",
      "            62.                                                     62.Django Unchained(2012)             8.4          (2012)\n",
      "            63.                                                63.The Dark Knight Rises(2012)             8.4          (2012)\n",
      "            64.                                                             64.3 Idiots(2009)             8.4          (2009)\n",
      "            65.                                                               65.WALL·E(2008)             8.4          (2008)\n",
      "            66.                                                  66.The Lives of Others(2006)             8.4          (2006)\n",
      "            67.                                                             67.Oldeuboi(2003)             8.4          (2003)\n",
      "            68.                                                              68.Memento(2000)             8.4          (2000)\n",
      "            69.                                                      69.American Beauty(1999)             8.4          (1999)\n",
      "            70.                                                        70.Mononoke-hime(1997)             8.4          (1997)\n",
      "            71.                                                           71.Braveheart(1995)             8.4          (1995)\n",
      "            72.                                                               72.Aliens(1986)             8.4          (1986)\n",
      "            73.                                                              73.Amadeus(1984)             8.4          (1984)\n",
      "            74.                                              74.Raiders of the Lost Ark(1981)             8.4          (1981)\n",
      "            75.                                                             75.Das Boot(1981)             8.4          (1981)\n",
      "            76.                                                          76.The Shining(1980)             8.4          (1980)\n",
      "            77.                                                    77.Tengoku to jigoku(1963)             8.4          (1963)\n",
      "            78. 78.Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb(1964)             8.4          (1964)\n",
      "            79.                                          79.Witness for the Prosecution(1957)             8.4          (1957)\n",
      "            80.                                                       80.Paths of Glory(1957)             8.4          (1957)\n",
      "            81.                                                         81.Sunset Blvd.(1950)             8.4          (1950)\n",
      "            82.                                                   82.The Great Dictator(1940)             8.4          (1940)\n",
      "            83.                                                               83.Jagten(2012)             8.3          (2012)\n",
      "            84.                                                          84.Toy Story 3(2010)             8.3          (2010)\n",
      "            85.                                                 85.Inglourious Basterds(2009)             8.3          (2009)\n",
      "            86.                                86.Eternal Sunshine of the Spotless Mind(2004)             8.3          (2004)\n",
      "            87.                                  87.Le fabuleux destin d'Amélie Poulain(2001)             8.3          (2001)\n",
      "            88.                                                  88.Requiem for a Dream(2000)             8.3          (2000)\n",
      "            89.                                                    89.Good Will Hunting(1997)             8.3          (1997)\n",
      "            90.                                                            90.Toy Story(1995)             8.3          (1995)\n",
      "            91.                                                       91.Reservoir Dogs(1992)             8.3          (1992)\n",
      "            92.                                          92.Once Upon a Time in America(1984)             8.3          (1984)\n",
      "            93.                           93.Star Wars: Episode VI - Return of the Jedi(1983)             8.3          (1983)\n",
      "            94.                                                94.2001: A Space Odyssey(1968)             8.3          (1968)\n",
      "            95.                                                   95.Lawrence of Arabia(1962)             8.3          (1962)\n",
      "            96.                                                   96.North by Northwest(1959)             8.3          (1959)\n",
      "            97.                                                              97.Vertigo(1958)             8.3          (1958)\n",
      "            98.                                                  98.Singin' in the Rain(1952)             8.3          (1952)\n",
      "            99.                                                         99.Citizen Kane(1941)             8.3          (1941)\n",
      "           100.                                   100.M - Eine Stadt sucht einen Mörder(1931)             8.3          (1931)\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "imdb(url1=\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\",\n",
    "    url2=\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04be71f",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "    \n",
    "Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of\n",
    "release) and make data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab42eb",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d6a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n",
    "    page2=requests.get(url2)                  # To connect with the desired webpage\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e5a31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p># robots.txt for https://www.imdb.com properties\n",
      "User-agent: *\n",
      "Disallow: /OnThisDay\n",
      "Disallow: /ads/\n",
      "Disallow: /ap/\n",
      "Disallow: /mymovies/\n",
      "Disallow: /r/\n",
      "Disallow: /register\n",
      "Disallow: /registration/\n",
      "Disallow: /search/name-text\n",
      "Disallow: /search/title-text\n",
      "Disallow: /find\n",
      "Disallow: /find$\n",
      "Disallow: /find/\n",
      "Disallow: /tvschedule\n",
      "Disallow: /updates\n",
      "Disallow: /watch/_ajax/option\n",
      "Disallow: /_json/video/mon\n",
      "Disallow: /_json/getAdsForMediaViewer/\n",
      "Disallow: /list/ls*/_ajax\n",
      "Disallow: /*/*/rg*/mediaviewer/rm*/tr\n",
      "Disallow: /*/rg*/mediaviewer/rm*/tr\n",
      "Disallow: /*/mediaviewer/*/tr\n",
      "Disallow: /title/tt*/mediaviewer/rm*/tr\n",
      "Disallow: /name/nm*/mediaviewer/rm*/tr\n",
      "Disallow: /gallery/rg*/mediaviewer/rm*/tr\n",
      "Disallow: /tr/\n",
      "Disallow: /title/tt*/watchoptions\n",
      "Disallow: /search/title/?title_type=feature,tv_movie,tv_miniseries,documentary,short,video,tv_short&amp;release_date=,2020-12-31&amp;lists=%21ls538187658,%21ls539867036,%21ls538186228&amp;view=simple&amp;sort=num_votes,asc&amp;aft\n",
      "\n",
      "User-agent: Baiduspider\n",
      "Disallow: /list/*\n",
      "Disallow: /user/*</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://www.imdb.com/india/top-rated-indian-movies/?sort=rk,asc&mode=simple&page=1/\",\n",
    "     url2=\"https://www.imdb.com/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d708a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_indian(url):\n",
    "# Creating empty list\n",
    "    rank=[]\n",
    "    name=[]\n",
    "    rating=[]\n",
    "    year=[]\n",
    "        \n",
    "# Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "# To establish connection with the required webpage\n",
    "#url=\"https://www.imdb.com/india/top-rated-indian-movies/?sort=rk,asc&mode=simple&page=1\" \n",
    "    page= requests.get(url)\n",
    "    \n",
    "\n",
    "# Fetching contents of the webpage\n",
    "    parse=BeautifulSoup(page.content)\n",
    "\n",
    "# Condition to fetch the desired content and saving the result in the specified list    \n",
    "\n",
    "    for i in parse.find_all('td', class_=\"titleColumn\"):\n",
    "        rank.append(i.contents)\n",
    "        \n",
    "    for i in parse.find_all('a'):\n",
    "        name.append(i.text)\n",
    "        \n",
    "    for i in parse.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
    "        rating.append(i.contents)\n",
    "        \n",
    "    for i in parse.find_all('span', class_=\"secondaryInfo\"):\n",
    "        year.append(i.contents)\n",
    "        \n",
    "    \n",
    "    new_col2=name[58:557:2]\n",
    "    new_col4=year[0:250]\n",
    "    \n",
    "    table=pd.DataFrame(rank)\n",
    "    table1=table.loc[:,0:0:]\n",
    "    table1=table1.replace('\\n',\"\",regex=True)   \n",
    "    \n",
    "\n",
    "# To drop index from being displayed\n",
    "    \n",
    "    new_col1=table1.values.tolist()\n",
    "    table2= pd.DataFrame({\"Rank\":new_col1, \"Name\":new_col2,\"Rating\":rating,\"Year Of Release\":new_col4 }) \n",
    "    table2=table2.replace('\\n', regex=True)\n",
    "    print(table2.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec89bd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Rank                               Name           Rating  \\\n",
      "0     [      1.      ]                         Anbe Sivam  [\\n, [8.4], \\n]   \n",
      "1     [      2.      ]                           Jai Bhim  [\\n, [8.4], \\n]   \n",
      "2     [      3.      ]                            Golmaal  [\\n, [8.4], \\n]   \n",
      "3     [      4.      ]                            Nayakan  [\\n, [8.4], \\n]   \n",
      "4     [      5.      ]                  Pariyerum Perumal  [\\n, [8.4], \\n]   \n",
      "5     [      6.      ]                           3 Idiots  [\\n, [8.3], \\n]   \n",
      "6     [      7.      ]                        Apur Sansar  [\\n, [8.3], \\n]   \n",
      "7     [      8.      ]                   Manichitrathazhu  [\\n, [8.3], \\n]   \n",
      "8     [      9.      ]                  Kumbalangi Nights  [\\n, [8.3], \\n]   \n",
      "9    [      10.      ]                       Black Friday  [\\n, [8.3], \\n]   \n",
      "10   [      11.      ]                  C/o Kancharapalem  [\\n, [8.3], \\n]   \n",
      "11   [      12.      ]                   Taare Zameen Par  [\\n, [8.3], \\n]   \n",
      "12   [      13.      ]                              #Home  [\\n, [8.3], \\n]   \n",
      "13   [      14.      ]                    Soorarai Pottru  [\\n, [8.3], \\n]   \n",
      "14   [      15.      ]                             Vikram  [\\n, [8.3], \\n]   \n",
      "15   [      16.      ]                           Kireedam  [\\n, [8.3], \\n]   \n",
      "16   [      17.      ]                             Dangal  [\\n, [8.3], \\n]   \n",
      "17   [      18.      ]                             Kaithi  [\\n, [8.3], \\n]   \n",
      "18   [      19.      ]                             Jersey  [\\n, [8.3], \\n]   \n",
      "19   [      20.      ]                       Thevar Magan  [\\n, [8.2], \\n]   \n",
      "20   [      21.      ]                    Pather Panchali  [\\n, [8.2], \\n]   \n",
      "21   [      22.      ]                             Asuran  [\\n, [8.2], \\n]   \n",
      "22   [      23.      ]                                 96  [\\n, [8.2], \\n]   \n",
      "23   [      24.      ]                         Visaaranai  [\\n, [8.2], \\n]   \n",
      "24   [      25.      ]                         Thalapathi  [\\n, [8.2], \\n]   \n",
      "25   [      26.      ]                          Natsamrat  [\\n, [8.2], \\n]   \n",
      "26   [      27.      ]                Sarpatta Parambarai  [\\n, [8.2], \\n]   \n",
      "27   [      28.      ]                         Drishyam 2  [\\n, [8.2], \\n]   \n",
      "28   [      29.      ]                       Sardar Udham  [\\n, [8.2], \\n]   \n",
      "29   [      30.      ]                       Thani Oruvan  [\\n, [8.2], \\n]   \n",
      "30   [      31.      ]                          Aparajito  [\\n, [8.2], \\n]   \n",
      "31   [      32.      ]                       Vada Chennai  [\\n, [8.2], \\n]   \n",
      "32   [      33.      ]                 Jaane Bhi Do Yaaro  [\\n, [8.2], \\n]   \n",
      "33   [      34.      ]                  Khosla Ka Ghosla!  [\\n, [8.2], \\n]   \n",
      "34   [      35.      ]                           Drishyam  [\\n, [8.1], \\n]   \n",
      "35   [      36.      ]                      Chupke Chupke  [\\n, [8.1], \\n]   \n",
      "36   [      37.      ]                            Peranbu  [\\n, [8.1], \\n]   \n",
      "37   [      38.      ]        Agent Sai Srinivasa Athreya  [\\n, [8.1], \\n]   \n",
      "38   [      39.      ]                            Anniyan  [\\n, [8.1], \\n]   \n",
      "39   [      40.      ]                           Mahanati  [\\n, [8.1], \\n]   \n",
      "40   [      41.      ]                       Super Deluxe  [\\n, [8.1], \\n]   \n",
      "41   [      42.      ]                     Bangalore Days  [\\n, [8.1], \\n]   \n",
      "42   [      43.      ]                              Satya  [\\n, [8.1], \\n]   \n",
      "43   [      44.      ]                             Premam  [\\n, [8.1], \\n]   \n",
      "44   [      45.      ]                           Ratsasan  [\\n, [8.1], \\n]   \n",
      "45   [      46.      ]                          Devasuram  [\\n, [8.1], \\n]   \n",
      "46   [      47.      ]                 Bhaag Milkha Bhaag  [\\n, [8.1], \\n]   \n",
      "47   [      48.      ]                 Gangs of Wasseypur  [\\n, [8.1], \\n]   \n",
      "48   [      49.      ]                              Aruvi  [\\n, [8.1], \\n]   \n",
      "49   [      50.      ]                          Andhadhun  [\\n, [8.1], \\n]   \n",
      "50   [      51.      ]                           Drishyam  [\\n, [8.1], \\n]   \n",
      "51   [      52.      ]              Kannathil Muthamittal  [\\n, [8.1], \\n]   \n",
      "52   [      53.      ]                              Guide  [\\n, [8.1], \\n]   \n",
      "53   [      54.      ]                           Chithram  [\\n, [8.1], \\n]   \n",
      "54   [      55.      ]                             Shahid  [\\n, [8.1], \\n]   \n",
      "55   [      56.      ]                             Iruvar  [\\n, [8.1], \\n]   \n",
      "56   [      57.      ]                             Sairat  [\\n, [8.1], \\n]   \n",
      "57   [      58.      ]           Zindagi Na Milegi Dobara  [\\n, [8.1], \\n]   \n",
      "58   [      59.      ]                   Paan Singh Tomar  [\\n, [8.1], \\n]   \n",
      "59   [      60.      ]                       Vikram Vedha  [\\n, [8.1], \\n]   \n",
      "60   [      61.      ]                            Tumbbad  [\\n, [8.1], \\n]   \n",
      "61   [      62.      ]                          Mudhalvan  [\\n, [8.1], \\n]   \n",
      "62   [      63.      ]             Dhuruvangal Pathinaaru  [\\n, [8.1], \\n]   \n",
      "63   [      64.      ]                           Spadikam  [\\n, [8.1], \\n]   \n",
      "64   [      65.      ]                         Chhichhore  [\\n, [8.1], \\n]   \n",
      "65   [      66.      ]                              Black  [\\n, [8.1], \\n]   \n",
      "66   [      67.      ]             Swades: We, the People  [\\n, [8.1], \\n]   \n",
      "67   [      68.      ]                     Chak De! India  [\\n, [8.1], \\n]   \n",
      "68   [      69.      ]             Jo Jeeta Wohi Sikandar  [\\n, [8.1], \\n]   \n",
      "69   [      70.      ]                       Pudhu Pettai  [\\n, [8.1], \\n]   \n",
      "70   [      71.      ]                          Papanasam  [\\n, [8.1], \\n]   \n",
      "71   [      72.      ]                             Pyaasa  [\\n, [8.1], \\n]   \n",
      "72   [      73.      ]                      Soodhu Kavvum  [\\n, [8.0], \\n]   \n",
      "73   [      74.      ]                Munna Bhai M.B.B.S.  [\\n, [8.0], \\n]   \n",
      "74   [      75.      ]                                 PK  [\\n, [8.0], \\n]   \n",
      "75   [      76.      ]                         Article 15  [\\n, [8.0], \\n]   \n",
      "76   [      77.      ]                            Mandela  [\\n, [8.0], \\n]   \n",
      "77   [      78.      ]                              Queen  [\\n, [8.0], \\n]   \n",
      "78   [      79.      ]           Uri: The Surgical Strike  [\\n, [8.0], \\n]   \n",
      "79   [      80.      ]                             Talvar  [\\n, [8.0], \\n]   \n",
      "80   [      81.      ]                     Kaakkaa Muttai  [\\n, [8.0], \\n]   \n",
      "81   [      82.      ]  Lagaan: Once Upon a Time in India  [\\n, [8.0], \\n]   \n",
      "82   [      83.      ]                    OMG: Oh My God!  [\\n, [8.0], \\n]   \n",
      "83   [      84.      ]                        Jigarthanda  [\\n, [8.0], \\n]   \n",
      "84   [      85.      ]                          Sarfarosh  [\\n, [8.0], \\n]   \n",
      "85   [      86.      ]                              Udaan  [\\n, [8.0], \\n]   \n",
      "86   [      87.      ]                             Barfi!  [\\n, [8.0], \\n]   \n",
      "87   [      88.      ]           Theeran Adhigaaram Ondru  [\\n, [8.0], \\n]   \n",
      "88   [      89.      ]                             Sholay  [\\n, [8.0], \\n]   \n",
      "89   [      90.      ]                        Ustad Hotel  [\\n, [8.0], \\n]   \n",
      "90   [      91.      ]                         Hera Pheri  [\\n, [8.0], \\n]   \n",
      "91   [      92.      ]                        777 Charlie  [\\n, [8.0], \\n]   \n",
      "92   [      93.      ]         The Legend of Bhagat Singh  [\\n, [8.0], \\n]   \n",
      "93   [      94.      ]                             Angoor  [\\n, [8.0], \\n]   \n",
      "94   [      95.      ]                    Rang De Basanti  [\\n, [8.0], \\n]   \n",
      "95   [      96.      ]                             Baasha  [\\n, [8.0], \\n]   \n",
      "96   [      97.      ]                             Masaan  [\\n, [8.0], \\n]   \n",
      "97   [      98.      ]        Baahubali 2: The Conclusion  [\\n, [8.0], \\n]   \n",
      "98   [      99.      ]                            Kahaani  [\\n, [8.0], \\n]   \n",
      "99  [      100.      ]                     Dil Chahta Hai  [\\n, [8.0], \\n]   \n",
      "\n",
      "   Year Of Release  \n",
      "0         [(2003)]  \n",
      "1         [(2021)]  \n",
      "2         [(1979)]  \n",
      "3         [(1987)]  \n",
      "4         [(2018)]  \n",
      "5         [(2009)]  \n",
      "6         [(1959)]  \n",
      "7         [(1993)]  \n",
      "8         [(2019)]  \n",
      "9         [(2004)]  \n",
      "10        [(2018)]  \n",
      "11        [(2007)]  \n",
      "12        [(2021)]  \n",
      "13        [(2020)]  \n",
      "14        [(2022)]  \n",
      "15        [(1989)]  \n",
      "16        [(2016)]  \n",
      "17        [(2019)]  \n",
      "18        [(2019)]  \n",
      "19        [(1992)]  \n",
      "20        [(1955)]  \n",
      "21        [(2019)]  \n",
      "22        [(2018)]  \n",
      "23        [(2015)]  \n",
      "24        [(1991)]  \n",
      "25        [(2016)]  \n",
      "26        [(2021)]  \n",
      "27        [(2021)]  \n",
      "28        [(2021)]  \n",
      "29        [(2015)]  \n",
      "30        [(1956)]  \n",
      "31        [(2018)]  \n",
      "32        [(1983)]  \n",
      "33        [(2006)]  \n",
      "34        [(2013)]  \n",
      "35        [(1975)]  \n",
      "36        [(2018)]  \n",
      "37        [(2019)]  \n",
      "38        [(2005)]  \n",
      "39        [(2018)]  \n",
      "40        [(2019)]  \n",
      "41        [(2014)]  \n",
      "42        [(1998)]  \n",
      "43        [(2015)]  \n",
      "44        [(2018)]  \n",
      "45        [(1993)]  \n",
      "46        [(2013)]  \n",
      "47        [(2012)]  \n",
      "48        [(2016)]  \n",
      "49        [(2018)]  \n",
      "50        [(2015)]  \n",
      "51        [(2002)]  \n",
      "52        [(1965)]  \n",
      "53        [(1988)]  \n",
      "54        [(2012)]  \n",
      "55        [(1997)]  \n",
      "56        [(2016)]  \n",
      "57        [(2011)]  \n",
      "58        [(2012)]  \n",
      "59        [(2017)]  \n",
      "60        [(2018)]  \n",
      "61        [(1999)]  \n",
      "62        [(2016)]  \n",
      "63        [(1995)]  \n",
      "64        [(2019)]  \n",
      "65        [(2005)]  \n",
      "66        [(2004)]  \n",
      "67        [(2007)]  \n",
      "68        [(1992)]  \n",
      "69        [(2006)]  \n",
      "70        [(2015)]  \n",
      "71        [(1957)]  \n",
      "72        [(2013)]  \n",
      "73        [(2003)]  \n",
      "74        [(2014)]  \n",
      "75        [(2019)]  \n",
      "76        [(2021)]  \n",
      "77        [(2013)]  \n",
      "78        [(2019)]  \n",
      "79        [(2015)]  \n",
      "80        [(2014)]  \n",
      "81        [(2001)]  \n",
      "82        [(2012)]  \n",
      "83        [(2014)]  \n",
      "84        [(1999)]  \n",
      "85        [(2010)]  \n",
      "86        [(2012)]  \n",
      "87        [(2017)]  \n",
      "88        [(1975)]  \n",
      "89        [(2012)]  \n",
      "90        [(2000)]  \n",
      "91        [(2022)]  \n",
      "92        [(2002)]  \n",
      "93        [(1982)]  \n",
      "94        [(2006)]  \n",
      "95        [(1995)]  \n",
      "96        [(2015)]  \n",
      "97        [(2017)]  \n",
      "98        [(2012)]  \n",
      "99        [(2001)]  \n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "imdb_indian(url=\"https://www.imdb.com/india/top-rated-indian-movies/?sort=rk,asc&mode=simple&page=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2a219",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa5131",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1049d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94c14a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\" https://presidentofindia.nic.in/former-presidents.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2b3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Function\n",
    "def president(url):\n",
    "\n",
    "# Creating empty list\n",
    "    col1=[]\n",
    "    col2=[]\n",
    "            \n",
    "# Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "# To establish connection with the required webpage\n",
    "    page= requests.get(url)\n",
    "    \n",
    "\n",
    "# Fetching contents of the webpage\n",
    "    parse=BeautifulSoup(page.content)\n",
    "        \n",
    "# Condition to fetch the desired content and saving the result in the specified list    \n",
    "    for i in parse.find_all('div', class_=\"presidentListing\"):\n",
    "        col1.append(i.contents)\n",
    "\n",
    "    table=pd.DataFrame(col1)\n",
    "    \n",
    "    name=table.loc[:,1:1]\n",
    "    terms=table.loc[:,3:3]\n",
    "    name=name.replace('\\n',\"\",regex=True)\n",
    "    terms=terms.replace('\\n',\"\",regex=True)\n",
    "    president=pd.DataFrame()\n",
    "    president=pd.concat([name,terms], axis=1)\n",
    "    president.reset_index()\n",
    "    print(president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c5ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                1  \\\n",
      "0             [Shri Pranab Mukherjee (1935-2020)]   \n",
      "1   [Smt Pratibha Devisingh Patil (birth - 1934)]   \n",
      "2            [DR. A.P.J. Abdul Kalam (1931-2015)]   \n",
      "3            [Shri K. R. Narayanan (1920 - 2005)]   \n",
      "4           [Dr Shankar Dayal Sharma (1918-1999)]   \n",
      "5               [Shri R Venkataraman (1910-2009)]   \n",
      "6                  [Giani Zail Singh (1916-1994)]   \n",
      "7         [Shri Neelam Sanjiva Reddy (1913-1996)]   \n",
      "8          [Dr. Fakhruddin Ali Ahmed (1905-1977)]   \n",
      "9      [Shri Varahagiri Venkata Giri (1894-1980)]   \n",
      "10                 [Dr. Zakir Husain (1897-1969)]   \n",
      "11     [Dr. Sarvepalli Radhakrishnan (1888-1975)]   \n",
      "12             [Dr. Rajendra Prasad (1884-1963) ]   \n",
      "\n",
      "                                                    3  \n",
      "0   [[Term of Office:],  25 July, 2012 to 25 July,...  \n",
      "1   [[Term of Office:],  25 July, 2007 to 25 July,...  \n",
      "2   [[Term of Office:],  25 July, 2002 to 25 July,...  \n",
      "3   [[Term of Office:],  25 July, 1997 to 25 July,...  \n",
      "4   [[Term of Office:],  25 July, 1992 to 25 July,...  \n",
      "5   [[Term of Office:],  25 July, 1987 to 25 July,...  \n",
      "6   [[Term of Office:],  25 July, 1982 to 25 July,...  \n",
      "7   [[Term of Office:],  25 July, 1977 to 25 July,...  \n",
      "8   [[Term of Office:],  24 August, 1974 to 11 Feb...  \n",
      "9   [[Term of Office:],  3 May, 1969 to 20 July, 1...  \n",
      "10  [[Term of Office:],  13 May, 1967 to 3 May, 1969]  \n",
      "11  [[Term of Office:],  13 May, 1962 to 13 May, 1...  \n",
      "12  [[Term of Office:],  26 January, 1950 to 13 Ma...  \n"
     ]
    }
   ],
   "source": [
    "president(url=\"https://presidentofindia.nic.in/former-presidents.htm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18bf8a9",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "\n",
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf160d",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb9784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d269ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://www.icc-cricket.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef763e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_men_a(url1):                     #Defining function\n",
    "    # Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "# To establish connection with the required webpage\n",
    "    page1=requests.get(url1)\n",
    "    \n",
    "# Fetching contents of the webpage\n",
    "    soup1=BeautifulSoup(page1.content)\n",
    "\n",
    "# Creating empty list to store data\n",
    "    col_rank=[]\n",
    "    col_team=[]\n",
    "    col_matches=[]\n",
    "    col_points=[]\n",
    "    col_ratings=[]\n",
    "    col5=[]\n",
    "# Condition to fetch the Top Ten ODI Team desired content and saving the result in the specified list  \n",
    "    \n",
    "    for i in soup1.find_all('td',class_=\"rankings-block__banner--pos\"):\n",
    "        col_rank.append(i.text)\n",
    "    for i in soup1.find_all('td',class_=\"table-body__cell table-body__cell--position u-text-right\"):\n",
    "        col_rank.extend(i.contents)\n",
    "    \n",
    "    for i in soup1.find_all('span', class_=\"u-show-phablet\"):\n",
    "        col_team.append(i.text)\n",
    "    for i in soup1.find_all('span', class_=\"u-show-phablet\"):\n",
    "        col_team.extend(i.contents) \n",
    "    \n",
    "    for i in soup1.find_all('td', class_=\"rankings-block__banner--matches\"):\n",
    "        col_matches.append(i.text)\n",
    "   \n",
    "    for i in soup1.find_all('td', class_=\"rankings-block__banner--points\"):\n",
    "        col_points.append(i.text)\n",
    "    \n",
    "    for i in soup1.find_all('td', class_=\"table-body__cell u-center-text\"):\n",
    "        col5.append(i.text)\n",
    "\n",
    "    for i in soup1.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        col_ratings.append(i.text)\n",
    "    for i in soup1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "        col_ratings.extend(i.contents)  \n",
    "# storing data as they were extracted from the same classes\n",
    "    pts=col5[1:19:2]\n",
    "    mat=col5[0:19:2]\n",
    "    \n",
    "\n",
    "# Concatenating lists\n",
    "    col_matches= col_matches + mat\n",
    "    col_points= col_points + pts\n",
    "    \n",
    "# Storing only top 10 records and maintaining the same number of arrays\n",
    "    col_rank=col_rank[0:10]\n",
    "    col_team=col_team[0:10]\n",
    "    col_matches=col_matches[0:10]\n",
    "    col_ratings=col_ratings[0:10]\n",
    "    col_points=col_points[0:10]\n",
    "\n",
    "# Creating Data Frames\n",
    "    table1= pd.DataFrame({\"Rank\":col_rank, \"Team\":col_team,\"Matches\":col_matches,\"Points\":col_points,\"Ratings\":col_ratings })\n",
    "    \n",
    "# To remove \"\\n\" tags\n",
    "    table1=table1.replace('\\n','',regex=True)\n",
    "    \n",
    "# To drop index from being displayed\n",
    "    table1=table1.to_string(index=False, col_space=15,justify='center')\n",
    "    \n",
    "    \n",
    "# To display result\n",
    "    print(\"Top 10 ICC-Womens ODI Team are:\",\"\\n\",table1,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641b1c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ICC-Womens ODI Team are: \n",
      "       Rank            Team          Matches          Points                               Ratings                          \n",
      "        1              NZ              12            1,505                                  125                            \n",
      "        2             ENG              22            2,756                                                              125\n",
      "        3             PAK              19            2,005                                                              106\n",
      "        4             IND              22            2,304                                                              105\n",
      "        5             AUS              23            2,325                                                              101\n",
      "        6              SA              19            1,872                                                               99\n",
      "        7             BAN              24            2,275                                                               95\n",
      "        8              SL              29            2,658                                                               92\n",
      "        9              WI              32            2,306                                                               72\n",
      "       10             AFG              18            1,238                                                               69 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "icc_men_a(url1=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1c240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_men(url2,url3):                     #Defining function\n",
    "# Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# To establish connection with the required webpage\n",
    "    page2=requests.get(url2)\n",
    "    page3=requests.get(url3)\n",
    "\n",
    "# Fetching contents of the webpage\n",
    "    soup2=BeautifulSoup(page2.content)\n",
    "    soup3=BeautifulSoup(page3.content)\n",
    "# Creating empty list to store data\n",
    "    \n",
    "    col1=[]\n",
    "    col2=[]\n",
    "    col3=[]\n",
    "    col4=[]\n",
    "    col6=[]\n",
    "    col7=[]\n",
    "    col8=[]\n",
    "    col9=[]\n",
    "        \n",
    "# Condition to fetch the Top Ten ODI Batting Players Men desired content and saving the result in the specified list  \n",
    "    td_tag=soup2.find_all('td')\n",
    "    string=str(td_tag)\n",
    "    \n",
    "    for i in soup2.find_all('span',class_=\"rankings-block__pos-number\"):\n",
    "        col1.append(i.text)\n",
    "    for i in soup2.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        col1.extend(i.contents)\n",
    "    \n",
    "    for i in soup2.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        col2.append(i.text)\n",
    "    for i in soup2.find_all('td', class_=\"table-body__cell rankings-table__name name\"):\n",
    "        col2.extend(i.contents)\n",
    "    \n",
    "    for i in soup2.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        col3.append(i.text)\n",
    "    for i in soup2.find_all('span', class_=\"table-body__logo-text\"):\n",
    "        col3.extend(i.contents)\n",
    "    \n",
    "    for i in soup2.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        col4.append(i.text)\n",
    "    for i in soup2.find_all('td', class_=\"table-body__cell rating\"):\n",
    "        col4.extend(i.contents)\n",
    "    \n",
    "        \n",
    "# Condition to fetch the Top Ten ODI Bowling Players Men desired content and saving the result in the specified list  \n",
    "    td_tag=soup3.find_all('')\n",
    "    string=str(td_tag)\n",
    "    \n",
    "    for i in soup3.find_all('span',class_=\"rankings-block__pos-number\"):\n",
    "        col6.append(i.text)\n",
    "    for i in soup3.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        col6.extend(i.contents)\n",
    "    \n",
    "    for i in soup3.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        col7.append(i.text)\n",
    "    for i in soup3.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "        col7.extend(i.contents)\n",
    "        \n",
    "    for i in soup3.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        col8.append(i.text)\n",
    "    for i in soup3.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        col8.extend(i.contents)\n",
    "    \n",
    "    for i in soup3.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        col9.append(i.text)\n",
    "    for i in soup3.find_all('td',class_=\"table-body__cell rating\"):\n",
    "        col9.extend(i.contents)\n",
    "\n",
    "# For batting data process \n",
    "\n",
    "    table=pd.DataFrame({\"Name\":col2})\n",
    "    table=table.drop_duplicates(subset=None)\n",
    "    table=table.drop(1,axis=0)\n",
    "    table=table.replace('\\n',regex=True)\n",
    "    new_col2=table.values.tolist()\n",
    "    \n",
    "# For bowling data process\n",
    "    table_b=pd.DataFrame({\"Name\":col7})\n",
    "    table_b=table_b.drop_duplicates(subset=None)\n",
    "    table_b=table_b.drop(1,axis=0)\n",
    "    table_b=table_b.replace('\\n',regex=True)\n",
    "    new_col7=table_b.values.tolist()\n",
    "    \n",
    "# Storing only top 10 records and maintaining the same number of arrays\n",
    "    col1=col1[0:10]\n",
    "    new_col2=new_col2[0:10]\n",
    "    col3=col3[0:10]\n",
    "    col4=col4[0:10]\n",
    "    \n",
    "    col6=col6[0:10]\n",
    "    new_col7=new_col7[0:10]\n",
    "    col8=col8[0:10]\n",
    "    col9=col9[0:10]\n",
    "\n",
    "\n",
    "# Creating Data Frames\n",
    "    table2= pd.DataFrame({\"Rank\":col1, \"Name\":new_col2,\"Team\":col3,\"Ratings\":col4 }) \n",
    "    table3= pd.DataFrame({\"Rank\":col6, \"Name\":new_col7,\"Team\":col8, \"Ratings\":col9 })\n",
    "\n",
    "# To remove \"\\n\" tags\n",
    "    table2=table2.replace('\\n','',regex=True)\n",
    "    table3=table3.replace('\\n','',regex=True)\n",
    "    \n",
    "# To drop index from being displayed\n",
    "    table2=table2.to_string(index=False, col_space=7,justify='center')\n",
    "    table3=table3.to_string(index=False, col_space=7,justify='center')\n",
    "    \n",
    "# To display result\n",
    "    print(\"Top 10 ICC-Womens ODI Batting Players are:\",\"\\n\",table2,\"\\n\")\n",
    "    print(\"Top 10 ICC-Womens ODI Bowling Players are:\",\"\\n\",table3,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35c2dc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ICC-Womens ODI Batting Players are: \n",
      "                                  Rank                                             Name                     Team          Ratings\n",
      "                                             1                                      [Babar Azam] PAK                       892  \n",
      "                                     2                                           [[Imam-ul-Haq]]                     PAK   815  \n",
      "                                     3                                           [[Virat Kohli]]                     IND   811  \n",
      "                                     4                                          [[Rohit Sharma]]                     IND   791  \n",
      "                                     5                                       [[Quinton de Kock]]                      SA   789  \n",
      "                                     6                                           [[Ross Taylor]]                      NZ   775  \n",
      "                                     7                                 [[Rassie van der Dussen]]                      SA   769  \n",
      "                                     8                                        [[Jonny Bairstow]]                     ENG   752  \n",
      "                                     9                                          [[David Warner]]                     AUS   737  \n",
      "                                    10                                             [[Shai Hope]]                      WI   718   \n",
      "\n",
      "Top 10 ICC-Womens ODI Bowling Players are: \n",
      "                                  Rank                                          Name                  Team          Ratings\n",
      "                                             1                                [Trent Boult] NZ                       726  \n",
      "                                     2                                       [[Matt Henry]]                     NZ   683  \n",
      "                                     3                                   [[Shaheen Afridi]]                    PAK   681  \n",
      "                                     4                                     [[Chris Woakes]]                    ENG   680  \n",
      "                                     5                                   [[Jasprit Bumrah]]                    IND   679  \n",
      "                                     6                                   [[Josh Hazlewood]]                    AUS   679  \n",
      "                                     7                                 [[Mujeeb Ur Rahman]]                    AFG   676  \n",
      "                                     8                                     [[Mehedi Hasan]]                    BAN   661  \n",
      "                                     9                                    [[Mohammad Nabi]]                    AFG   657  \n",
      "                                    10                                  [[Shakib Al Hasan]]                    BAN   657   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "icc_men(url2=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\",\n",
    "         url3=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ff8d5",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "\n",
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fd726",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f9fc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd61805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd8f1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_women_a(url1):                     #Defining function\n",
    "    # Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "# To establish connection with the required webpage\n",
    "    page1=requests.get(url1)\n",
    "    \n",
    "# Fetching contents of the webpage\n",
    "    soup1=BeautifulSoup(page1.content)\n",
    "\n",
    "# Creating empty list to store data\n",
    "    col_rank=[]\n",
    "    col_team=[]\n",
    "    col_matches=[]\n",
    "    col_points=[]\n",
    "    col_ratings=[]\n",
    "    col5=[]\n",
    "# Condition to fetch the Top Ten ODI Team desired content and saving the result in the specified list  \n",
    "    \n",
    "    for i in soup1.find_all('td',class_=\"rankings-block__banner--pos\"):\n",
    "        col_rank.append(i.text)\n",
    "    for i in soup1.find_all('td',class_=\"table-body__cell table-body__cell--position u-text-right\"):\n",
    "        col_rank.extend(i.contents)\n",
    "    \n",
    "    for i in soup1.find_all('span', class_=\"u-show-phablet\"):\n",
    "        col_team.append(i.text)\n",
    "    for i in soup1.find_all('span', class_=\"u-show-phablet\"):\n",
    "        col_team.extend(i.contents) \n",
    "    \n",
    "    for i in soup1.find_all('td', class_=\"rankings-block__banner--matches\"):\n",
    "        col_matches.append(i.text)\n",
    "   \n",
    "    for i in soup1.find_all('td', class_=\"rankings-block__banner--points\"):\n",
    "        col_points.append(i.text)\n",
    "    \n",
    "    for i in soup1.find_all('td', class_=\"table-body__cell u-center-text\"):\n",
    "        col5.append(i.text)\n",
    "\n",
    "    for i in soup1.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        col_ratings.append(i.text)\n",
    "    for i in soup1.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "        col_ratings.extend(i.contents)  \n",
    "# storing data as they were extracted from the same classes\n",
    "    pts=col5[1:19:2]\n",
    "    mat=col5[0:19:2]\n",
    "    \n",
    "\n",
    "# Concatenating lists\n",
    "    col_matches= col_matches + mat\n",
    "    col_points= col_points + pts\n",
    "    \n",
    "# Storing only top 10 records and maintaining the same number of arrays\n",
    "    col_rank=col_rank[0:10]\n",
    "    col_team=col_team[0:10]\n",
    "    col_matches=col_matches[0:10]\n",
    "    col_ratings=col_ratings[0:10]\n",
    "    col_points=col_points[0:10]\n",
    "\n",
    "# Creating Data Frames\n",
    "    table1= pd.DataFrame({\"Rank\":col_rank, \"Team\":col_team,\"Matches\":col_matches,\"Points\":col_points,\"Ratings\":col_ratings })\n",
    "    \n",
    "# To remove \"\\n\" tags\n",
    "    table1=table1.replace('\\n','',regex=True)\n",
    "    \n",
    "# To drop index from being displayed\n",
    "    table1=table1.to_string(index=False, col_space=15,justify='center')\n",
    "    \n",
    "    \n",
    "# To display result\n",
    "    print(\"Top 10 ICC-Womens ODI Team are:\",\"\\n\",table1,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "757abc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ICC-Womens ODI Team are: \n",
      "       Rank            Team          Matches          Points                               Ratings                          \n",
      "        1             AUS              29            4,837                                  167                            \n",
      "        2              SA              32            3,949                                                              123\n",
      "        3             ENG              30            3,531                                                              118\n",
      "        4             IND              29            2,889                                                              100\n",
      "        5              NZ              31            3,019                                                               97\n",
      "        6              WI              30            2,768                                                               92\n",
      "        7             BAN              12              930                                                               78\n",
      "        8             PAK              30            1,962                                                               65\n",
      "        9              SL               8              384                                                               48\n",
      "       10             IRE               8              351                                                               44 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "icc_women_a(url1=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba67a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_women(url2,url3):                     #Defining function\n",
    "# Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# To establish connection with the required webpage\n",
    "    page2=requests.get(url2)\n",
    "    page3=requests.get(url3)\n",
    "\n",
    "# Fetching contents of the webpage\n",
    "    soup2=BeautifulSoup(page2.content)\n",
    "    soup3=BeautifulSoup(page3.content)\n",
    "# Creating empty list to store data\n",
    "    \n",
    "    col1=[]\n",
    "    col2=[]\n",
    "    col3=[]\n",
    "    col4=[]\n",
    "    col6=[]\n",
    "    col7=[]\n",
    "    col8=[]\n",
    "    col9=[]\n",
    "        \n",
    "# Condition to fetch the Top Ten ODI Batting Players Women desired content and saving the result in the specified list  \n",
    "    td_tag=soup2.find_all('td')\n",
    "    string=str(td_tag)\n",
    "    \n",
    "    for i in soup2.find_all('span',class_=\"rankings-block__pos-number\"):\n",
    "        col1.append(i.text)\n",
    "    for i in soup2.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        col1.extend(i.contents)\n",
    "    \n",
    "    for i in soup2.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        col2.append(i.text)\n",
    "    for i in soup2.find_all('td', class_=\"table-body__cell rankings-table__name name\"):\n",
    "        col2.extend(i.contents)\n",
    "    \n",
    "    for i in soup2.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        col3.append(i.text)\n",
    "    for i in soup2.find_all('span', class_=\"table-body__logo-text\"):\n",
    "        col3.extend(i.contents)\n",
    "    \n",
    "    for i in soup2.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        col4.append(i.text)\n",
    "    for i in soup2.find_all('td', class_=\"table-body__cell rating\"):\n",
    "        col4.extend(i.contents)\n",
    "    \n",
    "        \n",
    "# Condition to fetch the Top Ten ODI Bowling Players Women desired content and saving the result in the specified list  \n",
    "    td_tag=soup3.find_all('td')\n",
    "    string=str(td_tag)\n",
    "    \n",
    "    for i in soup3.find_all('span',class_=\"rankings-block__pos-number\"):\n",
    "        col6.append(i.text)\n",
    "    for i in soup3.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        col6.extend(i.contents)\n",
    "    \n",
    "    for i in soup3.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        col7.append(i.text)\n",
    "    for i in soup3.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "        col7.extend(i.contents)\n",
    "        \n",
    "    for i in soup3.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        col8.append(i.text)\n",
    "    for i in soup3.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        col8.extend(i.contents)\n",
    "    \n",
    "    for i in soup3.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        col9.append(i.text)\n",
    "    for i in soup3.find_all('td',class_=\"table-body__cell rating\"):\n",
    "        col9.extend(i.contents)\n",
    "\n",
    "# For batting data process \n",
    "\n",
    "    table=pd.DataFrame({\"Name\":col2})\n",
    "    table=table.drop_duplicates(subset=None)\n",
    "    table=table.drop(1,axis=0)\n",
    "    table=table.replace('\\n',regex=True)\n",
    "    new_col2=table.values.tolist()\n",
    "    \n",
    "# For bowling data process\n",
    "    table_b=pd.DataFrame({\"Name\":col7})\n",
    "    table_b=table_b.drop_duplicates(subset=None)\n",
    "    table_b=table_b.drop(1,axis=0)\n",
    "    table_b=table_b.replace('\\n',regex=True)\n",
    "    new_col7=table_b.values.tolist()\n",
    "    \n",
    "# Storing only top 10 records and maintaining the same number of arrays\n",
    "    col1=col1[0:10]\n",
    "    new_col2=new_col2[0:10]\n",
    "    col3=col3[0:10]\n",
    "    col4=col4[0:10]\n",
    "    \n",
    "    col6=col6[0:10]\n",
    "    new_col7=new_col7[0:10]\n",
    "    col8=col8[0:10]\n",
    "    col9=col9[0:10]\n",
    "\n",
    "\n",
    "# Creating Data Frames\n",
    "    table2= pd.DataFrame({\"Rank\":col1, \"Name\":new_col2,\"Team\":col3,\"Ratings\":col4 }) \n",
    "    table3= pd.DataFrame({\"Rank\":col6, \"Name\":new_col7,\"Team\":col8, \"Ratings\":col9 })\n",
    "\n",
    "# To remove \"\\n\" tags\n",
    "    table2=table2.replace('\\n','',regex=True)\n",
    "    table3=table3.replace('\\n','',regex=True)\n",
    "    \n",
    "# To drop index from being displayed\n",
    "    table2=table2.to_string(index=False, col_space=7,justify='center')\n",
    "    table3=table3.to_string(index=False, col_space=7,justify='center')\n",
    "    \n",
    "# To display result\n",
    "    print(\"Top 10 ICC-Womens ODI Batting Players are:\",\"\\n\",table2,\"\\n\")\n",
    "    print(\"Top 10 ICC-Womens ODI Bowling Players are:\",\"\\n\",table3,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d67756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ICC-Womens ODI Batting Players are: \n",
      "                                  Rank                                           Name                   Team          Ratings\n",
      "                                             1                                [Alyssa Healy] AUS                       785  \n",
      "                                     2                                    [[Natalie Sciver]]                     ENG   750  \n",
      "                                     3                                       [[Beth Mooney]]                     AUS   748  \n",
      "                                     4                                   [[Laura Wolvaardt]]                      SA   713  \n",
      "                                     5                                       [[Meg Lanning]]                     AUS   710  \n",
      "                                     6                                    [[Rachael Haynes]]                     AUS   701  \n",
      "                                     7                                 [[Amy Satterthwaite]]                      NZ   681  \n",
      "                                     8                                   [[Smriti Mandhana]]                     IND   669  \n",
      "                                     9                                    [[Tammy Beaumont]]                     ENG   659  \n",
      "                                    10                                      [[Ellyse Perry]]                     AUS   642   \n",
      "\n",
      "Top 10 ICC-Womens ODI Bowling Players are: \n",
      "                                  Rank                                          Name                  Team          Ratings\n",
      "                                             1                         [Sophie Ecclestone] ENG                       771  \n",
      "                                     2                                  [[Shabnim Ismail]]                      SA   755  \n",
      "                                     3                                   [[Jess Jonassen]]                     AUS   725  \n",
      "                                     4                                    [[Megan Schutt]]                     AUS   722  \n",
      "                                     5                                  [[Ayabonga Khaka]]                      SA   668  \n",
      "                                     6                                  [[Jhulan Goswami]]                     IND   663  \n",
      "                                     7                                  [[Marizanne Kapp]]                      SA   642  \n",
      "                                     8                                  [[Anya Shrubsole]]                     ENG   629  \n",
      "                                     9                                      [[Kate Cross]]                     ENG   617  \n",
      "                                    10                                 [[Hayley Matthews]]                      WI   612   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "icc_women(url2=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\",\n",
    "         url3=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b135a44",
   "metadata": {},
   "source": [
    "Question 7:\n",
    "\n",
    "7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8ea0d",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406ecf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n",
    "    page2=requests.get(url2)                  # To connect with the desired webpage\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f035a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p>#\n",
      "# robots.txt\n",
      "#\n",
      "# This file is to prevent the crawling and indexing of certain parts\n",
      "# of your site by web crawlers and spiders run by sites like Yahoo!\n",
      "# and Google. By telling these \"robots\" where not to go on your site,\n",
      "# you save bandwidth and server resources.\n",
      "\n",
      "Sitemap: https://www.cnbc.com/sitemapAll.xml\n",
      "Sitemap: https://www.cnbc.com/sitemap_news.xml\n",
      "Sitemap: https://www.cnbc.com/sitemapvideoAll.xml\n",
      "Sitemap: https://www.cnbc.com/SitemapQuotes.xml\n",
      "Sitemap: https://www.cnbc.com/sitemapSelectAll.xml\n",
      "\n",
      "User-agent: googlebot\n",
      "Disallow: /*native-android-mobile\n",
      "Disallow: /*native-android-tablet\n",
      "Disallow: /*mobile-native\n",
      "Disallow: /preview/\n",
      "Disallow: /undefined/\n",
      "Disallow: /proplayer\n",
      "Disallow: /appchart/*\n",
      "Disallow: /search/\n",
      "\n",
      "User-agent: *\n",
      "Disallow: /preview/\n",
      "Disallow: /undefined/\n",
      "Disallow: /proplayer\n",
      "Disallow: /appchart/*\n",
      "Disallow: /search/\n",
      "</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://www.cnbc.com/world/?region=world\",\n",
    "     url2=\"https://www.cnbc.com/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df36478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnbc(url):                                             # Defining function to get headings from wikipedia's main page\n",
    "    page= requests.get(url)                                     # for getting respnse and hitting wikipedia server\n",
    "    parse= BeautifulSoup(page.content)                       # to get contents of the webpage\n",
    "    col1=[]                                                 # Creating an empty list to store data\n",
    "    col2=[]\n",
    "    col3=[]\n",
    "    \n",
    "    \n",
    "    for i in parse.find_all('a', class_=\"LatestNews-headline\"):  # condition to get headings\n",
    "        col1.append(i.text)                                # storing headers in the empty list we had created\n",
    "    for i in parse.find_all('time', class_=\"LatestNews-timestamp\"):  # condition to get headings\n",
    "        col2.append(i.text)                                # storing headers in the empty list we had created\n",
    "    for i in parse.find_all('a', class_=\"LatestNews-headline\"):       # condition to get headings\n",
    "        col3.append(i['href'])                                # storing headers in the empty list we had created\n",
    "                                 # storing headers in the empty list we had created\n",
    "        \n",
    "    table= pd.DataFrame({\"Headline\":col1, \"Time\":col2,\"Paper URL\":col3})         # Creating a Data Frame or table\n",
    "    \n",
    "# To display result\n",
    "    print(table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f68dae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline          Time  \\\n",
      "0   'Minions: The Rise of Gru' tops $108 million a...   2 Hours Ago   \n",
      "1   Cheney: It's possible the Jan. 6 committee ref...   4 Hours Ago   \n",
      "2   How much money it takes to be considered wealt...   6 Hours Ago   \n",
      "3   Economy passengers could soon lie down on plan...   6 Hours Ago   \n",
      "4   Oil show no signs of easing as China starts to...   7 Hours Ago   \n",
      "5   Student loan payments could restart in Septemb...   7 Hours Ago   \n",
      "6   Analysts say these stocks are a must-own in th...   7 Hours Ago   \n",
      "7   American Airlines scheduling glitch allows pil...  July 2, 2022   \n",
      "8   Op-ed: In Putin's evil vs. good war against Uk...  July 2, 2022   \n",
      "9   Tesla delivered 254,695 electric vehicles in t...  July 2, 2022   \n",
      "10  Grilling boomed amid the pandemic. The grillin...  July 2, 2022   \n",
      "11  These Americans all left the U.S. for Mexico: ...  July 2, 2022   \n",
      "12  Former Intel exec Diane Bryant on her biggest ...  July 2, 2022   \n",
      "13  The U.S. economy is entering the back half of ...  July 2, 2022   \n",
      "14  Investors are counting on the third quarter to...  July 2, 2022   \n",
      "15  Some experts see a recession coming. How to pr...  July 2, 2022   \n",
      "16  3 easy ways to save money on gas this summer a...  July 2, 2022   \n",
      "17  Omicron's newest variant might threaten your J...  July 2, 2022   \n",
      "18  Bitcoin Family say they lost $1 million on the...  July 2, 2022   \n",
      "19  Google says it will delete location history fo...  July 1, 2022   \n",
      "20  Biden opens the possibility of more offshore o...  July 1, 2022   \n",
      "21  The last remnant of Facebook's crypto project ...  July 1, 2022   \n",
      "22  Best trades on CNBC Friday: Where to buy these...  July 1, 2022   \n",
      "23     What to watch in the markets in the week ahead  July 1, 2022   \n",
      "24  Trump media company subpoenaed in federal crim...  July 1, 2022   \n",
      "25  What Micron's earnings beat but light guidance...  July 1, 2022   \n",
      "26  Investing Club: The week in review, the week a...  July 1, 2022   \n",
      "27  FTX signs a deal giving it the option to buy c...  July 1, 2022   \n",
      "28  Walmart working on response to Supreme Court a...  July 1, 2022   \n",
      "29  Crypto broker Voyager Digital suspends trading...  July 1, 2022   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.cnbc.com/2022/07/03/minions-rise-o...  \n",
      "1   https://www.cnbc.com/2022/07/03/cheney-the-jan...  \n",
      "2   https://www.cnbc.com/2022/07/03/what-it-takes-...  \n",
      "3   https://www.cnbc.com/2022/07/03/air-new-zealan...  \n",
      "4   https://www.cnbc.com/2022/07/03/quarterly-inve...  \n",
      "5   https://www.cnbc.com/2022/07/03/student-loan-p...  \n",
      "6   https://www.cnbc.com/2022/07/03/quarterly-inve...  \n",
      "7   https://www.cnbc.com/2022/07/02/american-airli...  \n",
      "8   https://www.cnbc.com/2022/07/02/op-ed-in-putin...  \n",
      "9   https://www.cnbc.com/2022/07/02/tesla-tsla-q2-...  \n",
      "10  https://www.cnbc.com/2022/07/02/outdoor-cookin...  \n",
      "11  https://www.cnbc.com/2022/07/02/these-american...  \n",
      "12  https://www.cnbc.com/2022/07/02/former-intel-e...  \n",
      "13  https://www.cnbc.com/2022/07/02/quarterly-inve...  \n",
      "14  https://www.cnbc.com/2022/07/02/quarterly-inve...  \n",
      "15  https://www.cnbc.com/2022/07/02/some-experts-s...  \n",
      "16  https://www.cnbc.com/2022/07/02/3-ways-to-save...  \n",
      "17  https://www.cnbc.com/2022/07/02/omicrons-bapoi...  \n",
      "18  https://www.cnbc.com/2022/07/02/bitcoin-family...  \n",
      "19  https://www.cnbc.com/2022/07/01/google-will-de...  \n",
      "20  https://www.cnbc.com/2022/07/01/biden-interior...  \n",
      "21  https://www.cnbc.com/2022/07/01/the-last-remna...  \n",
      "22  https://www.cnbc.com/2022/07/01/best-trades-on...  \n",
      "23  https://www.cnbc.com/2022/07/01/recession-fear...  \n",
      "24  https://www.cnbc.com/2022/07/01/trump-media-co...  \n",
      "25  https://www.cnbc.com/2022/07/01/investing-club...  \n",
      "26  https://www.cnbc.com/2022/07/01/investing-club...  \n",
      "27  https://www.cnbc.com/2022/07/01/ftx-signs-a-de...  \n",
      "28  https://www.cnbc.com/2022/07/01/walmart-workin...  \n",
      "29  https://www.cnbc.com/2022/07/01/voyager-digita...  \n"
     ]
    }
   ],
   "source": [
    "cnbc(url=\"https://www.cnbc.com/world/?region=world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a579a5",
   "metadata": {},
   "source": [
    "Question 8:\n",
    "\n",
    "\n",
    "Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27613a2b",
   "metadata": {},
   "source": [
    "A: To start with web scraping we must first check with the legality and server permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bd87aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n",
    "    page2=requests.get(url2)                  # To connect with the desired webpage\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4762766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p># Robots.txt file for https://www.elsevier.com\n",
      "# Do Not Delete This File\n",
      "\n",
      "User-agent: FAST Enterprise Crawler 6 / Scirus\n",
      "Disallow:\n",
      "\n",
      "User-agent: innosense/Nutch-1.0\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Sogou web spider/4.0\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Xenu Link Sleuth/1.3.8\n",
      "Disallow: /\n",
      "\n",
      "User-agent: discoverybot/2.0\n",
      "Disallow: /\n",
      "\n",
      "User-agent: YoudaoBot/1.0\n",
      "Disallow: /\n",
      "\n",
      "User-agent: Sogou web spider/3.0\n",
      "Disallow: /\n",
      "\n",
      "User-agent: *\n",
      "Disallow: /connect/archive\n",
      "Disallow: /about/press-releases/archive\n",
      "Disallow: /_dynamic-products/\n",
      "Disallow: /administration/\n",
      "Disallow: /_squiz-test/\n",
      "Disallow: /test-folder/\n",
      "Disallow: /ajax-content/\n",
      "Disallow: /fb-search/\n",
      "Disallow: /_Test folder/\n",
      "Disallow: /infermed-community/\n",
      "Disallow: /PII/\n",
      "Disallow: /gej-ng/\n",
      "Disallow: /cgi-bin/\n",
      "Disallow: /__returned/\n",
      "Disallow: /ctx/\n",
      "Disallow: /s/\n",
      "Disallow: /inca\n",
      "Disallow: /wps/find/\n",
      "Disallow: /wps/inca/\n",
      "Disallow: /wps/subject/\n",
      "Disallow: /wps/product/cws_home/\n",
      "Disallow: /wps/locate\n",
      "Disallow: /wps/product\n",
      "Disallow: /pub/\n",
      "Disallow: /IVP/\n",
      "Disallow: /febs/\n",
      "Disallow: /framework_librarians/\n",
      "Disallow: /framework_aboutus/\n",
      "Disallow: /elsevier-products/\n",
      "Disallow: */homepage/\n",
      "Disallow: */_dynamic/\n",
      "Disallow: */books-and-journals/book-companion/\n",
      "Disallow: */rd-solutions/industry-insights/pharma-and-life-sciences/ty\n",
      "Disallow: */about/our-business?a=568780\n",
      "Disallow: */clinical-solutions/contact/business-development-contact-us/drug-information-busdevform*\n",
      "Disallow: */search-results?*\n",
      "Disallow: /locate\n",
      "Disallow: /%20locate\n",
      "Disallow: /cas\n",
      "Disallow: /online-tools\n",
      "Disallow: /framework_products\n",
      "Disallow: /trends\n",
      "Disallow: /cdweb\n",
      "Disallow: /authored_subject_sections\n",
      "Disallow: /conferences\n",
      "Disallow: /jp\n",
      "Disallow: /jump\n",
      "Disallow: /catalogue\n",
      "Disallow: /pathway-studio\n",
      "Disallow: /framework_authors\n",
      "Disallow: /action\n",
      "Disallow: /catalog?*&amp;cat*\n",
      "Disallow: /zh-cn/test2\n",
      "Disallow: /promo/\n",
      "\n",
      "User-Agent: *\n",
      "Sitemap: https://www.elsevier.com/sitemaps/sitemap_index.xml\n",
      "</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\",\n",
    "     url2=\"https://www.elsevier.com/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1baf3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal(url):                                             # Defining function to get headings from wikipedia's main page\n",
    "    page= requests.get(url)                                     # for getting respnse and hitting wikipedia server\n",
    "    parse= BeautifulSoup(page.content)                       # to get contents of the webpage\n",
    "    col1=[]                                                 # Creating an empty list to store data\n",
    "    col2=[]\n",
    "    col3=[]\n",
    "    col4=[]\n",
    "    \n",
    "    for i in parse.find_all('h2', class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):  # condition to get headings\n",
    "        col1.append(i.text)                                # storing headers in the empty list we had created\n",
    "    for i in parse.find_all('span', class_=\"sc-1w3fpd7-0 pgLAT\"):  # condition to get headings\n",
    "        col2.append(i.text)                                # storing headers in the empty list we had created\n",
    "    for i in parse.find_all('span', class_=\"sc-1thf9ly-2 bKddwo\"):  # condition to get headings\n",
    "        col3.append(i.text)                                # storing headers in the empty list we had created\n",
    "    for i in parse.find_all('a', class_=\"sc-5smygv-0 nrDZj\"):  # condition to get headings\n",
    "        col4.append(i['href'])                                # storing headers in the empty list we had created\n",
    "        \n",
    "    table= pd.DataFrame({\"Paper Title\":col1,\"Authors\":col2, \"Published Date\":col3,\"Paper URL\":col4})         # Creating a Data Frame or table\n",
    "    \n",
    "# To display result\n",
    "    print(table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dac528d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1                           Making sense of raw input   \n",
      "2   Law and logic: A review from an argumentation ...   \n",
      "3              Creativity and artificial intelligence   \n",
      "4   Artificial cognition for social human–robot in...   \n",
      "5   Explanation in artificial intelligence: Insigh...   \n",
      "6                       Making sense of sensory input   \n",
      "7   Conflict-based search for optimal multi-agent ...   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...   \n",
      "9   The Hanabi challenge: A new frontier for AI re...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11           Argumentation in artificial intelligence   \n",
      "12  Algorithms for computing strategies in two-pla...   \n",
      "13      Multiple object tracking: A literature review   \n",
      "14  Selection of relevant features and examples in...   \n",
      "15  A survey of inverse reinforcement learning: Ch...   \n",
      "16  Explaining individual predictions when feature...   \n",
      "17  A review of possible effects of cognitive bias...   \n",
      "18  Integrating social power into the decision-mak...   \n",
      "19  “That's (not) the output I expected!” On the r...   \n",
      "20  Explaining black-box classifiers using post-ho...   \n",
      "21  Algorithm runtime prediction: Methods & evalua...   \n",
      "22              Wrappers for feature subset selection   \n",
      "23  Commonsense visual sensemaking for autonomous ...   \n",
      "24         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
      "3                                 Boden, Margaret A.      August 1998   \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "5                                        Miller, Tim    February 2019   \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
      "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
      "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
      "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "22                      Kohavi, Ron, John, George H.    December 1997   \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "24                                   Ying, Mingsheng    February 2010   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "journal(url=\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ac3e6",
   "metadata": {},
   "source": [
    "Question 9:\n",
    "\n",
    "Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a60da7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n",
    "    page2=requests.get(url2)                  # To connect with the desired webpage\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "831a7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p>User-agent: *\n",
      "Disallow: /*/*/book\n",
      "Disallow: /*/deli-cuisine\n",
      "Disallow: /*/pastries-cuisine\n",
      "Disallow: /*/chettinad-cuisine\n",
      "Disallow: /*/drinks-cuisine\n",
      "Disallow: /*/juice-cuisine\n",
      "Disallow: /*/street-food-cuisine\n",
      "Disallow: /*/tea-cuisine\n",
      "Disallow: /*/world-cuisine\n",
      "Disallow: /*/new-world-wine\n",
      "Disallow: /*/*/deli-cuisine\n",
      "Disallow: /*/*/pastries-cuisine\n",
      "Disallow: /*/*/chettinad-cuisine\n",
      "Disallow: /*/*/drinks-cuisine\n",
      "Disallow: /*/*/juice-cuisine\n",
      "Disallow: /*/*/street-food-cuisine\n",
      "Disallow: /*/*/tea-cuisine\n",
      "Disallow: /*/*/world-cuisine\n",
      "Disallow: /*/*/new-world-wine\n",
      "Disallow: /*/*/*/deli-cuisine\n",
      "Disallow: /*/*/*/pastries-cuisine\n",
      "Disallow: /*/*/*/chettinad-cuisine\n",
      "Disallow: /*/*/*/drinks-cuisine\n",
      "Disallow: /*/*/*/juice-cuisine\n",
      "Disallow: /*/*/*/street-food-cuisine\n",
      "Disallow: /*/*/*/tea-cuisine\n",
      "Disallow: /*/*/*/world-cuisine\n",
      "Disallow: /*/*/*/new-world-wine\n",
      "Disallow: /xhrajaxrequest\n",
      "Disallow: /xhr-request\n",
      "Disallow: /*/*/info\n",
      "Disallow: /*/*/*/book\n",
      "Disallow: /support-restaurants/\n",
      "Disallow: /sc/\n",
      "Disallow: /carlson/\n",
      "Disallow: /ahmedabad/shara-barbque-hansol-north-ahmedabad-21637\n",
      "\n",
      "\n",
      "Allow: /*?v=\n",
      "Allow: /assets/\n",
      "</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://www.dineout.co.in/mumbai-restaurants?search_str=buffet%20special\",\n",
    "     url2=\"https://www.dineout.co.in/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "145dc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dineout(url):                     #Defining function\n",
    "    # Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "# To establish connection with the required webpage\n",
    "    page=requests.get(url)\n",
    "    \n",
    "# Fetching contents of the webpage\n",
    "    soup=BeautifulSoup(page.content)\n",
    "\n",
    "# Creating empty list to store data\n",
    "    name=[]\n",
    "    cuisine=[]\n",
    "    location=[]\n",
    "    ratings=[]\n",
    "    img=[]\n",
    "    col5=[]\n",
    "# Condition to fetch the Top Ten ODI Team desired content and saving the result in the specified list  \n",
    "    \n",
    "    for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "        name.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "        location.append(i.text)\n",
    "     \n",
    "    \n",
    "    for i in soup.find_all('span', class_=\"double-line-ellipsis\"):\n",
    "        cuisine.append(i.text)\n",
    "   \n",
    "    for i in soup.find_all('div', class_=\"restnt-rating rating-4\"):\n",
    "        ratings.append(i.text)\n",
    "    \n",
    "    for i in soup.find_all('img', class_=\"no-img\"):\n",
    "        img.append(i['data-src'])\n",
    "\n",
    "\n",
    "        \n",
    "# Storing only top 10 records and maintaining the same number of arrays\n",
    "    name=name[0:10]\n",
    "    location=location[0:10]\n",
    "    cuisine=cuisine[0:10]\n",
    "    ratings=ratings[0:10]\n",
    "    img=img[0:10]\n",
    "    \n",
    "\n",
    "# Creating Data Frames\n",
    "    table1= pd.DataFrame({\"Restaurant Name\":name, \"Location\":location,\"Cuisine\":cuisine,\"Ratings\":ratings,\"Image\":img })\n",
    "    \n",
    "# To remove \"\\n\" tags\n",
    "    table1=table1.replace('\\n','',regex=True)\n",
    "\n",
    "    \n",
    "# To display result\n",
    "    print(\"Dineout:\",\"\\n\",table1,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d57d91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dineout: \n",
      "                   Restaurant Name  \\\n",
      "0             Sigree Global Grill   \n",
      "1                  Mainland China   \n",
      "2        ABs - Absolute Barbecues   \n",
      "3                 Barbeque Nation   \n",
      "4                       Hoppipola   \n",
      "5                  Lake View Cafe   \n",
      "6         Mini Punjab's Lake Side   \n",
      "7                         JW Cafe   \n",
      "8                         Saptami   \n",
      "9  Peninsula - The Spice of India   \n",
      "\n",
      "                                            Location  \\\n",
      "0                      Ventura Building,Powai, Powai   \n",
      "1                                 Hiranandani, Powai   \n",
      "2                                  Chandivali, Powai   \n",
      "3  Phoenix Marketcity Mall,Kurla West, Central Su...   \n",
      "4              Galleria Shopping Centre,Powai, Powai   \n",
      "5          The Westin Mumbai Powai Lake,Powai, Powai   \n",
      "6                                       Powai, Powai   \n",
      "7  JW Marriott Mumbai Sahar,Near Andheri East Sta...   \n",
      "8  Holiday Inn Mumbai International Airport,Near ...   \n",
      "9   The Peninsula Grand Hotel,Sakinaka, Andheri East   \n",
      "\n",
      "                                             Cuisine Ratings  \\\n",
      "0     ₹ 1,500 for 2 (approx) | North Indian, Biryani     4.3   \n",
      "1      ₹ 1,200 for 2 (approx) | Chinese, Asian, Thai     4.3   \n",
      "2              ₹ 1,400 for 2 (approx) | North Indian     4.3   \n",
      "3     ₹ 2,000 for 2 (approx) | North Indian, Chinese     4.3   \n",
      "4  ₹ 1,400 for 2 (approx) | Continental, Finger F...     4.2   \n",
      "5  ₹ 3,000 for 2 (approx) | North Indian, Chinese...       4   \n",
      "6  ₹ 1,700 for 2 (approx) | Mughlai, North Indian...     4.3   \n",
      "7  ₹ 4,000 for 2 (approx) | North Indian, Asian, ...     3.9   \n",
      "8  ₹ 2,000 for 2 (approx) | North Indian, Contine...     4.4   \n",
      "9  ₹ 1,700 for 2 (approx) | North Indian, Contine...       4   \n",
      "\n",
      "                                               Image  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9  https://im1.dineout.co.in/images/uploads/resta...   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dineout(url=\"https://www.dineout.co.in/mumbai-restaurants?search_str=buffet%20special\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affa5f1",
   "metadata": {},
   "source": [
    "Question 10:\n",
    "\n",
    "Write a python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank\n",
    "ii) Publication\n",
    "iii) h5-index\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cf77f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url,url2):                          # Defining function to check whether connection can be established and permissions\n",
    "    page= requests.get(url)              # Checking for the server to be accessible of the website to be scrapped\n",
    "    print(page,\"\\n\")                     # Display result\n",
    "    \n",
    "    if page.status_code==200:                        # Condition for accessibility\n",
    "        print (\"Server is accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "    else:\n",
    "        print(\"Server is not accessible for Web Scrapping\",\"\\n\",\"\\n\")\n",
    "        \n",
    "    page2=requests.get(url2)                  # To connect with the desired webpage\n",
    "    permission=BeautifulSoup(page2.content)   # To parse the permissions\n",
    "    print (permission)                        # Display permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24a21c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "Server is accessible for Web Scrapping \n",
      " \n",
      "\n",
      "<html><body><p>User-agent: *\n",
      "Disallow: /search\n",
      "Disallow: /index.html\n",
      "Disallow: /scholar\n",
      "Disallow: /citations?\n",
      "Allow: /citations?user=\n",
      "Disallow: /citations?*cstart=\n",
      "Disallow: /citations?user=*%40\n",
      "Disallow: /citations?user=*@\n",
      "Allow: /citations?view_op=list_classic_articles\n",
      "Allow: /citations?view_op=metrics_intro\n",
      "Allow: /citations?view_op=new_profile\n",
      "Allow: /citations?view_op=sitemap\n",
      "Allow: /citations?view_op=top_venues\n",
      "\n",
      "User-agent: Twitterbot\n",
      "Disallow:\n",
      "\n",
      "User-agent: facebookexternalhit\n",
      "Disallow:\n",
      "\n",
      "User-agent: PetalBot\n",
      "Disallow: /\n",
      "</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Recalling the function\n",
    "check(url=\"https://scholar.google.com/citations?view_op=top_venues&hl=en\",\n",
    "     url2=\"https://scholar.google.ca/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25d8635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scholar(url):                     #Defining function\n",
    "    # Setting rows and columns for max result display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "# To establish connection with the required webpage\n",
    "    page=requests.get(url)\n",
    "    \n",
    "# Fetching contents of the webpage\n",
    "    soup=BeautifulSoup(page.content)\n",
    "\n",
    "# Creating empty list to store data\n",
    "    rank=[]\n",
    "    publication=[]\n",
    "    index=[]\n",
    "    median=[]\n",
    "    \n",
    "# Condition to fetch the Top Ten ODI Team desired content and saving the result in the specified list  \n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"gsc_mvt_p\"):\n",
    "        rank.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('td', class_=\"gsc_mvt_t\"):\n",
    "        publication.append(i.text)\n",
    "\n",
    "    \n",
    "    for i in soup.find_all('a', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        index.append(i.text)\n",
    "   \n",
    "    for i in soup.find_all('span', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        median.append(i.text)\n",
    "\n",
    "\n",
    "# Creating Data Frames\n",
    "    table1= pd.DataFrame({\"Rank\":rank, \"Publication\":publication,\"h5 Index\":index,\"h5 Median\":median })\n",
    "    \n",
    "# To remove \"\\n\" tags\n",
    "    table1=table1.replace('\\n','',regex=True)\n",
    "\n",
    "    \n",
    "# To display result\n",
    "    print(\"Google Scholar:\",\"\\n\",table1,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae99deaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Scholar: \n",
      "     Rank                                        Publication h5 Index h5 Median\n",
      "0     1.                                             Nature      444       667\n",
      "1     2.                The New England Journal of Medicine      432       780\n",
      "2     3.                                            Science      401       614\n",
      "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
      "4     5.                                         The Lancet      354       635\n",
      "5     6.                                 Advanced Materials      312       418\n",
      "6     7.                              Nature Communications      307       428\n",
      "7     8.                                               Cell      300       505\n",
      "8     9.  International Conference on Learning Represent...      286       533\n",
      "9    10.              Neural Information Processing Systems      278       436\n",
      "10   11.                                               JAMA      267       425\n",
      "11   12.                                   Chemical Reviews      265       444\n",
      "12   13.    Proceedings of the National Academy of Sciences      256       364\n",
      "13   14.                                  Angewandte Chemie      245       332\n",
      "14   15.                           Chemical Society Reviews      244       386\n",
      "15   16.           Journal of the American Chemical Society      242       344\n",
      "16   17.  IEEE/CVF International Conference on Computer ...      239       415\n",
      "17   18.                             Nucleic Acids Research      238       550\n",
      "18   19.       International Conference on Machine Learning      237       421\n",
      "19   20.                                    Nature Medicine      235       389\n",
      "20   21.           Renewable and Sustainable Energy Reviews      227       324\n",
      "21   22.                   Science of The Total Environment      225       311\n",
      "22   23.                          Advanced Energy Materials      220       300\n",
      "23   24.                       Journal of Clinical Oncology      213       315\n",
      "24   25.                                           ACS Nano      211       277\n",
      "25   26.                      Journal of Cleaner Production      211       273\n",
      "26   27.                      Advanced Functional Materials      210       280\n",
      "27   28.                            Physical Review Letters      207       294\n",
      "28   29.                                 Scientific Reports      206       274\n",
      "29   30.                                The Lancet Oncology      202       329\n",
      "30   31.                     Energy & Environmental Science      202       290\n",
      "31   32.                                        IEEE Access      200       303\n",
      "32   33.                                           PLoS ONE      198       278\n",
      "33   34.                                   Science Advances      197       294\n",
      "34   35.      Journal of the American College of Cardiology      195       276\n",
      "35   36.                 Applied Catalysis B: Environmental      192       246\n",
      "36   37.                                    Nature Genetics      191       297\n",
      "37   38.                                                BMJ      190       307\n",
      "38   39.                                        Circulation      189       301\n",
      "39   40.             European Conference on Computer Vision      186       321\n",
      "40   41.        International Journal of Molecular Sciences      183       253\n",
      "41   42.                                   Nature Materials      181       265\n",
      "42   43.                       Chemical engineering journal      181       224\n",
      "43   44.         AAAI Conference on Artificial Intelligence      180       296\n",
      "44   45.                   Journal of Materials Chemistry A      178       220\n",
      "45   46.                 ACS Applied Materials & Interfaces      177       223\n",
      "46   47.                               Nature Biotechnology      175       315\n",
      "47   48.                     The Lancet Infectious Diseases      173       296\n",
      "48   49.                            Frontiers in Immunology      173       228\n",
      "49   50.                                     Applied Energy      173       217\n",
      "50   51.                                        Nano Energy      172       232\n",
      "51   52.                                      Nature Energy      170       314\n",
      "52   53.  Meeting of the Association for Computational L...      169       304\n",
      "53   54.                          The Astrophysical Journal      167       234\n",
      "54   55.                                   Gastroenterology      166       254\n",
      "55   56.                                     Nature Methods      165       296\n",
      "56   57.  IEEE Transactions on Pattern Analysis and Mach...      165       293\n",
      "57   58.            Cochrane Database of Systematic Reviews      165       243\n",
      "58   59.                                              Blood      165       229\n",
      "59   60.                                             Neuron      164       231\n",
      "60   61.                                       Nano Letters      164       207\n",
      "61   62.              Morbidity and Mortality Weekly Report      163       302\n",
      "62   63.                             European Heart Journal      163       265\n",
      "63   64.                              Nature Nanotechnology      163       264\n",
      "64   65.                                      ACS Catalysis      163       220\n",
      "65   66.                                Nature Neuroscience      162       248\n",
      "66   67.                           American Economic Review      160       263\n",
      "67   68.                     Journal of High Energy Physics      160       220\n",
      "68   69.            IEEE Communications Surveys & Tutorials      159       304\n",
      "69   70.                                 Annals of Oncology      159       243\n",
      "70   71.                                          Nutrients      159       214\n",
      "71   72.                      Accounts of Chemical Research      159       211\n",
      "72   73.                                           Immunity      158       242\n",
      "73   74.                 Environmental Science & Technology      158       214\n",
      "74   75.             Nature Reviews. Molecular Cell Biology      155       340\n",
      "75   76.                                                Gut      155       235\n",
      "76   77.                                  Physical Review D      155       217\n",
      "77   78.                                 ACS Energy Letters      155       212\n",
      "78   79.  Monthly Notices of the Royal Astronomical Society      155       194\n",
      "79   80.  Conference on Empirical Methods in Natural Lan...      154       249\n",
      "80   81.                       Clinical Infectious Diseases      153       278\n",
      "81   82.                                    Cell Metabolism      153       211\n",
      "82   83.                          Nature Reviews Immunology      152       292\n",
      "83   84.                                              Joule      152       233\n",
      "84   85.                                   Nature Photonics      152       228\n",
      "85   86.  International Journal of Environmental Researc...      152       225\n",
      "86   87.                            Environmental Pollution      152       222\n",
      "87   88.                        Computers in Human Behavior      152       214\n",
      "88   89.                          Frontiers in Microbiology      151       225\n",
      "89   90.                                     Nature Physics      151       222\n",
      "90   91.                                              Small      150       196\n",
      "91   92.                                       Cell Reports      149       205\n",
      "92   93.                                     Molecular Cell      149       202\n",
      "93   94.                           Clinical Cancer Research      146       201\n",
      "94   95.                             Bioresource Technology      146       190\n",
      "95   96.                       Journal of Business Research      145       233\n",
      "96   97.                                   Molecular Cancer      145       209\n",
      "97   98.                                            Sensors      145       201\n",
      "98   99.                              Nature Climate Change      144       228\n",
      "99  100.                    IEEE Internet of Things Journal      144       212 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scholar(url=\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93e499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
